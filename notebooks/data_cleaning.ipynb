{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9447e1",
   "metadata": {},
   "source": [
    "It cleans up  medical safety data by finding and removing duplicate patient reports so each adverse event case is only counted once instead of multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcef855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# DRUG FILE PATHS - Focus on the massive drug dataset\n",
    "DRUG_INPUT_PATH = \"/Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/notebooks/faers_drugs_combined_2020_2024.csv\"\n",
    "DRUG_OUTPUT_PATH = \"/Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/processed_cleaned/faers_drugs_deduplicated.csv\"\n",
    "\n",
    "def analyze_drug_file_structure():\n",
    "    \"\"\"\n",
    "    Analyze the massive 9GB drug file structure\n",
    "    \"\"\"\n",
    "    print(\"🔍 ANALYZING 9GB DRUG FILE STRUCTURE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not os.path.exists(DRUG_INPUT_PATH):\n",
    "        print(f\"❌ Drug file not found: {DRUG_INPUT_PATH}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Check file size\n",
    "    file_size_gb = os.path.getsize(DRUG_INPUT_PATH) / (1024**3)\n",
    "    print(f\"📁 File size: {file_size_gb:.2f} GB\")\n",
    "    \n",
    "    try:\n",
    "        # Load small sample to understand structure\n",
    "        print(f\"📊 Loading sample to check structure...\")\n",
    "        sample_df = pd.read_csv(DRUG_INPUT_PATH, nrows=5000, low_memory=False)\n",
    "        \n",
    "        print(f\"  Sample: {sample_df.shape[0]:,} rows, {sample_df.shape[1]} columns\")\n",
    "        print(f\"  Columns: {list(sample_df.columns)}\")\n",
    "        \n",
    "        # Find deduplication columns\n",
    "        caseid_col = None\n",
    "        caseversion_col = None\n",
    "        \n",
    "        # Look for case ID variations\n",
    "        caseid_candidates = ['caseid', 'case_id', 'primaryid', 'isr']\n",
    "        for col in sample_df.columns:\n",
    "            if any(candidate.lower() in col.lower() for candidate in caseid_candidates):\n",
    "                caseid_col = col\n",
    "                break\n",
    "        \n",
    "        # Look for version variations  \n",
    "        version_candidates = ['caseversion', 'case_version', 'version']\n",
    "        for col in sample_df.columns:\n",
    "            if any(candidate.lower() in col.lower() for candidate in version_candidates):\n",
    "                caseversion_col = col\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n🎯 DEDUPLICATION COLUMNS:\")\n",
    "        print(f\"  Case ID column: {caseid_col}\")\n",
    "        print(f\"  Case version column: {caseversion_col}\")\n",
    "        \n",
    "        if caseid_col and caseversion_col:\n",
    "            # Show sample data\n",
    "            print(f\"\\n📋 Sample deduplication data:\")\n",
    "            sample_data = sample_df[[caseid_col, caseversion_col]].head(10)\n",
    "            print(sample_data)\n",
    "            \n",
    "            # Show version distribution\n",
    "            version_counts = sample_df[caseversion_col].value_counts().head(10)\n",
    "            print(f\"\\n📈 Version distribution (sample):\")\n",
    "            for version, count in version_counts.items():\n",
    "                print(f\"    Version {version}: {count:,} records\")\n",
    "        \n",
    "        return caseid_col, caseversion_col\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error analyzing file: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def estimate_drug_duplicates(caseid_col, caseversion_col, sample_size=200000):\n",
    "    \"\"\"\n",
    "    Estimate duplicate rate in the massive drug file\n",
    "    \"\"\"\n",
    "    print(f\"\\n📊 ESTIMATING DUPLICATES IN 9GB DRUG FILE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Analyzing sample of {sample_size:,} rows...\")\n",
    "    \n",
    "    try:\n",
    "        # Load larger sample for better estimate\n",
    "        sample_df = pd.read_csv(DRUG_INPUT_PATH, nrows=sample_size, low_memory=False)\n",
    "        \n",
    "        total_rows = len(sample_df)\n",
    "        unique_cases = sample_df[caseid_col].nunique()\n",
    "        \n",
    "        # Convert version to numeric for analysis\n",
    "        sample_df[caseversion_col] = pd.to_numeric(sample_df[caseversion_col], errors='coerce')\n",
    "        \n",
    "        # Count cases with multiple versions\n",
    "        case_counts = sample_df.groupby(caseid_col).size()\n",
    "        cases_with_multiple_versions = (case_counts > 1).sum()\n",
    "        \n",
    "        # Find max versions per case\n",
    "        max_versions = sample_df.groupby(caseid_col)[caseversion_col].max()\n",
    "        avg_max_version = max_versions.mean()\n",
    "        \n",
    "        duplicate_rate = (total_rows - unique_cases) / total_rows * 100\n",
    "        \n",
    "        print(f\"📈 DUPLICATE ANALYSIS RESULTS:\")\n",
    "        print(f\"  Sample size: {total_rows:,} rows\")\n",
    "        print(f\"  Unique cases: {unique_cases:,}\")\n",
    "        print(f\"  Total duplicates: {total_rows - unique_cases:,}\")\n",
    "        print(f\"  Duplicate rate: {duplicate_rate:.1f}%\")\n",
    "        print(f\"  Cases with multiple versions: {cases_with_multiple_versions:,}\")\n",
    "        print(f\"  Average max version: {avg_max_version:.1f}\")\n",
    "        \n",
    "        # Estimate final file size\n",
    "        estimated_final_rows = unique_cases\n",
    "        size_reduction = duplicate_rate\n",
    "        \n",
    "        print(f\"\\n🎯 PROJECTED RESULTS:\")\n",
    "        print(f\"  Estimated final rows: {estimated_final_rows:,}\")\n",
    "        print(f\"  Estimated size reduction: {size_reduction:.1f}%\")\n",
    "        \n",
    "        return duplicate_rate, unique_cases\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error estimating duplicates: {e}\")\n",
    "        return 0, 0\n",
    "\n",
    "def deduplicate_massive_drug_file(caseid_col, caseversion_col, chunk_size=25000):\n",
    "    \"\"\"\n",
    "    Deduplicate the massive 9GB drug file using optimized chunked processing\n",
    "    \"\"\"\n",
    "    print(f\"\\n🚀 DEDUPLICATING MASSIVE 9GB DRUG FILE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Strategy: Keep latest {caseversion_col} for each {caseid_col}\")\n",
    "    print(f\"Chunk size: {chunk_size:,} rows (optimized for 9GB file)\")\n",
    "    \n",
    "    try:\n",
    "        # PHASE 1: Scan entire file to find latest version per case\n",
    "        print(f\"\\n📋 PHASE 1: Scanning 9GB file to find latest versions...\")\n",
    "        print(\"This may take 15-30 minutes for 9GB file...\")\n",
    "        \n",
    "        latest_versions = {}\n",
    "        total_rows_scanned = 0\n",
    "        chunk_count = 0\n",
    "        \n",
    "        # Use smaller chunks for better memory management with 9GB file\n",
    "        chunk_iterator = pd.read_csv(DRUG_INPUT_PATH, chunksize=chunk_size, low_memory=False)\n",
    "        \n",
    "        for chunk in chunk_iterator:\n",
    "            chunk_count += 1\n",
    "            \n",
    "            # Convert version to numeric\n",
    "            chunk[caseversion_col] = pd.to_numeric(chunk[caseversion_col], errors='coerce')\n",
    "            \n",
    "            # Find max version per case in this chunk\n",
    "            chunk_latest = chunk.groupby(caseid_col)[caseversion_col].max()\n",
    "            \n",
    "            # Update global latest versions\n",
    "            for caseid, version in chunk_latest.items():\n",
    "                if pd.notna(version):\n",
    "                    if caseid not in latest_versions or version > latest_versions[caseid]:\n",
    "                        latest_versions[caseid] = version\n",
    "            \n",
    "            total_rows_scanned += len(chunk)\n",
    "            \n",
    "            # Progress update every 50 chunks\n",
    "            if chunk_count % 50 == 0:\n",
    "                gb_processed = (total_rows_scanned * 4) / (1024**3)  # Rough estimate\n",
    "                print(f\"    Processed {chunk_count} chunks, ~{gb_processed:.1f}GB, {total_rows_scanned:,} rows...\")\n",
    "                print(f\"    Found {len(latest_versions):,} unique cases so far...\")\n",
    "            \n",
    "            # Clear chunk memory aggressively\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "        \n",
    "        print(f\"✅ Phase 1 complete!\")\n",
    "        print(f\"  Total rows scanned: {total_rows_scanned:,}\")\n",
    "        print(f\"  Unique cases found: {len(latest_versions):,}\")\n",
    "        print(f\"  Latest versions identified for all cases\")\n",
    "        \n",
    "        # PHASE 2: Filter and save only latest versions\n",
    "        print(f\"\\n💾 PHASE 2: Filtering and saving latest versions...\")\n",
    "        print(\"This may take another 15-30 minutes...\")\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(os.path.dirname(DRUG_OUTPUT_PATH), exist_ok=True)\n",
    "        \n",
    "        # Save header\n",
    "        header_df = pd.read_csv(DRUG_INPUT_PATH, nrows=0)\n",
    "        header_df.to_csv(DRUG_OUTPUT_PATH, index=False)\n",
    "        \n",
    "        # Process file again, keeping only latest versions\n",
    "        kept_rows = 0\n",
    "        total_processed = 0\n",
    "        chunk_count = 0\n",
    "        \n",
    "        chunk_iterator = pd.read_csv(DRUG_INPUT_PATH, chunksize=chunk_size, low_memory=False)\n",
    "        \n",
    "        for chunk in chunk_iterator:\n",
    "            chunk_count += 1\n",
    "            chunk[caseversion_col] = pd.to_numeric(chunk[caseversion_col], errors='coerce')\n",
    "            \n",
    "            # Filter to keep only latest versions\n",
    "            def is_latest_version(row):\n",
    "                caseid = row[caseid_col]\n",
    "                version = row[caseversion_col]\n",
    "                \n",
    "                if pd.isna(version) or caseid not in latest_versions:\n",
    "                    return False\n",
    "                \n",
    "                return version == latest_versions[caseid]\n",
    "            \n",
    "            # Apply filter efficiently\n",
    "            mask = chunk.apply(is_latest_version, axis=1)\n",
    "            filtered_chunk = chunk[mask].copy()\n",
    "            \n",
    "            # Save filtered chunk\n",
    "            if not filtered_chunk.empty:\n",
    "                filtered_chunk.to_csv(DRUG_OUTPUT_PATH, mode='a', header=False, index=False)\n",
    "                kept_rows += len(filtered_chunk)\n",
    "            \n",
    "            total_processed += len(chunk)\n",
    "            \n",
    "            # Progress update every 50 chunks\n",
    "            if chunk_count % 50 == 0:\n",
    "                gb_processed = (total_processed * 4) / (1024**3)\n",
    "                print(f\"    Processed {chunk_count} chunks, ~{gb_processed:.1f}GB\")\n",
    "                print(f\"    Kept {kept_rows:,}/{total_processed:,} rows so far...\")\n",
    "            \n",
    "            # Aggressive memory cleanup\n",
    "            del chunk, filtered_chunk, mask\n",
    "            gc.collect()\n",
    "        \n",
    "        # Final statistics\n",
    "        removed_rows = total_processed - kept_rows\n",
    "        removal_rate = (removed_rows / total_processed) * 100\n",
    "        \n",
    "        # Calculate file size reduction\n",
    "        output_size_gb = os.path.getsize(DRUG_OUTPUT_PATH) / (1024**3) if os.path.exists(DRUG_OUTPUT_PATH) else 0\n",
    "        input_size_gb = os.path.getsize(DRUG_INPUT_PATH) / (1024**3)\n",
    "        size_reduction = ((input_size_gb - output_size_gb) / input_size_gb) * 100\n",
    "        \n",
    "        print(f\"\\n🎉 MASSIVE FILE DEDUPLICATION COMPLETE!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"📊 FINAL STATISTICS:\")\n",
    "        print(f\"  Input file size: {input_size_gb:.2f} GB\")\n",
    "        print(f\"  Output file size: {output_size_gb:.2f} GB\")\n",
    "        print(f\"  Size reduction: {size_reduction:.1f}%\")\n",
    "        print(f\"  Total input rows: {total_processed:,}\")\n",
    "        print(f\"  Final output rows: {kept_rows:,}\")\n",
    "        print(f\"  Removed rows: {removed_rows:,}\")\n",
    "        print(f\"  Row reduction: {removal_rate:.1f}%\")\n",
    "        print(f\"  Output file: {DRUG_OUTPUT_PATH}\")\n",
    "        \n",
    "        return {\n",
    "            'input_rows': total_processed,\n",
    "            'output_rows': kept_rows,\n",
    "            'removed_rows': removed_rows,\n",
    "            'removal_rate': removal_rate,\n",
    "            'input_size_gb': input_size_gb,\n",
    "            'output_size_gb': output_size_gb,\n",
    "            'size_reduction': size_reduction\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during massive file deduplication: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_drug_deduplication():\n",
    "    \"\"\"\n",
    "    Complete drug file deduplication workflow\n",
    "    \"\"\"\n",
    "    print(\"💊 MASSIVE DRUG FILE DEDUPLICATION WORKFLOW\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Optimized for 9GB FAERS drug dataset\")\n",
    "    \n",
    "    # Step 1: Analyze file structure\n",
    "    caseid_col, caseversion_col = analyze_drug_file_structure()\n",
    "    \n",
    "    if not caseid_col or not caseversion_col:\n",
    "        print(\"❌ Cannot proceed - missing required columns\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Estimate duplicates\n",
    "    duplicate_rate, unique_cases = estimate_drug_duplicates(caseid_col, caseversion_col)\n",
    "    \n",
    "    # Step 3: Ask for confirmation before processing 9GB file\n",
    "    print(f\"\\n⚠️  READY TO PROCESS 9GB FILE\")\n",
    "    print(f\"This will take 30-60 minutes and use significant processing power.\")\n",
    "    print(f\"Estimated duplicate removal: {duplicate_rate:.1f}%\")\n",
    "    print(f\"Proceed with deduplication? Uncomment the line below:\")\n",
    "    print(f\"# result = deduplicate_massive_drug_file('{caseid_col}', '{caseversion_col}')\")\n",
    "    \n",
    "    return caseid_col, caseversion_col\n",
    "\n",
    "# 🚀 MAIN EXECUTION\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"💊 MASSIVE DRUG FILE DEDUPLICATION TOOLKIT\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Specialized for 9GB FAERS drug dataset\")\n",
    "    print(\"FDA Standard: Keep latest CASEVERSION per CASEID\")\n",
    "    \n",
    "    print(f\"\\n🎯 OPTIMIZATIONS FOR 9GB FILE:\")\n",
    "    print(\"✅ Small chunk sizes (25K rows) for memory efficiency\")\n",
    "    print(\"✅ Two-phase processing (scan → filter)\")\n",
    "    print(\"✅ Aggressive memory cleanup between chunks\")\n",
    "    print(\"✅ Progress tracking for long-running process\")\n",
    "    print(\"✅ File size reduction statistics\")\n",
    "    \n",
    "    print(f\"\\n⏱️  EXPECTED TIME:\")\n",
    "    print(\"📋 Phase 1 (scan): 15-30 minutes\")\n",
    "    print(\"💾 Phase 2 (filter): 15-30 minutes\") \n",
    "    print(\"🎯 Total: 30-60 minutes for 9GB file\")\n",
    "    \n",
    "    print(f\"\\n🚀 TO START:\")\n",
    "    print(\"# caseid_col, caseversion_col = run_drug_deduplication()\")\n",
    "    \n",
    "    # Uncomment to run analysis:\n",
    "    # caseid_col, caseversion_col = run_drug_deduplication()\n",
    "    \n",
    "    # After analysis, uncomment to run full deduplication:\n",
    "    # result = deduplicate_massive_drug_file(caseid_col, caseversion_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6443071",
   "metadata": {},
   "source": [
    "####  Standardize Your Data Demo DF\"\n",
    "This is the most critical step.\n",
    "\n",
    "Dates: Convert all date fields (EVENT_DT, INIT_FDA_DT) to a single, consistent format (e.g., YYYY-MM-DD). Be aware that dates can be incomplete.\n",
    "\n",
    "Age: Standardize PATIENT_AGE into a single unit (years). FAERS reports age in various units (decades, years, months, days). You'll need to write a script to convert everything to years.\n",
    "\n",
    "Sex: Clean the PATIENT_SEX field to a uniform format (e.g., 'M', 'F', 'U' for Unknown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88be882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 KEEP ALL COLUMNS + CLEAN ESSENTIALS STRATEGY\n",
      "======================================================================\n",
      "Best of both worlds: Complete data + Fast cleaning!\n",
      "\n",
      "1️⃣ PREVIEW (see all columns + cleaning plan):\n",
      "# preview_all_columns()\n",
      "\n",
      "2️⃣ RUN COMPLETE CLEANING:\n",
      "# demo_cleaned = clean_demographics_all_columns()\n",
      "\n",
      "🎯 STRATEGY BENEFITS:\n",
      "✅ ALL original columns preserved\n",
      "✅ Only essential columns cleaned (faster)\n",
      "✅ Quality assessment on essential data\n",
      "✅ Automatic removal of junk rows\n",
      "✅ Chunked saving (no crashes)\n",
      "✅ New standardized columns added\n",
      "\n",
      "📊 RESULT:\n",
      "Complete dataset with all original data PLUS cleaned essential columns\n",
      "🔍 PREVIEW - ALL COLUMNS + CLEANING PLAN\n",
      "============================================================\n",
      "Sample: 1000 rows, 28 columns\n",
      "\n",
      "ALL COLUMNS (🎯 = will be cleaned):\n",
      "   1. 🎯 primaryid\n",
      "   2. 🎯 caseid\n",
      "   3. 📁 caseversion\n",
      "   4. 📁 i_f_code\n",
      "   5. 🎯 event_dt\n",
      "   6. 📁 mfr_dt\n",
      "   7. 🎯 init_fda_dt\n",
      "   8. 📁 fda_dt\n",
      "   9. 📁 rept_cod\n",
      "  10. 📁 auth_num\n",
      "  11. 📁 mfr_num\n",
      "  12. 📁 mfr_sndr\n",
      "  13. 📁 lit_ref\n",
      "  14. 🎯 age\n",
      "  15. 🎯 age_cod\n",
      "  16. 📁 age_grp\n",
      "  17. 🎯 sex\n",
      "  18. 📁 e_sub\n",
      "  19. 📁 wt\n",
      "  20. 📁 wt_cod\n",
      "  21. 🎯 rept_dt\n",
      "  22. 📁 to_mfr\n",
      "  23. 📁 occp_cod\n",
      "  24. 🎯 reporter_country\n",
      "  25. 📁 occr_country\n",
      "  26. 🎯 year\n",
      "  27. 🎯 quarter\n",
      "  28. 📁 source_file\n",
      "\n",
      "📋 CLEANING SUMMARY:\n",
      "  Will clean: ['primaryid', 'caseid', 'event_dt', 'init_fda_dt', 'rept_dt', 'age', 'age_cod', 'sex', 'reporter_country', 'year', 'quarter']\n",
      "\n",
      "Sample data:\n",
      "   primaryid    caseid    event_dt  init_fda_dt   rept_dt\n",
      "0  100046942  10004694         NaN     20140312  20200110\n",
      "1  100048206  10004820         NaN     20140312  20200309\n",
      "2  100048622  10004862  20051230.0     20140312  20200316\n",
      "🚀 FAERS DEMOGRAPHICS - KEEP ALL COLUMNS, CLEAN ESSENTIALS\n",
      "======================================================================\n",
      "Strategy: Load everything, clean only important columns!\n",
      "📊 Loading ALL COLUMNS (cleaning only essential ones for speed)\n",
      "Will clean these essential columns: ['primaryid', 'caseid', 'event_dt', 'init_fda_dt', 'rept_dt', 'age', 'age_cod', 'sex', 'reporter_country', 'year', 'quarter']\n",
      "✅ Loaded: 17,476,908 rows, 28 columns\n",
      "📋 Available essential columns: ['primaryid', 'caseid', 'event_dt', 'init_fda_dt', 'rept_dt', 'age', 'age_cod', 'sex', 'reporter_country', 'year', 'quarter']\n",
      "📋 ALL columns in file:\n",
      "   1. 🎯 primaryid\n",
      "   2. 🎯 caseid\n",
      "   3. 📁 caseversion\n",
      "   4. 📁 i_f_code\n",
      "   5. 🎯 event_dt\n",
      "   6. 📁 mfr_dt\n",
      "   7. 🎯 init_fda_dt\n",
      "   8. 📁 fda_dt\n",
      "   9. 📁 rept_cod\n",
      "  10. 📁 auth_num\n",
      "  11. 📁 mfr_num\n",
      "  12. 📁 mfr_sndr\n",
      "  13. 📁 lit_ref\n",
      "  14. 🎯 age\n",
      "  15. 🎯 age_cod\n",
      "  16. 📁 age_grp\n",
      "  17. 🎯 sex\n",
      "  18. 📁 e_sub\n",
      "  19. 📁 wt\n",
      "  20. 📁 wt_cod\n",
      "  21. 🎯 rept_dt\n",
      "  22. 📁 to_mfr\n",
      "  23. 📁 occp_cod\n",
      "  24. 🎯 reporter_country\n",
      "  25. 📁 occr_country\n",
      "  26. 🎯 year\n",
      "  27. 🎯 quarter\n",
      "  28. 📁 source_file\n",
      "\n",
      "==================================================\n",
      "🔍 CHECKING DATA QUALITY (Essential Columns Only)\n",
      "Quality assessment using:\n",
      "  Critical: ['primaryid']\n",
      "  Important: ['event_dt', 'age', 'sex']\n",
      "  Useful: ['init_fda_dt', 'rept_dt', 'age_cod', 'reporter_country']\n",
      "\n",
      "DATA QUALITY ASSESSMENT:\n",
      "  Total rows: 17,476,908\n",
      "  🟢 KEEP_GOOD_QUALITY: 6,360,318 (36.4%)\n",
      "  🔴 CONSIDER_REMOVE: 5,191,852 (29.7%)\n",
      "  🟡 KEEP_WITH_CAUTION: 4,324,544 (24.7%)\n",
      "  🔴 REMOVE_TOO_EMPTY: 1,600,194 (9.2%)\n",
      "\n",
      "RECOMMENDATIONS:\n",
      "  🔴 Definitely remove: 1,600,194 rows (9.2%)\n",
      "  🟡 Review carefully: 9,516,396 rows (54.5%)\n",
      "  🟢 Keep (good quality): 6,360,318 rows (36.4%)\n",
      "\n",
      "==================================================\n",
      "🗑️  REMOVING POOR QUALITY ROWS...\n",
      "  Removed 1,600,194 poor quality rows\n",
      "  Remaining: 15,876,714 rows (90.8% of original)\n",
      "\n",
      "==================================================\n",
      "🗓️  Cleaning ESSENTIAL date columns only: ['event_dt', 'init_fda_dt', 'rept_dt']\n",
      "\n",
      "Processing event_dt...\n",
      "  Original non-null: 8,152,904\n",
      "  Successfully cleaned: 0\n",
      "  Success rate: 0.0%\n",
      "\n",
      "Processing init_fda_dt...\n",
      "  Original non-null: 15,876,714\n",
      "  Successfully cleaned: 15,876,714\n",
      "  Success rate: 100.0%\n",
      "\n",
      "Processing rept_dt...\n",
      "  Original non-null: 15,874,616\n",
      "  Successfully cleaned: 0\n",
      "  Success rate: 0.0%\n",
      "\n",
      "==================================================\n",
      "👶 Cleaning essential age data\n",
      "Found age columns: age=True, age_cod=True\n",
      "  Age conversion success rate: 100.0%\n",
      "\n",
      "==================================================\n",
      "⚥ Cleaning essential sex data\n",
      "  Sex distribution:\n",
      "    F: 55.1%\n",
      "    M: 38.6%\n",
      "    U: 6.2%\n",
      "\n",
      "==================================================\n",
      "💾 Saving ALL COLUMNS with cleaned essentials...\n",
      "💾 Saving 15,876,714 rows in chunks of 50,000 to avoid crashes...\n",
      "  ✅ Saved chunk 1/318: rows 0 to 50,000\n",
      "  ✅ Saved chunk 2/318: rows 50,000 to 100,000\n",
      "  ✅ Saved chunk 3/318: rows 100,000 to 150,000\n",
      "  ✅ Saved chunk 4/318: rows 150,000 to 200,000\n",
      "  ✅ Saved chunk 5/318: rows 200,000 to 250,000\n",
      "  ✅ Saved chunk 6/318: rows 250,000 to 300,000\n",
      "  ✅ Saved chunk 7/318: rows 300,000 to 350,000\n",
      "  ✅ Saved chunk 8/318: rows 350,000 to 400,000\n",
      "  ✅ Saved chunk 9/318: rows 400,000 to 450,000\n",
      "  ✅ Saved chunk 10/318: rows 450,000 to 500,000\n",
      "  ✅ Saved chunk 11/318: rows 500,000 to 550,000\n",
      "  ✅ Saved chunk 12/318: rows 550,000 to 600,000\n",
      "  ✅ Saved chunk 13/318: rows 600,000 to 650,000\n",
      "  ✅ Saved chunk 14/318: rows 650,000 to 700,000\n",
      "  ✅ Saved chunk 15/318: rows 700,000 to 750,000\n",
      "  ✅ Saved chunk 16/318: rows 750,000 to 800,000\n",
      "  ✅ Saved chunk 17/318: rows 800,000 to 850,000\n",
      "  ✅ Saved chunk 18/318: rows 850,000 to 900,000\n",
      "  ✅ Saved chunk 19/318: rows 900,000 to 950,000\n",
      "  ✅ Saved chunk 20/318: rows 950,000 to 1,000,000\n",
      "  ✅ Saved chunk 21/318: rows 1,000,000 to 1,050,000\n",
      "  ✅ Saved chunk 22/318: rows 1,050,000 to 1,100,000\n",
      "  ✅ Saved chunk 23/318: rows 1,100,000 to 1,150,000\n",
      "  ✅ Saved chunk 24/318: rows 1,150,000 to 1,200,000\n",
      "  ✅ Saved chunk 25/318: rows 1,200,000 to 1,250,000\n",
      "  ✅ Saved chunk 26/318: rows 1,250,000 to 1,300,000\n",
      "  ✅ Saved chunk 27/318: rows 1,300,000 to 1,350,000\n",
      "  ✅ Saved chunk 28/318: rows 1,350,000 to 1,400,000\n",
      "  ✅ Saved chunk 29/318: rows 1,400,000 to 1,450,000\n",
      "  ✅ Saved chunk 30/318: rows 1,450,000 to 1,500,000\n",
      "  ✅ Saved chunk 31/318: rows 1,500,000 to 1,550,000\n",
      "  ✅ Saved chunk 32/318: rows 1,550,000 to 1,600,000\n",
      "  ✅ Saved chunk 33/318: rows 1,600,000 to 1,650,000\n",
      "  ✅ Saved chunk 34/318: rows 1,650,000 to 1,700,000\n",
      "  ✅ Saved chunk 35/318: rows 1,700,000 to 1,750,000\n",
      "  ✅ Saved chunk 36/318: rows 1,750,000 to 1,800,000\n",
      "  ✅ Saved chunk 37/318: rows 1,800,000 to 1,850,000\n",
      "  ✅ Saved chunk 38/318: rows 1,850,000 to 1,900,000\n",
      "  ✅ Saved chunk 39/318: rows 1,900,000 to 1,950,000\n",
      "  ✅ Saved chunk 40/318: rows 1,950,000 to 2,000,000\n",
      "  ✅ Saved chunk 41/318: rows 2,000,000 to 2,050,000\n",
      "  ✅ Saved chunk 42/318: rows 2,050,000 to 2,100,000\n",
      "  ✅ Saved chunk 43/318: rows 2,100,000 to 2,150,000\n",
      "  ✅ Saved chunk 44/318: rows 2,150,000 to 2,200,000\n",
      "  ✅ Saved chunk 45/318: rows 2,200,000 to 2,250,000\n",
      "  ✅ Saved chunk 46/318: rows 2,250,000 to 2,300,000\n",
      "  ✅ Saved chunk 47/318: rows 2,300,000 to 2,350,000\n",
      "  ✅ Saved chunk 48/318: rows 2,350,000 to 2,400,000\n",
      "  ✅ Saved chunk 49/318: rows 2,400,000 to 2,450,000\n",
      "  ✅ Saved chunk 50/318: rows 2,450,000 to 2,500,000\n",
      "  ✅ Saved chunk 51/318: rows 2,500,000 to 2,550,000\n",
      "  ✅ Saved chunk 52/318: rows 2,550,000 to 2,600,000\n",
      "  ✅ Saved chunk 53/318: rows 2,600,000 to 2,650,000\n",
      "  ✅ Saved chunk 54/318: rows 2,650,000 to 2,700,000\n",
      "  ✅ Saved chunk 55/318: rows 2,700,000 to 2,750,000\n",
      "  ✅ Saved chunk 56/318: rows 2,750,000 to 2,800,000\n",
      "  ✅ Saved chunk 57/318: rows 2,800,000 to 2,850,000\n",
      "  ✅ Saved chunk 58/318: rows 2,850,000 to 2,900,000\n",
      "  ✅ Saved chunk 59/318: rows 2,900,000 to 2,950,000\n",
      "  ✅ Saved chunk 60/318: rows 2,950,000 to 3,000,000\n",
      "  ✅ Saved chunk 61/318: rows 3,000,000 to 3,050,000\n",
      "  ✅ Saved chunk 62/318: rows 3,050,000 to 3,100,000\n",
      "  ✅ Saved chunk 63/318: rows 3,100,000 to 3,150,000\n",
      "  ✅ Saved chunk 64/318: rows 3,150,000 to 3,200,000\n",
      "  ✅ Saved chunk 65/318: rows 3,200,000 to 3,250,000\n",
      "  ✅ Saved chunk 66/318: rows 3,250,000 to 3,300,000\n",
      "  ✅ Saved chunk 67/318: rows 3,300,000 to 3,350,000\n",
      "  ✅ Saved chunk 68/318: rows 3,350,000 to 3,400,000\n",
      "  ✅ Saved chunk 69/318: rows 3,400,000 to 3,450,000\n",
      "  ✅ Saved chunk 70/318: rows 3,450,000 to 3,500,000\n",
      "  ✅ Saved chunk 71/318: rows 3,500,000 to 3,550,000\n",
      "  ✅ Saved chunk 72/318: rows 3,550,000 to 3,600,000\n",
      "  ✅ Saved chunk 73/318: rows 3,600,000 to 3,650,000\n",
      "  ✅ Saved chunk 74/318: rows 3,650,000 to 3,700,000\n",
      "  ✅ Saved chunk 75/318: rows 3,700,000 to 3,750,000\n",
      "  ✅ Saved chunk 76/318: rows 3,750,000 to 3,800,000\n",
      "  ✅ Saved chunk 77/318: rows 3,800,000 to 3,850,000\n",
      "  ✅ Saved chunk 78/318: rows 3,850,000 to 3,900,000\n",
      "  ✅ Saved chunk 79/318: rows 3,900,000 to 3,950,000\n",
      "  ✅ Saved chunk 80/318: rows 3,950,000 to 4,000,000\n",
      "  ✅ Saved chunk 81/318: rows 4,000,000 to 4,050,000\n",
      "  ✅ Saved chunk 82/318: rows 4,050,000 to 4,100,000\n",
      "  ✅ Saved chunk 83/318: rows 4,100,000 to 4,150,000\n",
      "  ✅ Saved chunk 84/318: rows 4,150,000 to 4,200,000\n",
      "  ✅ Saved chunk 85/318: rows 4,200,000 to 4,250,000\n",
      "  ✅ Saved chunk 86/318: rows 4,250,000 to 4,300,000\n",
      "  ✅ Saved chunk 87/318: rows 4,300,000 to 4,350,000\n",
      "  ✅ Saved chunk 88/318: rows 4,350,000 to 4,400,000\n",
      "  ✅ Saved chunk 89/318: rows 4,400,000 to 4,450,000\n",
      "  ✅ Saved chunk 90/318: rows 4,450,000 to 4,500,000\n",
      "  ✅ Saved chunk 91/318: rows 4,500,000 to 4,550,000\n",
      "  ✅ Saved chunk 92/318: rows 4,550,000 to 4,600,000\n",
      "  ✅ Saved chunk 93/318: rows 4,600,000 to 4,650,000\n",
      "  ✅ Saved chunk 94/318: rows 4,650,000 to 4,700,000\n",
      "  ✅ Saved chunk 95/318: rows 4,700,000 to 4,750,000\n",
      "  ✅ Saved chunk 96/318: rows 4,750,000 to 4,800,000\n",
      "  ✅ Saved chunk 97/318: rows 4,800,000 to 4,850,000\n",
      "  ✅ Saved chunk 98/318: rows 4,850,000 to 4,900,000\n",
      "  ✅ Saved chunk 99/318: rows 4,900,000 to 4,950,000\n",
      "  ✅ Saved chunk 100/318: rows 4,950,000 to 5,000,000\n",
      "  ✅ Saved chunk 101/318: rows 5,000,000 to 5,050,000\n",
      "  ✅ Saved chunk 102/318: rows 5,050,000 to 5,100,000\n",
      "  ✅ Saved chunk 103/318: rows 5,100,000 to 5,150,000\n",
      "  ✅ Saved chunk 104/318: rows 5,150,000 to 5,200,000\n",
      "  ✅ Saved chunk 105/318: rows 5,200,000 to 5,250,000\n",
      "  ✅ Saved chunk 106/318: rows 5,250,000 to 5,300,000\n",
      "  ✅ Saved chunk 107/318: rows 5,300,000 to 5,350,000\n",
      "  ✅ Saved chunk 108/318: rows 5,350,000 to 5,400,000\n",
      "  ✅ Saved chunk 109/318: rows 5,400,000 to 5,450,000\n",
      "  ✅ Saved chunk 110/318: rows 5,450,000 to 5,500,000\n",
      "  ✅ Saved chunk 111/318: rows 5,500,000 to 5,550,000\n",
      "  ✅ Saved chunk 112/318: rows 5,550,000 to 5,600,000\n",
      "  ✅ Saved chunk 113/318: rows 5,600,000 to 5,650,000\n",
      "  ✅ Saved chunk 114/318: rows 5,650,000 to 5,700,000\n",
      "  ✅ Saved chunk 115/318: rows 5,700,000 to 5,750,000\n",
      "  ✅ Saved chunk 116/318: rows 5,750,000 to 5,800,000\n",
      "  ✅ Saved chunk 117/318: rows 5,800,000 to 5,850,000\n",
      "  ✅ Saved chunk 118/318: rows 5,850,000 to 5,900,000\n",
      "  ✅ Saved chunk 119/318: rows 5,900,000 to 5,950,000\n",
      "  ✅ Saved chunk 120/318: rows 5,950,000 to 6,000,000\n",
      "  ✅ Saved chunk 121/318: rows 6,000,000 to 6,050,000\n",
      "  ✅ Saved chunk 122/318: rows 6,050,000 to 6,100,000\n",
      "  ✅ Saved chunk 123/318: rows 6,100,000 to 6,150,000\n",
      "  ✅ Saved chunk 124/318: rows 6,150,000 to 6,200,000\n",
      "  ✅ Saved chunk 125/318: rows 6,200,000 to 6,250,000\n",
      "  ✅ Saved chunk 126/318: rows 6,250,000 to 6,300,000\n",
      "  ✅ Saved chunk 127/318: rows 6,300,000 to 6,350,000\n",
      "  ✅ Saved chunk 128/318: rows 6,350,000 to 6,400,000\n",
      "  ✅ Saved chunk 129/318: rows 6,400,000 to 6,450,000\n",
      "  ✅ Saved chunk 130/318: rows 6,450,000 to 6,500,000\n",
      "  ✅ Saved chunk 131/318: rows 6,500,000 to 6,550,000\n",
      "  ✅ Saved chunk 132/318: rows 6,550,000 to 6,600,000\n",
      "  ✅ Saved chunk 133/318: rows 6,600,000 to 6,650,000\n",
      "  ✅ Saved chunk 134/318: rows 6,650,000 to 6,700,000\n",
      "  ✅ Saved chunk 135/318: rows 6,700,000 to 6,750,000\n",
      "  ✅ Saved chunk 136/318: rows 6,750,000 to 6,800,000\n",
      "  ✅ Saved chunk 137/318: rows 6,800,000 to 6,850,000\n",
      "  ✅ Saved chunk 138/318: rows 6,850,000 to 6,900,000\n",
      "  ✅ Saved chunk 139/318: rows 6,900,000 to 6,950,000\n",
      "  ✅ Saved chunk 140/318: rows 6,950,000 to 7,000,000\n",
      "  ✅ Saved chunk 141/318: rows 7,000,000 to 7,050,000\n",
      "  ✅ Saved chunk 142/318: rows 7,050,000 to 7,100,000\n",
      "  ✅ Saved chunk 143/318: rows 7,100,000 to 7,150,000\n",
      "  ✅ Saved chunk 144/318: rows 7,150,000 to 7,200,000\n",
      "  ✅ Saved chunk 145/318: rows 7,200,000 to 7,250,000\n",
      "  ✅ Saved chunk 146/318: rows 7,250,000 to 7,300,000\n",
      "  ✅ Saved chunk 147/318: rows 7,300,000 to 7,350,000\n",
      "  ✅ Saved chunk 148/318: rows 7,350,000 to 7,400,000\n",
      "  ✅ Saved chunk 149/318: rows 7,400,000 to 7,450,000\n",
      "  ✅ Saved chunk 150/318: rows 7,450,000 to 7,500,000\n",
      "  ✅ Saved chunk 151/318: rows 7,500,000 to 7,550,000\n",
      "  ✅ Saved chunk 152/318: rows 7,550,000 to 7,600,000\n",
      "  ✅ Saved chunk 153/318: rows 7,600,000 to 7,650,000\n",
      "  ✅ Saved chunk 154/318: rows 7,650,000 to 7,700,000\n",
      "  ✅ Saved chunk 155/318: rows 7,700,000 to 7,750,000\n",
      "  ✅ Saved chunk 156/318: rows 7,750,000 to 7,800,000\n",
      "  ✅ Saved chunk 157/318: rows 7,800,000 to 7,850,000\n",
      "  ✅ Saved chunk 158/318: rows 7,850,000 to 7,900,000\n",
      "  ✅ Saved chunk 159/318: rows 7,900,000 to 7,950,000\n",
      "  ✅ Saved chunk 160/318: rows 7,950,000 to 8,000,000\n",
      "  ✅ Saved chunk 161/318: rows 8,000,000 to 8,050,000\n",
      "  ✅ Saved chunk 162/318: rows 8,050,000 to 8,100,000\n",
      "  ✅ Saved chunk 163/318: rows 8,100,000 to 8,150,000\n",
      "  ✅ Saved chunk 164/318: rows 8,150,000 to 8,200,000\n",
      "  ✅ Saved chunk 165/318: rows 8,200,000 to 8,250,000\n",
      "  ✅ Saved chunk 166/318: rows 8,250,000 to 8,300,000\n",
      "  ✅ Saved chunk 167/318: rows 8,300,000 to 8,350,000\n",
      "  ✅ Saved chunk 168/318: rows 8,350,000 to 8,400,000\n",
      "  ✅ Saved chunk 169/318: rows 8,400,000 to 8,450,000\n",
      "  ✅ Saved chunk 170/318: rows 8,450,000 to 8,500,000\n",
      "  ✅ Saved chunk 171/318: rows 8,500,000 to 8,550,000\n",
      "  ✅ Saved chunk 172/318: rows 8,550,000 to 8,600,000\n",
      "  ✅ Saved chunk 173/318: rows 8,600,000 to 8,650,000\n",
      "  ✅ Saved chunk 174/318: rows 8,650,000 to 8,700,000\n",
      "  ✅ Saved chunk 175/318: rows 8,700,000 to 8,750,000\n",
      "  ✅ Saved chunk 176/318: rows 8,750,000 to 8,800,000\n",
      "  ✅ Saved chunk 177/318: rows 8,800,000 to 8,850,000\n",
      "  ✅ Saved chunk 178/318: rows 8,850,000 to 8,900,000\n",
      "  ✅ Saved chunk 179/318: rows 8,900,000 to 8,950,000\n",
      "  ✅ Saved chunk 180/318: rows 8,950,000 to 9,000,000\n",
      "  ✅ Saved chunk 181/318: rows 9,000,000 to 9,050,000\n",
      "  ✅ Saved chunk 182/318: rows 9,050,000 to 9,100,000\n",
      "  ✅ Saved chunk 183/318: rows 9,100,000 to 9,150,000\n",
      "  ✅ Saved chunk 184/318: rows 9,150,000 to 9,200,000\n",
      "  ✅ Saved chunk 185/318: rows 9,200,000 to 9,250,000\n",
      "  ✅ Saved chunk 186/318: rows 9,250,000 to 9,300,000\n",
      "  ✅ Saved chunk 187/318: rows 9,300,000 to 9,350,000\n",
      "  ✅ Saved chunk 188/318: rows 9,350,000 to 9,400,000\n",
      "  ✅ Saved chunk 189/318: rows 9,400,000 to 9,450,000\n",
      "  ✅ Saved chunk 190/318: rows 9,450,000 to 9,500,000\n",
      "  ✅ Saved chunk 191/318: rows 9,500,000 to 9,550,000\n",
      "  ✅ Saved chunk 192/318: rows 9,550,000 to 9,600,000\n",
      "  ✅ Saved chunk 193/318: rows 9,600,000 to 9,650,000\n",
      "  ✅ Saved chunk 194/318: rows 9,650,000 to 9,700,000\n",
      "  ✅ Saved chunk 195/318: rows 9,700,000 to 9,750,000\n",
      "  ✅ Saved chunk 196/318: rows 9,750,000 to 9,800,000\n",
      "  ✅ Saved chunk 197/318: rows 9,800,000 to 9,850,000\n",
      "  ✅ Saved chunk 198/318: rows 9,850,000 to 9,900,000\n",
      "  ✅ Saved chunk 199/318: rows 9,900,000 to 9,950,000\n",
      "  ✅ Saved chunk 200/318: rows 9,950,000 to 10,000,000\n",
      "  ✅ Saved chunk 201/318: rows 10,000,000 to 10,050,000\n",
      "  ✅ Saved chunk 202/318: rows 10,050,000 to 10,100,000\n",
      "  ✅ Saved chunk 203/318: rows 10,100,000 to 10,150,000\n",
      "  ✅ Saved chunk 204/318: rows 10,150,000 to 10,200,000\n",
      "  ✅ Saved chunk 205/318: rows 10,200,000 to 10,250,000\n",
      "  ✅ Saved chunk 206/318: rows 10,250,000 to 10,300,000\n",
      "  ✅ Saved chunk 207/318: rows 10,300,000 to 10,350,000\n",
      "  ✅ Saved chunk 208/318: rows 10,350,000 to 10,400,000\n",
      "  ✅ Saved chunk 209/318: rows 10,400,000 to 10,450,000\n",
      "  ✅ Saved chunk 210/318: rows 10,450,000 to 10,500,000\n",
      "  ✅ Saved chunk 211/318: rows 10,500,000 to 10,550,000\n",
      "  ✅ Saved chunk 212/318: rows 10,550,000 to 10,600,000\n",
      "  ✅ Saved chunk 213/318: rows 10,600,000 to 10,650,000\n",
      "  ✅ Saved chunk 214/318: rows 10,650,000 to 10,700,000\n",
      "  ✅ Saved chunk 215/318: rows 10,700,000 to 10,750,000\n",
      "  ✅ Saved chunk 216/318: rows 10,750,000 to 10,800,000\n",
      "  ✅ Saved chunk 217/318: rows 10,800,000 to 10,850,000\n",
      "  ✅ Saved chunk 218/318: rows 10,850,000 to 10,900,000\n",
      "  ✅ Saved chunk 219/318: rows 10,900,000 to 10,950,000\n",
      "  ✅ Saved chunk 220/318: rows 10,950,000 to 11,000,000\n",
      "  ✅ Saved chunk 221/318: rows 11,000,000 to 11,050,000\n",
      "  ✅ Saved chunk 222/318: rows 11,050,000 to 11,100,000\n",
      "  ✅ Saved chunk 223/318: rows 11,100,000 to 11,150,000\n",
      "  ✅ Saved chunk 224/318: rows 11,150,000 to 11,200,000\n",
      "  ✅ Saved chunk 225/318: rows 11,200,000 to 11,250,000\n",
      "  ✅ Saved chunk 226/318: rows 11,250,000 to 11,300,000\n",
      "  ✅ Saved chunk 227/318: rows 11,300,000 to 11,350,000\n",
      "  ✅ Saved chunk 228/318: rows 11,350,000 to 11,400,000\n",
      "  ✅ Saved chunk 229/318: rows 11,400,000 to 11,450,000\n",
      "  ✅ Saved chunk 230/318: rows 11,450,000 to 11,500,000\n",
      "  ✅ Saved chunk 231/318: rows 11,500,000 to 11,550,000\n",
      "  ✅ Saved chunk 232/318: rows 11,550,000 to 11,600,000\n",
      "  ✅ Saved chunk 233/318: rows 11,600,000 to 11,650,000\n",
      "  ✅ Saved chunk 234/318: rows 11,650,000 to 11,700,000\n",
      "  ✅ Saved chunk 235/318: rows 11,700,000 to 11,750,000\n",
      "  ✅ Saved chunk 236/318: rows 11,750,000 to 11,800,000\n",
      "  ✅ Saved chunk 237/318: rows 11,800,000 to 11,850,000\n",
      "  ✅ Saved chunk 238/318: rows 11,850,000 to 11,900,000\n",
      "  ✅ Saved chunk 239/318: rows 11,900,000 to 11,950,000\n",
      "  ✅ Saved chunk 240/318: rows 11,950,000 to 12,000,000\n",
      "  ✅ Saved chunk 241/318: rows 12,000,000 to 12,050,000\n",
      "  ✅ Saved chunk 242/318: rows 12,050,000 to 12,100,000\n",
      "  ✅ Saved chunk 243/318: rows 12,100,000 to 12,150,000\n",
      "  ✅ Saved chunk 244/318: rows 12,150,000 to 12,200,000\n",
      "  ✅ Saved chunk 245/318: rows 12,200,000 to 12,250,000\n",
      "  ✅ Saved chunk 246/318: rows 12,250,000 to 12,300,000\n",
      "  ✅ Saved chunk 247/318: rows 12,300,000 to 12,350,000\n",
      "  ✅ Saved chunk 248/318: rows 12,350,000 to 12,400,000\n",
      "  ✅ Saved chunk 249/318: rows 12,400,000 to 12,450,000\n",
      "  ✅ Saved chunk 250/318: rows 12,450,000 to 12,500,000\n",
      "  ✅ Saved chunk 251/318: rows 12,500,000 to 12,550,000\n",
      "  ✅ Saved chunk 252/318: rows 12,550,000 to 12,600,000\n",
      "  ✅ Saved chunk 253/318: rows 12,600,000 to 12,650,000\n",
      "  ✅ Saved chunk 254/318: rows 12,650,000 to 12,700,000\n",
      "  ✅ Saved chunk 255/318: rows 12,700,000 to 12,750,000\n",
      "  ✅ Saved chunk 256/318: rows 12,750,000 to 12,800,000\n",
      "  ✅ Saved chunk 257/318: rows 12,800,000 to 12,850,000\n",
      "  ✅ Saved chunk 258/318: rows 12,850,000 to 12,900,000\n",
      "  ✅ Saved chunk 259/318: rows 12,900,000 to 12,950,000\n",
      "  ✅ Saved chunk 260/318: rows 12,950,000 to 13,000,000\n",
      "  ✅ Saved chunk 261/318: rows 13,000,000 to 13,050,000\n",
      "  ✅ Saved chunk 262/318: rows 13,050,000 to 13,100,000\n",
      "  ✅ Saved chunk 263/318: rows 13,100,000 to 13,150,000\n",
      "  ✅ Saved chunk 264/318: rows 13,150,000 to 13,200,000\n",
      "  ✅ Saved chunk 265/318: rows 13,200,000 to 13,250,000\n",
      "  ✅ Saved chunk 266/318: rows 13,250,000 to 13,300,000\n",
      "  ✅ Saved chunk 267/318: rows 13,300,000 to 13,350,000\n",
      "  ✅ Saved chunk 268/318: rows 13,350,000 to 13,400,000\n",
      "  ✅ Saved chunk 269/318: rows 13,400,000 to 13,450,000\n",
      "  ✅ Saved chunk 270/318: rows 13,450,000 to 13,500,000\n",
      "  ✅ Saved chunk 271/318: rows 13,500,000 to 13,550,000\n",
      "  ✅ Saved chunk 272/318: rows 13,550,000 to 13,600,000\n",
      "  ✅ Saved chunk 273/318: rows 13,600,000 to 13,650,000\n",
      "  ✅ Saved chunk 274/318: rows 13,650,000 to 13,700,000\n",
      "  ✅ Saved chunk 275/318: rows 13,700,000 to 13,750,000\n",
      "  ✅ Saved chunk 276/318: rows 13,750,000 to 13,800,000\n",
      "  ✅ Saved chunk 277/318: rows 13,800,000 to 13,850,000\n",
      "  ✅ Saved chunk 278/318: rows 13,850,000 to 13,900,000\n",
      "  ✅ Saved chunk 279/318: rows 13,900,000 to 13,950,000\n",
      "  ✅ Saved chunk 280/318: rows 13,950,000 to 14,000,000\n",
      "  ✅ Saved chunk 281/318: rows 14,000,000 to 14,050,000\n",
      "  ✅ Saved chunk 282/318: rows 14,050,000 to 14,100,000\n",
      "  ✅ Saved chunk 283/318: rows 14,100,000 to 14,150,000\n",
      "  ✅ Saved chunk 284/318: rows 14,150,000 to 14,200,000\n",
      "  ✅ Saved chunk 285/318: rows 14,200,000 to 14,250,000\n",
      "  ✅ Saved chunk 286/318: rows 14,250,000 to 14,300,000\n",
      "  ✅ Saved chunk 287/318: rows 14,300,000 to 14,350,000\n",
      "  ✅ Saved chunk 288/318: rows 14,350,000 to 14,400,000\n",
      "  ✅ Saved chunk 289/318: rows 14,400,000 to 14,450,000\n",
      "  ✅ Saved chunk 290/318: rows 14,450,000 to 14,500,000\n",
      "  ✅ Saved chunk 291/318: rows 14,500,000 to 14,550,000\n",
      "  ✅ Saved chunk 292/318: rows 14,550,000 to 14,600,000\n",
      "  ✅ Saved chunk 293/318: rows 14,600,000 to 14,650,000\n",
      "  ✅ Saved chunk 294/318: rows 14,650,000 to 14,700,000\n",
      "  ✅ Saved chunk 295/318: rows 14,700,000 to 14,750,000\n",
      "  ✅ Saved chunk 296/318: rows 14,750,000 to 14,800,000\n",
      "  ✅ Saved chunk 297/318: rows 14,800,000 to 14,850,000\n",
      "  ✅ Saved chunk 298/318: rows 14,850,000 to 14,900,000\n",
      "  ✅ Saved chunk 299/318: rows 14,900,000 to 14,950,000\n",
      "  ✅ Saved chunk 300/318: rows 14,950,000 to 15,000,000\n",
      "  ✅ Saved chunk 301/318: rows 15,000,000 to 15,050,000\n",
      "  ✅ Saved chunk 302/318: rows 15,050,000 to 15,100,000\n",
      "  ✅ Saved chunk 303/318: rows 15,100,000 to 15,150,000\n",
      "  ✅ Saved chunk 304/318: rows 15,150,000 to 15,200,000\n",
      "  ✅ Saved chunk 305/318: rows 15,200,000 to 15,250,000\n",
      "  ✅ Saved chunk 306/318: rows 15,250,000 to 15,300,000\n",
      "  ✅ Saved chunk 307/318: rows 15,300,000 to 15,350,000\n",
      "  ✅ Saved chunk 308/318: rows 15,350,000 to 15,400,000\n",
      "  ✅ Saved chunk 309/318: rows 15,400,000 to 15,450,000\n",
      "  ✅ Saved chunk 310/318: rows 15,450,000 to 15,500,000\n",
      "  ✅ Saved chunk 311/318: rows 15,500,000 to 15,550,000\n",
      "  ✅ Saved chunk 312/318: rows 15,550,000 to 15,600,000\n",
      "  ✅ Saved chunk 313/318: rows 15,600,000 to 15,650,000\n",
      "  ✅ Saved chunk 314/318: rows 15,650,000 to 15,700,000\n",
      "  ✅ Saved chunk 315/318: rows 15,700,000 to 15,750,000\n",
      "  ✅ Saved chunk 316/318: rows 15,750,000 to 15,800,000\n",
      "  ✅ Saved chunk 317/318: rows 15,800,000 to 15,850,000\n",
      "  ✅ Saved chunk 318/318: rows 15,850,000 to 15,876,714\n",
      "✅ Successfully saved all 15,876,714 rows to /Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/processed_cleaned/faers_demographics_cleaned.csv\n",
      "\n",
      "✅ COMPLETE SUCCESS!\n",
      "\n",
      "FINAL SUMMARY:\n",
      "  Original records: 17,476,908\n",
      "  Original columns: 28\n",
      "  🗑️  Removed (poor quality): 1,600,194 (9.2%)\n",
      "  ✅ Final clean records: 15,876,714\n",
      "  ✅ Final columns: 39 (all preserved + new clean columns)\n",
      "  Valid event dates: 0\n",
      "  Valid ages: 9,849,690\n",
      "  Valid sex (M/F): 14,888,922\n",
      "  File saved to: /Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/processed_cleaned/faers_demographics_cleaned.csv\n",
      "\n",
      "💡 Added columns:\n",
      "     - data_quality_flag & data_quality_score\n",
      "     - *_standardized columns for cleaned essential data\n",
      "     - age_in_years & age_group\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FILE PATHS\n",
    "DEMO_FILE_PATH = \"/Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/processed/faers_demographics_combined_2020_2024.csv\"\n",
    "OUTPUT_FILE_PATH = \"/Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/processed_cleaned/faers_demographics_cleaned.csv\"\n",
    "\n",
    "# ESSENTIAL COLUMNS TO CLEAN (but we'll keep ALL columns in the file)\n",
    "ESSENTIAL_COLUMNS_TO_CLEAN = [\n",
    "    'primaryid',        # Key identifier\n",
    "    'caseid',          # Alternative key\n",
    "    'event_dt',        # Main event date\n",
    "    'init_fda_dt',     # FDA initial date\n",
    "    'rept_dt',         # Report date\n",
    "    'age',             # Age value\n",
    "    'age_cod',         # Age unit\n",
    "    'sex',             # Sex/gender\n",
    "    'reporter_country', # Country (useful for analysis)\n",
    "    'year',            # Year (useful for filtering)\n",
    "    'quarter'          # Quarter (useful for filtering)\n",
    "]\n",
    "\n",
    "def load_all_columns():\n",
    "    \"\"\"\n",
    "    Load ALL columns but focus cleaning on essential ones only\n",
    "    \"\"\"\n",
    "    print(\"📊 Loading ALL COLUMNS (cleaning only essential ones for speed)\")\n",
    "    print(f\"Will clean these essential columns: {ESSENTIAL_COLUMNS_TO_CLEAN}\")\n",
    "    \n",
    "    try:\n",
    "        # Load ALL columns\n",
    "        demo_df = pd.read_csv(DEMO_FILE_PATH, low_memory=False)\n",
    "        print(f\"✅ Loaded: {demo_df.shape[0]:,} rows, {demo_df.shape[1]} columns\")\n",
    "        \n",
    "        # Check which essential columns are available\n",
    "        available_essential = [col for col in ESSENTIAL_COLUMNS_TO_CLEAN if col in demo_df.columns]\n",
    "        missing_essential = [col for col in ESSENTIAL_COLUMNS_TO_CLEAN if col not in demo_df.columns]\n",
    "        \n",
    "        print(f\"📋 Available essential columns: {available_essential}\")\n",
    "        if missing_essential:\n",
    "            print(f\"⚠️  Missing essential columns: {missing_essential}\")\n",
    "        \n",
    "        print(f\"📋 ALL columns in file:\")\n",
    "        for i, col in enumerate(demo_df.columns, 1):\n",
    "            status = \"🎯\" if col in ESSENTIAL_COLUMNS_TO_CLEAN else \"📁\"\n",
    "            print(f\"  {i:2d}. {status} {col}\")\n",
    "        \n",
    "        return demo_df, available_essential\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading file: {e}\")\n",
    "        return None, []\n",
    "\n",
    "def check_data_quality_essential_only(demo_df, available_essential):\n",
    "    \"\"\"\n",
    "    Check data quality focusing ONLY on essential columns\n",
    "    Adds 'data_quality_flag' column based on essential data\n",
    "    \"\"\"\n",
    "    print(\"🔍 CHECKING DATA QUALITY (Essential Columns Only)\")\n",
    "    \n",
    "    demo_clean = demo_df.copy()\n",
    "    \n",
    "    # Define critical columns that should not be empty (from essential list)\n",
    "    critical_columns = ['primaryid'] if 'primaryid' in available_essential else []\n",
    "    \n",
    "    # Define important columns (from available essential)\n",
    "    important_columns = [col for col in ['event_dt', 'age', 'sex'] if col in available_essential]\n",
    "    \n",
    "    # Define useful columns (from available essential)\n",
    "    useful_columns = [col for col in ['init_fda_dt', 'rept_dt', 'age_cod', 'reporter_country'] if col in available_essential]\n",
    "    \n",
    "    print(f\"Quality assessment using:\")\n",
    "    print(f\"  Critical: {critical_columns}\")\n",
    "    print(f\"  Important: {important_columns}\")\n",
    "    print(f\"  Useful: {useful_columns}\")\n",
    "    \n",
    "    # Count missing values for each row (only essential columns)\n",
    "    def assess_row_quality(row):\n",
    "        # Critical: If missing primaryid, definitely remove\n",
    "        if critical_columns and (pd.isna(row.get(critical_columns[0])) or row.get(critical_columns[0]) == ''):\n",
    "            return 'REMOVE_NO_ID'\n",
    "        \n",
    "        # Count missing important fields\n",
    "        important_missing = 0\n",
    "        for col in important_columns:\n",
    "            if pd.isna(row[col]) or row[col] == '' or row[col] == 'nan':\n",
    "                important_missing += 1\n",
    "        \n",
    "        # Count missing useful fields  \n",
    "        useful_missing = 0\n",
    "        for col in useful_columns:\n",
    "            if pd.isna(row[col]) or row[col] == '' or row[col] == 'nan':\n",
    "                useful_missing += 1\n",
    "        \n",
    "        # Decision logic\n",
    "        if important_missing >= len(important_columns):  # Missing all important fields\n",
    "            return 'REMOVE_TOO_EMPTY'\n",
    "        elif important_missing >= len(important_columns) - 1:  # Missing most important fields\n",
    "            return 'CONSIDER_REMOVE'\n",
    "        elif important_missing >= 1:  # Missing some important fields\n",
    "            return 'KEEP_WITH_CAUTION'\n",
    "        else:\n",
    "            return 'KEEP_GOOD_QUALITY'\n",
    "    \n",
    "    # Apply quality assessment\n",
    "    demo_clean['data_quality_flag'] = demo_clean.apply(assess_row_quality, axis=1)\n",
    "    \n",
    "    # Add numeric quality score (0-100, higher = better)\n",
    "    def calculate_quality_score(row):\n",
    "        if not (critical_columns or important_columns or useful_columns):\n",
    "            return 100  # If no essential columns to check, assume good quality\n",
    "            \n",
    "        total_possible = len(critical_columns) * 3 + len(important_columns) * 2 + len(useful_columns) * 1\n",
    "        if total_possible == 0:\n",
    "            return 100\n",
    "            \n",
    "        filled_fields = 0\n",
    "        \n",
    "        # Critical fields (weight = 3)\n",
    "        for col in critical_columns:\n",
    "            if pd.notna(row[col]) and row[col] != '' and row[col] != 'nan':\n",
    "                filled_fields += 3\n",
    "        \n",
    "        # Important fields (weight = 2)  \n",
    "        for col in important_columns:\n",
    "            if pd.notna(row[col]) and row[col] != '' and row[col] != 'nan':\n",
    "                filled_fields += 2\n",
    "        \n",
    "        # Useful fields (weight = 1)\n",
    "        for col in useful_columns:\n",
    "            if pd.notna(row[col]) and row[col] != '' and row[col] != 'nan':\n",
    "                filled_fields += 1\n",
    "        \n",
    "        return round((filled_fields / total_possible) * 100, 1)\n",
    "    \n",
    "    demo_clean['data_quality_score'] = demo_clean.apply(calculate_quality_score, axis=1)\n",
    "    \n",
    "    # Report quality statistics\n",
    "    quality_counts = demo_clean['data_quality_flag'].value_counts()\n",
    "    total_rows = len(demo_clean)\n",
    "    \n",
    "    print(f\"\\nDATA QUALITY ASSESSMENT:\")\n",
    "    print(f\"  Total rows: {total_rows:,}\")\n",
    "    \n",
    "    for flag, count in quality_counts.items():\n",
    "        pct = (count / total_rows) * 100\n",
    "        status = \"🔴\" if \"REMOVE\" in flag else \"🟡\" if \"CAUTION\" in flag else \"🟢\"\n",
    "        print(f\"  {status} {flag}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Summary recommendations\n",
    "    remove_rows = quality_counts.get('REMOVE_NO_ID', 0) + quality_counts.get('REMOVE_TOO_EMPTY', 0)\n",
    "    caution_rows = quality_counts.get('CONSIDER_REMOVE', 0) + quality_counts.get('KEEP_WITH_CAUTION', 0)\n",
    "    keep_rows = quality_counts.get('KEEP_GOOD_QUALITY', 0)\n",
    "    \n",
    "    print(f\"\\nRECOMMENDATIONS:\")\n",
    "    print(f\"  🔴 Definitely remove: {remove_rows:,} rows ({(remove_rows/total_rows)*100:.1f}%)\")\n",
    "    print(f\"  🟡 Review carefully: {caution_rows:,} rows ({(caution_rows/total_rows)*100:.1f}%)\")\n",
    "    print(f\"  🟢 Keep (good quality): {keep_rows:,} rows ({(keep_rows/total_rows)*100:.1f}%)\")\n",
    "    \n",
    "    return demo_clean\n",
    "\n",
    "def standardize_date_string(date_str):\n",
    "    \"\"\"\n",
    "    Convert various date formats to YYYY-MM-DD\n",
    "    Handles incomplete dates by padding with defaults\n",
    "    \"\"\"\n",
    "    if not date_str or date_str == '' or pd.isna(date_str):\n",
    "        return None\n",
    "    \n",
    "    # Remove any non-digit characters except hyphens and slashes\n",
    "    date_str = re.sub(r'[^\\d\\-/]', '', str(date_str))\n",
    "    \n",
    "    if not date_str:\n",
    "        return None\n",
    "    \n",
    "    # Handle different date formats\n",
    "    patterns = [\n",
    "        # YYYYMMDD\n",
    "        (r'^(\\d{4})(\\d{2})(\\d{2})$', r'\\1-\\2-\\3'),\n",
    "        # YYYY-MM-DD or YYYY/MM/DD\n",
    "        (r'^(\\d{4})[-/](\\d{1,2})[-/](\\d{1,2})$', r'\\1-\\2-\\3'),\n",
    "        # MM/DD/YYYY or MM-DD-YYYY\n",
    "        (r'^(\\d{1,2})[-/](\\d{1,2})[-/](\\d{4})$', r'\\3-\\1-\\2'),\n",
    "        # DD/MM/YYYY or DD-MM-YYYY (assuming day first if day > 12)\n",
    "        (r'^(\\d{1,2})[-/](\\d{1,2})[-/](\\d{4})$', r'\\3-\\2-\\1'),\n",
    "        # YYYY-MM (incomplete - pad day with 01)\n",
    "        (r'^(\\d{4})[-/](\\d{1,2})$', r'\\1-\\2-01'),\n",
    "        # YYYY only (incomplete - pad with 01-01)\n",
    "        (r'^(\\d{4})$', r'\\1-01-01'),\n",
    "    ]\n",
    "    \n",
    "    for pattern, replacement in patterns:\n",
    "        if re.match(pattern, date_str):\n",
    "            result = re.sub(pattern, replacement, date_str)\n",
    "            # Ensure two-digit month and day\n",
    "            parts = result.split('-')\n",
    "            if len(parts) == 3:\n",
    "                year, month, day = parts\n",
    "                try:\n",
    "                    # Validate and format\n",
    "                    month = f\"{int(month):02d}\"\n",
    "                    day = f\"{int(day):02d}\"\n",
    "                    \n",
    "                    # Basic validation\n",
    "                    if 1 <= int(month) <= 12 and 1 <= int(day) <= 31:\n",
    "                        return f\"{year}-{month}-{day}\"\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "def clean_essential_dates_only(demo_df, available_essential):\n",
    "    \"\"\"\n",
    "    Clean ONLY the essential date columns\n",
    "    \"\"\"\n",
    "    # Only clean essential date columns that exist\n",
    "    essential_dates = ['event_dt', 'init_fda_dt', 'rept_dt']\n",
    "    priority_dates = [col for col in essential_dates if col in available_essential]\n",
    "    \n",
    "    print(f\"🗓️  Cleaning ESSENTIAL date columns only: {priority_dates}\")\n",
    "    \n",
    "    demo_clean = demo_df.copy()\n",
    "    \n",
    "    for col in priority_dates:\n",
    "        print(f\"\\nProcessing {col}...\")\n",
    "        original_count = demo_clean[col].notna().sum()\n",
    "        \n",
    "        # Convert to string and handle missing values\n",
    "        demo_clean[col] = demo_clean[col].astype(str).replace('nan', '')\n",
    "        \n",
    "        # Clean the date strings\n",
    "        demo_clean[f'{col}_cleaned'] = demo_clean[col].apply(standardize_date_string)\n",
    "        \n",
    "        # Convert to datetime\n",
    "        demo_clean[f'{col}_standardized'] = pd.to_datetime(\n",
    "            demo_clean[f'{col}_cleaned'], \n",
    "            format='%Y-%m-%d', \n",
    "            errors='coerce'\n",
    "        )\n",
    "        \n",
    "        # Count successful conversions\n",
    "        clean_count = demo_clean[f'{col}_standardized'].notna().sum()\n",
    "        success_rate = (clean_count / original_count * 100) if original_count > 0 else 0\n",
    "        \n",
    "        print(f\"  Original non-null: {original_count:,}\")\n",
    "        print(f\"  Successfully cleaned: {clean_count:,}\")\n",
    "        print(f\"  Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    return demo_clean\n",
    "\n",
    "def clean_essential_age_only(demo_df, available_essential):\n",
    "    \"\"\"\n",
    "    Clean ONLY essential age columns\n",
    "    \"\"\"\n",
    "    print(f\"👶 Cleaning essential age data\")\n",
    "    \n",
    "    demo_clean = demo_df.copy()\n",
    "    \n",
    "    # Check if essential age columns exist\n",
    "    has_age = 'age' in available_essential\n",
    "    has_age_cod = 'age_cod' in available_essential\n",
    "    \n",
    "    if not has_age:\n",
    "        print(f\"⚠️  Essential age column not found\")\n",
    "        return demo_clean\n",
    "    \n",
    "    print(f\"Found age columns: age={has_age}, age_cod={has_age_cod}\")\n",
    "    \n",
    "    # Convert age to numeric\n",
    "    demo_clean['age'] = pd.to_numeric(demo_clean['age'], errors='coerce')\n",
    "    \n",
    "    if has_age_cod:\n",
    "        # Standardize age unit codes\n",
    "        demo_clean['age_cod'] = demo_clean['age_cod'].astype(str).str.upper()\n",
    "        \n",
    "        # Simple age conversion - most common units only\n",
    "        age_conversions = {\n",
    "            'YR': 1.0,      # Years\n",
    "            'MON': 1/12,    # Months \n",
    "            'DY': 1/365,    # Days\n",
    "            'DEC': 10.0,    # Decades\n",
    "            'WK': 1/52,     # Weeks\n",
    "        }\n",
    "        \n",
    "        # Vectorized conversion for speed\n",
    "        def convert_age_to_years(row):\n",
    "            age_val = row['age']\n",
    "            age_unit = row['age_cod']\n",
    "            \n",
    "            if pd.isna(age_val) or pd.isna(age_unit):\n",
    "                return None\n",
    "            \n",
    "            if age_unit in age_conversions:\n",
    "                converted_age = age_val * age_conversions[age_unit]\n",
    "                if 0 <= converted_age <= 120:\n",
    "                    return round(converted_age, 2)\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        demo_clean['age_in_years'] = demo_clean.apply(convert_age_to_years, axis=1)\n",
    "    else:\n",
    "        # If no age_cod, assume age is already in years\n",
    "        demo_clean['age_in_years'] = demo_clean['age'].apply(\n",
    "            lambda x: round(x, 2) if pd.notna(x) and 0 <= x <= 120 else None\n",
    "        )\n",
    "    \n",
    "    # Simple age groups\n",
    "    demo_clean['age_group'] = pd.cut(\n",
    "        demo_clean['age_in_years'], \n",
    "        bins=[0, 18, 35, 65, 120], \n",
    "        labels=['Child', 'Young Adult', 'Middle-aged', 'Senior'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    # Quick stats\n",
    "    original_count = demo_clean['age'].notna().sum()\n",
    "    converted_count = demo_clean['age_in_years'].notna().sum()\n",
    "    success_rate = (converted_count / original_count * 100) if original_count > 0 else 0\n",
    "    \n",
    "    print(f\"  Age conversion success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    return demo_clean\n",
    "\n",
    "def clean_essential_sex_only(demo_df, available_essential):\n",
    "    \"\"\"\n",
    "    Clean ONLY essential sex column\n",
    "    \"\"\"\n",
    "    print(f\"⚥ Cleaning essential sex data\")\n",
    "    \n",
    "    demo_clean = demo_df.copy()\n",
    "    \n",
    "    if 'sex' not in available_essential:\n",
    "        print(f\"⚠️  Essential sex column not found\")\n",
    "        return demo_clean\n",
    "    \n",
    "    # Simple sex standardization\n",
    "    demo_clean['sex'] = demo_clean['sex'].astype(str).str.upper().str.strip()\n",
    "    \n",
    "    # Quick mapping\n",
    "    sex_map = {'M': 'M', 'F': 'F'}\n",
    "    demo_clean['sex_standardized'] = demo_clean['sex'].map(sex_map).fillna('U')\n",
    "    \n",
    "    # Quick stats\n",
    "    sex_counts = demo_clean['sex_standardized'].value_counts()\n",
    "    total = len(demo_clean)\n",
    "    \n",
    "    print(f\"  Sex distribution:\")\n",
    "    for sex, count in sex_counts.items():\n",
    "        pct = (count / total) * 100\n",
    "        print(f\"    {sex}: {pct:.1f}%\")\n",
    "    \n",
    "    return demo_clean\n",
    "\n",
    "def save_large_csv_chunked(df, output_path, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Save large DataFrame to CSV in chunks to avoid memory issues and KeyboardInterrupt\n",
    "    \"\"\"\n",
    "    print(f\"💾 Saving {len(df):,} rows in chunks of {chunk_size:,} to avoid crashes...\")\n",
    "    \n",
    "    try:\n",
    "        # Save header first\n",
    "        df.head(0).to_csv(output_path, index=False)\n",
    "        \n",
    "        # Save in chunks\n",
    "        total_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size > 0 else 0)\n",
    "        \n",
    "        for i in range(0, len(df), chunk_size):\n",
    "            chunk_num = i // chunk_size + 1\n",
    "            chunk = df.iloc[i:i+chunk_size]\n",
    "            chunk.to_csv(output_path, mode='a', header=False, index=False)\n",
    "            print(f\"  ✅ Saved chunk {chunk_num}/{total_chunks}: rows {i:,} to {min(i+chunk_size, len(df)):,}\")\n",
    "        \n",
    "        print(f\"✅ Successfully saved all {len(df):,} rows to {output_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving file: {e}\")\n",
    "        return False\n",
    "\n",
    "def clean_demographics_all_columns():\n",
    "    \"\"\"\n",
    "    Clean demographics with ALL columns preserved, but only standardize essential ones\n",
    "    \"\"\"\n",
    "    print(\"🚀 FAERS DEMOGRAPHICS - KEEP ALL COLUMNS, CLEAN ESSENTIALS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Strategy: Load everything, clean only important columns!\")\n",
    "    \n",
    "    # Step 1: Load ALL columns\n",
    "    demo_df, available_essential = load_all_columns()\n",
    "    if demo_df is None:\n",
    "        return None\n",
    "    \n",
    "    original_rows = len(demo_df)\n",
    "    original_cols = len(demo_df.columns)\n",
    "    \n",
    "    # Step 2: Check data quality (essential columns only)\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    demo_clean = check_data_quality_essential_only(demo_df, available_essential)\n",
    "    \n",
    "    # Step 3: REMOVE poor quality rows automatically\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(\"🗑️  REMOVING POOR QUALITY ROWS...\")\n",
    "    \n",
    "    rows_to_remove = demo_clean['data_quality_flag'].isin(['REMOVE_NO_ID', 'REMOVE_TOO_EMPTY'])\n",
    "    removed_count = rows_to_remove.sum()\n",
    "    \n",
    "    demo_clean = demo_clean[~rows_to_remove].copy()\n",
    "    \n",
    "    print(f\"  Removed {removed_count:,} poor quality rows\")\n",
    "    print(f\"  Remaining: {len(demo_clean):,} rows ({((len(demo_clean)/original_rows)*100):.1f}% of original)\")\n",
    "    \n",
    "    # Step 4: Clean essential dates only\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    demo_clean = clean_essential_dates_only(demo_clean, available_essential)\n",
    "    \n",
    "    # Step 5: Clean essential age only\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    demo_clean = clean_essential_age_only(demo_clean, available_essential)\n",
    "    \n",
    "    # Step 6: Clean essential sex only\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    demo_clean = clean_essential_sex_only(demo_clean, available_essential)\n",
    "    \n",
    "    # Step 7: Save ALL data with cleaned essentials\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(\"💾 Saving ALL COLUMNS with cleaned essentials...\")\n",
    "    \n",
    "    # Always use chunked saving to prevent crashes\n",
    "    save_success = save_large_csv_chunked(demo_clean, OUTPUT_FILE_PATH, chunk_size=50000)\n",
    "    \n",
    "    if save_success:\n",
    "        print(f\"\\n✅ COMPLETE SUCCESS!\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Error during saving - but cleaning completed successfully\")\n",
    "    \n",
    "    # Final summary\n",
    "    final_rows = len(demo_clean)\n",
    "    final_cols = len(demo_clean.columns)\n",
    "    removal_rate = (removed_count / original_rows) * 100\n",
    "    \n",
    "    print(f\"\\nFINAL SUMMARY:\")\n",
    "    print(f\"  Original records: {original_rows:,}\")\n",
    "    print(f\"  Original columns: {original_cols}\")\n",
    "    print(f\"  🗑️  Removed (poor quality): {removed_count:,} ({removal_rate:.1f}%)\")\n",
    "    print(f\"  ✅ Final clean records: {final_rows:,}\")\n",
    "    print(f\"  ✅ Final columns: {final_cols} (all preserved + new clean columns)\")\n",
    "    \n",
    "    # Show cleaning results for available essential columns\n",
    "    if 'event_dt_standardized' in demo_clean.columns:\n",
    "        print(f\"  Valid event dates: {demo_clean['event_dt_standardized'].notna().sum():,}\")\n",
    "    if 'age_in_years' in demo_clean.columns:\n",
    "        print(f\"  Valid ages: {demo_clean['age_in_years'].notna().sum():,}\")\n",
    "    if 'sex_standardized' in demo_clean.columns:\n",
    "        print(f\"  Valid sex (M/F): {(demo_clean['sex_standardized'].isin(['M', 'F'])).sum():,}\")\n",
    "    \n",
    "    print(f\"  File saved to: {OUTPUT_FILE_PATH}\")\n",
    "    print(f\"\\n💡 Added columns:\")\n",
    "    print(f\"     - data_quality_flag & data_quality_score\")\n",
    "    print(f\"     - *_standardized columns for cleaned essential data\")\n",
    "    print(f\"     - age_in_years & age_group\")\n",
    "    \n",
    "    return demo_clean\n",
    "\n",
    "def preview_all_columns():\n",
    "    \"\"\"\n",
    "    Quick preview of ALL columns and which will be cleaned\n",
    "    \"\"\"\n",
    "    print(\"🔍 PREVIEW - ALL COLUMNS + CLEANING PLAN\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load just first 1000 rows to preview\n",
    "    try:\n",
    "        sample_df = pd.read_csv(DEMO_FILE_PATH, nrows=1000)\n",
    "        print(f\"Sample: {sample_df.shape[0]} rows, {sample_df.shape[1]} columns\")\n",
    "        \n",
    "        print(f\"\\nALL COLUMNS (🎯 = will be cleaned):\")\n",
    "        for i, col in enumerate(sample_df.columns, 1):\n",
    "            status = \"🎯\" if col in ESSENTIAL_COLUMNS_TO_CLEAN else \"📁\"\n",
    "            print(f\"  {i:2d}. {status} {col}\")\n",
    "        \n",
    "        print(f\"\\n📋 CLEANING SUMMARY:\")\n",
    "        essential_found = [col for col in ESSENTIAL_COLUMNS_TO_CLEAN if col in sample_df.columns]\n",
    "        essential_missing = [col for col in ESSENTIAL_COLUMNS_TO_CLEAN if col not in sample_df.columns]\n",
    "        \n",
    "        print(f\"  Will clean: {essential_found}\")\n",
    "        if essential_missing:\n",
    "            print(f\"  Missing (won't clean): {essential_missing}\")\n",
    "        \n",
    "        print(f\"\\nSample data:\")\n",
    "        print(sample_df[essential_found[:5]].head(3))\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading preview: {e}\")\n",
    "        return False\n",
    "\n",
    "# 🚀 MAIN EXECUTION\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🎯 KEEP ALL COLUMNS + CLEAN ESSENTIALS STRATEGY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Best of both worlds: Complete data + Fast cleaning!\")\n",
    "    \n",
    "    # Option 1: Preview first\n",
    "    print(\"\\n1️⃣ PREVIEW (see all columns + cleaning plan):\")\n",
    "    print(\"# preview_all_columns()\")\n",
    "    \n",
    "    # Option 2: Run complete cleaning \n",
    "    print(\"\\n2️⃣ RUN COMPLETE CLEANING:\")\n",
    "    print(\"# demo_cleaned = clean_demographics_all_columns()\")\n",
    "    \n",
    "    print(f\"\\n🎯 STRATEGY BENEFITS:\")\n",
    "    print(\"✅ ALL original columns preserved\")\n",
    "    print(\"✅ Only essential columns cleaned (faster)\")\n",
    "    print(\"✅ Quality assessment on essential data\")\n",
    "    print(\"✅ Automatic removal of junk rows\")\n",
    "    print(\"✅ Chunked saving (no crashes)\")\n",
    "    print(\"✅ New standardized columns added\")\n",
    "    \n",
    "    print(f\"\\n📊 RESULT:\")\n",
    "    print(\"Complete dataset with all original data PLUS cleaned essential columns\")\n",
    "    \n",
    "    # Uncomment to run:\n",
    "    preview_all_columns()\n",
    "    demo_cleaned = clean_demographics_all_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09349eb1",
   "metadata": {},
   "source": [
    " This code removes duplicate drug records from your massive 9GB file by keeping only the newest version of each case, shrinking the file size and giving you cleaner data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f0da7",
   "metadata": {},
   "source": [
    "This still removes duplicate drug/patient records by keeping only the newest version of each case, but now it won't crash when your files are too massive to read normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442b18a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛡️ TIMEOUT-SAFE FAERS DEDUPLICATION\n",
      "======================================================================\n",
      "Handles massive files without timeout errors!\n",
      "\n",
      "🔧 SAFETY FEATURES:\n",
      "✅ Tiny sample sizes (50-1000 rows) for analysis\n",
      "✅ Small chunk sizes (10K rows) for processing\n",
      "✅ Aggressive memory cleanup\n",
      "✅ Timeout-resistant file reading\n",
      "✅ Progress tracking every 100 chunks\n",
      "\n",
      "🚀 SAFE EXECUTION:\n",
      "# demo_caseid, demo_version, drug_caseid, drug_version = run_safe_deduplication()\n",
      "🛡️ TIMEOUT-SAFE FAERS DEDUPLICATION\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 1: ANALYZING CLEANED DEMOGRAPHICS\n",
      "\n",
      "🔍 SAFE ANALYSIS: CLEANED DEMOGRAPHICS\n",
      "==================================================\n",
      "🔍 Quick check: faers_demographics_cleaned.csv\n",
      "✅ File exists: 3.44 GB\n",
      "📋 Reading header...\n",
      "✅ Found 39 columns\n",
      "📊 Reading tiny sample (50 rows)...\n",
      "✅ Sample loaded: (50, 39)\n",
      "🎯 Looking for deduplication columns...\n",
      "✅ Found case ID: caseid\n",
      "✅ Found version: caseversion\n",
      "\n",
      "📋 Sample data:\n",
      "     caseid  caseversion\n",
      "0  10004694            2\n",
      "1  10004820            6\n",
      "2  10004862            2\n",
      "3  10005135            2\n",
      "4  10005138            2\n",
      "\n",
      "📈 Versions found in sample: [2, 3, 4, 5, 6, 25, 29]\n",
      "\n",
      "📊 SAFE DUPLICATE ESTIMATION\n",
      "Using tiny sample of 500 rows to avoid timeout...\n",
      "📈 QUICK ESTIMATE (tiny sample):\n",
      "  Sample size: 500\n",
      "  Unique cases: 500\n",
      "  Estimated duplicate rate: 0.0%\n",
      "  Version distribution: {2: 123, 3: 93, 4: 71, 5: 41, 6: 22}\n",
      "\n",
      "🚀 Ready to deduplicate demographics\n",
      "# demo_result = deduplicate_small_file_fast(DEMO_CLEANED_PATH, DEMO_DEDUPE_PATH, 'caseid', 'caseversion', 'demographics')\n",
      "\n",
      "======================================================================\n",
      "STEP 2: ANALYZING DRUG FILE\n",
      "\n",
      "🔍 SAFE ANALYSIS: DRUG\n",
      "==================================================\n",
      "🔍 Quick check: faers_drugs_combined_2020_2024.csv\n",
      "✅ File exists: 8.77 GB\n",
      "📋 Reading header...\n",
      "✅ Found 23 columns\n",
      "📊 Reading tiny sample (50 rows)...\n",
      "✅ Sample loaded: (50, 23)\n",
      "🎯 Looking for deduplication columns...\n",
      "✅ Found case ID: caseid\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FILE PATHS - Using your cleaned demographics file\n",
    "DEMO_CLEANED_PATH = \"/Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/processed_cleaned/faers_demographics_cleaned.csv\"\n",
    "DEMO_DEDUPE_PATH = \"/Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/processed_cleaned/faers_demographics_deduplicated.csv\"\n",
    "\n",
    "DRUG_INPUT_PATH = \"/Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/processed/faers_drugs_combined_2020_2024.csv\"\n",
    "DRUG_DEDUPE_PATH = \"/Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/processed_cleaned/faers_drugs_deduplicated.csv\"\n",
    "\n",
    "def quick_file_check(file_path):\n",
    "    \"\"\"\n",
    "    Quick file accessibility and size check\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Quick check: {os.path.basename(file_path)}\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"❌ File not found\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        file_size_gb = os.path.getsize(file_path) / (1024**3)\n",
    "        print(f\"✅ File exists: {file_size_gb:.2f} GB\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ File access error: {e}\")\n",
    "        return False\n",
    "\n",
    "def safe_analyze_file(file_path, file_type, tiny_sample=100):\n",
    "    \"\"\"\n",
    "    TIMEOUT-RESISTANT file analysis with very small samples\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 SAFE ANALYSIS: {file_type.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not quick_file_check(file_path):\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # STEP 1: Try to read just header first\n",
    "        print(f\"📋 Reading header...\")\n",
    "        header_df = pd.read_csv(file_path, nrows=0, low_memory=False)\n",
    "        columns = list(header_df.columns)\n",
    "        print(f\"✅ Found {len(columns)} columns\")\n",
    "        \n",
    "        # STEP 2: Try tiny sample\n",
    "        print(f\"📊 Reading tiny sample ({tiny_sample} rows)...\")\n",
    "        tiny_df = pd.read_csv(file_path, nrows=tiny_sample, low_memory=False)\n",
    "        print(f\"✅ Sample loaded: {tiny_df.shape}\")\n",
    "        \n",
    "        # STEP 3: Find deduplication columns\n",
    "        print(f\"🎯 Looking for deduplication columns...\")\n",
    "        \n",
    "        caseid_col = None\n",
    "        caseversion_col = None\n",
    "        \n",
    "        # Look for case ID (prioritize caseid over primaryid)\n",
    "        caseid_candidates = ['caseid', 'primaryid', 'case_id', 'isr']\n",
    "        for candidate in caseid_candidates:\n",
    "            if candidate in columns:\n",
    "                caseid_col = candidate\n",
    "                print(f\"✅ Found case ID: {caseid_col}\")\n",
    "                break\n",
    "        \n",
    "        # Look for version\n",
    "        version_candidates = ['caseversion', 'version', 'case_version']\n",
    "        for candidate in version_candidates:\n",
    "            if candidate in columns:\n",
    "                caseversion_col = candidate\n",
    "                print(f\"✅ Found version: {caseversion_col}\")\n",
    "                break\n",
    "        \n",
    "        if caseid_col and caseversion_col:\n",
    "            # Show tiny sample of key columns\n",
    "            print(f\"\\n📋 Sample data:\")\n",
    "            key_sample = tiny_df[[caseid_col, caseversion_col]].head(5)\n",
    "            print(key_sample)\n",
    "            \n",
    "            # Quick version check\n",
    "            versions = tiny_df[caseversion_col].unique()\n",
    "            print(f\"\\n📈 Versions found in sample: {sorted(versions)}\")\n",
    "            \n",
    "        return caseid_col, caseversion_col\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in safe analysis: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def estimate_duplicates_safely(file_path, caseid_col, caseversion_col, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Safe duplicate estimation with very small sample\n",
    "    \"\"\"\n",
    "    print(f\"\\n📊 SAFE DUPLICATE ESTIMATION\")\n",
    "    print(f\"Using tiny sample of {sample_size} rows to avoid timeout...\")\n",
    "    \n",
    "    try:\n",
    "        # Use very small sample to avoid timeout\n",
    "        sample_df = pd.read_csv(file_path, nrows=sample_size, low_memory=False)\n",
    "        \n",
    "        total_rows = len(sample_df)\n",
    "        unique_cases = sample_df[caseid_col].nunique()\n",
    "        duplicate_rate = (total_rows - unique_cases) / total_rows * 100\n",
    "        \n",
    "        print(f\"📈 QUICK ESTIMATE (tiny sample):\")\n",
    "        print(f\"  Sample size: {total_rows}\")\n",
    "        print(f\"  Unique cases: {unique_cases}\")\n",
    "        print(f\"  Estimated duplicate rate: {duplicate_rate:.1f}%\")\n",
    "        \n",
    "        # Version analysis\n",
    "        sample_df[caseversion_col] = pd.to_numeric(sample_df[caseversion_col], errors='coerce')\n",
    "        version_counts = sample_df[caseversion_col].value_counts()\n",
    "        print(f\"  Version distribution: {dict(version_counts.head())}\")\n",
    "        \n",
    "        return duplicate_rate\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in estimation: {e}\")\n",
    "        return 0\n",
    "\n",
    "def deduplicate_small_file_fast(input_path, output_path, caseid_col, caseversion_col, file_type):\n",
    "    \"\"\"\n",
    "    Fast in-memory deduplication for smaller files (like cleaned demographics)\n",
    "    \"\"\"\n",
    "    print(f\"\\n🚀 FAST DEDUPLICATION: {file_type.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Load entire file (assuming it's not too large)\n",
    "        print(f\"📊 Loading entire file...\")\n",
    "        df = pd.read_csv(input_path, low_memory=False)\n",
    "        print(f\"✅ Loaded: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "        \n",
    "        # Convert version to numeric for proper sorting\n",
    "        print(f\"🔄 Converting versions to numeric...\")\n",
    "        df[caseversion_col] = pd.to_numeric(df[caseversion_col], errors='coerce')\n",
    "        \n",
    "        # Keep only latest version per case\n",
    "        print(f\"🎯 Keeping latest version per case...\")\n",
    "        df_dedupe = df.sort_values([caseid_col, caseversion_col]).groupby(caseid_col).tail(1)\n",
    "        \n",
    "        # Save result\n",
    "        print(f\"💾 Saving deduplicated file...\")\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        df_dedupe.to_csv(output_path, index=False)\n",
    "        \n",
    "        # Statistics\n",
    "        original_rows = len(df)\n",
    "        final_rows = len(df_dedupe)\n",
    "        removed_rows = original_rows - final_rows\n",
    "        removal_rate = (removed_rows / original_rows) * 100\n",
    "        \n",
    "        input_size_mb = os.path.getsize(input_path) / (1024**2)\n",
    "        output_size_mb = os.path.getsize(output_path) / (1024**2)\n",
    "        size_reduction = ((input_size_mb - output_size_mb) / input_size_mb) * 100\n",
    "        \n",
    "        print(f\"\\n✅ DEDUPLICATION COMPLETE!\")\n",
    "        print(f\"📊 RESULTS:\")\n",
    "        print(f\"  Original rows: {original_rows:,}\")\n",
    "        print(f\"  Final rows: {final_rows:,}\")\n",
    "        print(f\"  Removed: {removed_rows:,} ({removal_rate:.1f}%)\")\n",
    "        print(f\"  Size: {input_size_mb:.1f} MB → {output_size_mb:.1f} MB ({size_reduction:.1f}% reduction)\")\n",
    "        print(f\"  Output: {output_path}\")\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'original_rows': original_rows,\n",
    "            'final_rows': final_rows,\n",
    "            'removal_rate': removal_rate,\n",
    "            'size_reduction': size_reduction\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during deduplication: {e}\")\n",
    "        return {'success': False, 'error': str(e)}\n",
    "\n",
    "def deduplicate_massive_file_safe(input_path, output_path, caseid_col, caseversion_col, file_type):\n",
    "    \"\"\"\n",
    "    Safe chunked deduplication for massive files (like 9GB drug file)\n",
    "    \"\"\"\n",
    "    print(f\"\\n🚀 MASSIVE FILE DEDUPLICATION: {file_type.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"⚠️  This will take 30-60 minutes for 9GB file\")\n",
    "    \n",
    "    # Get file size\n",
    "    file_size_gb = os.path.getsize(input_path) / (1024**3)\n",
    "    print(f\"📁 File size: {file_size_gb:.2f} GB\")\n",
    "    \n",
    "    # Use very small chunks to avoid timeout\n",
    "    chunk_size = 10000  # Extra small for safety\n",
    "    \n",
    "    try:\n",
    "        # PHASE 1: Find latest versions\n",
    "        print(f\"\\n📋 PHASE 1: Finding latest versions (chunk size: {chunk_size:,})...\")\n",
    "        \n",
    "        latest_versions = {}\n",
    "        total_scanned = 0\n",
    "        chunk_count = 0\n",
    "        \n",
    "        chunk_iterator = pd.read_csv(input_path, chunksize=chunk_size, low_memory=False)\n",
    "        \n",
    "        for chunk in chunk_iterator:\n",
    "            chunk_count += 1\n",
    "            \n",
    "            # Convert version to numeric\n",
    "            chunk[caseversion_col] = pd.to_numeric(chunk[caseversion_col], errors='coerce')\n",
    "            \n",
    "            # Find max version per case in chunk\n",
    "            chunk_latest = chunk.groupby(caseid_col)[caseversion_col].max()\n",
    "            \n",
    "            # Update global latest versions\n",
    "            for caseid, version in chunk_latest.items():\n",
    "                if pd.notna(version):\n",
    "                    if caseid not in latest_versions or version > latest_versions[caseid]:\n",
    "                        latest_versions[caseid] = version\n",
    "            \n",
    "            total_scanned += len(chunk)\n",
    "            \n",
    "            # Progress every 100 chunks (less frequent to avoid spam)\n",
    "            if chunk_count % 100 == 0:\n",
    "                print(f\"    {chunk_count} chunks, {total_scanned:,} rows, {len(latest_versions):,} unique cases...\")\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "        \n",
    "        print(f\"✅ Phase 1 complete: {len(latest_versions):,} unique cases found\")\n",
    "        \n",
    "        # PHASE 2: Filter and save\n",
    "        print(f\"\\n💾 PHASE 2: Filtering and saving...\")\n",
    "        \n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # Save header\n",
    "        header_df = pd.read_csv(input_path, nrows=0)\n",
    "        header_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        kept_rows = 0\n",
    "        total_processed = 0\n",
    "        chunk_count = 0\n",
    "        \n",
    "        chunk_iterator = pd.read_csv(input_path, chunksize=chunk_size, low_memory=False)\n",
    "        \n",
    "        for chunk in chunk_iterator:\n",
    "            chunk_count += 1\n",
    "            chunk[caseversion_col] = pd.to_numeric(chunk[caseversion_col], errors='coerce')\n",
    "            \n",
    "            # Filter efficiently\n",
    "            mask = (\n",
    "                chunk[caseid_col].isin(latest_versions.keys()) &\n",
    "                chunk.apply(lambda row: (\n",
    "                    pd.notna(row[caseversion_col]) and \n",
    "                    row[caseversion_col] == latest_versions.get(row[caseid_col], -1)\n",
    "                ), axis=1)\n",
    "            )\n",
    "            \n",
    "            filtered_chunk = chunk[mask]\n",
    "            \n",
    "            if not filtered_chunk.empty:\n",
    "                filtered_chunk.to_csv(output_path, mode='a', header=False, index=False)\n",
    "                kept_rows += len(filtered_chunk)\n",
    "            \n",
    "            total_processed += len(chunk)\n",
    "            \n",
    "            if chunk_count % 100 == 0:\n",
    "                print(f\"    {chunk_count} chunks, kept {kept_rows:,}/{total_processed:,} rows...\")\n",
    "            \n",
    "            del chunk, filtered_chunk, mask\n",
    "            gc.collect()\n",
    "        \n",
    "        # Final stats\n",
    "        removed_rows = total_processed - kept_rows\n",
    "        removal_rate = (removed_rows / total_processed) * 100\n",
    "        \n",
    "        output_size_gb = os.path.getsize(output_path) / (1024**3)\n",
    "        size_reduction = ((file_size_gb - output_size_gb) / file_size_gb) * 100\n",
    "        \n",
    "        print(f\"\\n🎉 MASSIVE FILE DEDUPLICATION COMPLETE!\")\n",
    "        print(f\"📊 FINAL RESULTS:\")\n",
    "        print(f\"  Input: {total_processed:,} rows ({file_size_gb:.2f} GB)\")\n",
    "        print(f\"  Output: {kept_rows:,} rows ({output_size_gb:.2f} GB)\")\n",
    "        print(f\"  Removed: {removed_rows:,} ({removal_rate:.1f}%)\")\n",
    "        print(f\"  Size reduction: {size_reduction:.1f}%\")\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'original_rows': total_processed,\n",
    "            'final_rows': kept_rows,\n",
    "            'removal_rate': removal_rate,\n",
    "            'size_reduction': size_reduction\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during massive deduplication: {e}\")\n",
    "        return {'success': False, 'error': str(e)}\n",
    "\n",
    "def run_safe_deduplication():\n",
    "    \"\"\"\n",
    "    Safe deduplication workflow that handles timeouts\n",
    "    \"\"\"\n",
    "    print(\"🛡️ TIMEOUT-SAFE FAERS DEDUPLICATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # STEP 1: Analyze cleaned demographics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"STEP 1: ANALYZING CLEANED DEMOGRAPHICS\")\n",
    "    demo_caseid, demo_version = safe_analyze_file(DEMO_CLEANED_PATH, \"cleaned demographics\", tiny_sample=50)\n",
    "    \n",
    "    if demo_caseid and demo_version:\n",
    "        # Estimate duplicates\n",
    "        estimate_duplicates_safely(DEMO_CLEANED_PATH, demo_caseid, demo_version, sample_size=500)\n",
    "        \n",
    "        # Option to deduplicate\n",
    "        print(f\"\\n🚀 Ready to deduplicate demographics\")\n",
    "        print(f\"# demo_result = deduplicate_small_file_fast(DEMO_CLEANED_PATH, DEMO_DEDUPE_PATH, '{demo_caseid}', '{demo_version}', 'demographics')\")\n",
    "    \n",
    "    # STEP 2: Analyze drug file\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"STEP 2: ANALYZING DRUG FILE\") \n",
    "    drug_caseid, drug_version = safe_analyze_file(DRUG_INPUT_PATH, \"drug\", tiny_sample=50)\n",
    "    \n",
    "    if drug_caseid and drug_version:\n",
    "        # Estimate duplicates\n",
    "        estimate_duplicates_safely(DRUG_INPUT_PATH, drug_caseid, drug_version, sample_size=500)\n",
    "        \n",
    "        # Option to deduplicate\n",
    "        print(f\"\\n🚀 Ready to deduplicate drug file\")\n",
    "        print(f\"# drug_result = deduplicate_massive_file_safe(DRUG_INPUT_PATH, DRUG_DEDUPE_PATH, '{drug_caseid}', '{drug_version}', 'drug')\")\n",
    "    \n",
    "    return demo_caseid, demo_version, drug_caseid, drug_version\n",
    "\n",
    "# 🚀 MAIN EXECUTION\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🛡️ TIMEOUT-SAFE FAERS DEDUPLICATION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Handles massive files without timeout errors!\")\n",
    "    \n",
    "    print(f\"\\n🔧 SAFETY FEATURES:\")\n",
    "    print(\"✅ Tiny sample sizes (50-1000 rows) for analysis\")\n",
    "    print(\"✅ Small chunk sizes (10K rows) for processing\")\n",
    "    print(\"✅ Aggressive memory cleanup\")\n",
    "    print(\"✅ Timeout-resistant file reading\")\n",
    "    print(\"✅ Progress tracking every 100 chunks\")\n",
    "    \n",
    "    print(f\"\\n🚀 SAFE EXECUTION:\")\n",
    "    print(\"# demo_caseid, demo_version, drug_caseid, drug_version = run_safe_deduplication()\")\n",
    "    \n",
    "    # Uncomment to run safe analysis:\n",
    "    demo_caseid, demo_version, drug_caseid, drug_version = run_safe_deduplication()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff35829b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 FAST DEDUPLICATION: DEMOGRAPHICS\n",
      "==================================================\n",
      "📊 Loading entire file...\n",
      "✅ Loaded: 15,876,714 rows, 39 columns\n",
      "🔄 Converting versions to numeric...\n",
      "🎯 Keeping latest version per case...\n",
      "💾 Saving deduplicated file...\n",
      "\n",
      "✅ DEDUPLICATION COMPLETE!\n",
      "📊 RESULTS:\n",
      "  Original rows: 15,876,714\n",
      "  Final rows: 6,775,056\n",
      "  Removed: 9,101,658 (57.3%)\n",
      "  Size: 3522.7 MB → 1508.3 MB (57.2% reduction)\n",
      "  Output: /Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/processed_cleaned/faers_demographics_deduplicated.csv\n"
     ]
    }
   ],
   "source": [
    "demo_result = deduplicate_small_file_fast(DEMO_CLEANED_PATH, DEMO_DEDUPE_PATH, 'caseid', 'caseversion', 'demographics')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c0494",
   "metadata": {},
   "source": [
    "Instead of keeping all the drugs a patient takes (like vitamins, blood pressure meds, etc.), this code only keeps the ONE drug that doctors think actually caused the bad reaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2c0d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💊 PRIMARY SUSPECT DRUG FILTERING\n",
      "======================================================================\n",
      "Strategy: Keep only ROLE_COD = 'PS' drugs\n",
      "Result: One suspected culprit drug per case\n",
      "\n",
      "🎯 WHY THIS IS BETTER:\n",
      "✅ Focuses on the actual problem drug\n",
      "✅ Eliminates noise from unrelated medications\n",
      "✅ Perfect for menstrual disturbance analysis\n",
      "✅ Much smaller, cleaner dataset\n",
      "\n",
      "🚀 TO START:\n",
      "# role_col = run_primary_suspect_filtering()\n",
      "🎯 PRIMARY SUSPECT DRUG FILTERING\n",
      "======================================================================\n",
      "Keep only the ONE drug most likely to cause adverse events per case\n",
      "🔍 CHECKING ROLE_COD COLUMN\n",
      "==================================================\n",
      "📁 Drug file size: 8.77 GB\n",
      "📊 Reading sample to check ROLE_COD...\n",
      "Sample: 10,000 rows, 23 columns\n",
      "✅ Found role column: role_cod\n",
      "\n",
      "📈 Role distribution in sample:\n",
      "  SS: 4,431 (44.3%) - Secondary Suspect\n",
      "  C: 4,353 (43.5%) - Concomitant\n",
      "  PS: 1,152 (11.5%) - Primary Suspect\n",
      "  I: 64 (0.6%) - Interacting\n",
      "\n",
      "🎯 PRIMARY SUSPECTS:\n",
      "  Count in sample: 1,152\n",
      "  Percentage: 11.5%\n",
      "  Estimated in full file: ~1.0 GB\n",
      "\n",
      "⚠️  READY TO FILTER 8.77 GB DRUG FILE\n",
      "This will take 20-40 minutes but create much cleaner data\n",
      "Ready to proceed? Uncomment line below:\n",
      "# result = filter_primary_suspects_chunked('role_cod')\n",
      "\n",
      "💊 FILTERING TO PRIMARY SUSPECTS ONLY\n",
      "============================================================\n",
      "Strategy: Keep only ROLE_COD = 'PS' (Primary Suspect drugs)\n",
      "Chunk size: 50,000 rows\n",
      "📋 Setting up output file...\n",
      "🔄 Processing file in chunks...\n",
      "    Processed 100 chunks, ~0.0GB\n",
      "    Kept 1,153,670 primary suspects from 5,000,000 total drugs...\n",
      "    Processed 200 chunks, ~0.0GB\n",
      "    Kept 2,279,378 primary suspects from 10,000,000 total drugs...\n",
      "    Processed 300 chunks, ~0.1GB\n",
      "    Kept 3,467,365 primary suspects from 15,000,000 total drugs...\n",
      "    Processed 400 chunks, ~0.1GB\n",
      "    Kept 4,483,069 primary suspects from 20,000,000 total drugs...\n",
      "    Processed 500 chunks, ~0.1GB\n",
      "    Kept 5,518,640 primary suspects from 25,000,000 total drugs...\n",
      "    Processed 600 chunks, ~0.1GB\n",
      "    Kept 6,693,695 primary suspects from 30,000,000 total drugs...\n",
      "    Processed 700 chunks, ~0.1GB\n",
      "    Kept 7,805,330 primary suspects from 35,000,000 total drugs...\n",
      "    Processed 800 chunks, ~0.1GB\n",
      "    Kept 9,036,028 primary suspects from 40,000,000 total drugs...\n",
      "    Processed 900 chunks, ~0.2GB\n",
      "    Kept 10,221,194 primary suspects from 45,000,000 total drugs...\n",
      "    Processed 1000 chunks, ~0.2GB\n",
      "    Kept 11,364,283 primary suspects from 50,000,000 total drugs...\n",
      "    Processed 1100 chunks, ~0.2GB\n",
      "    Kept 12,546,053 primary suspects from 55,000,000 total drugs...\n",
      "    Processed 1200 chunks, ~0.2GB\n",
      "    Kept 13,644,570 primary suspects from 60,000,000 total drugs...\n",
      "    Processed 1300 chunks, ~0.2GB\n",
      "    Kept 14,695,629 primary suspects from 65,000,000 total drugs...\n",
      "    Processed 1400 chunks, ~0.3GB\n",
      "    Kept 15,810,973 primary suspects from 70,000,000 total drugs...\n",
      "    Processed 1500 chunks, ~0.3GB\n",
      "    Kept 16,835,493 primary suspects from 75,000,000 total drugs...\n",
      "\n",
      "🎉 PRIMARY SUSPECT FILTERING COMPLETE!\n",
      "============================================================\n",
      "📊 FILTERING RESULTS:\n",
      "  Total input drugs: 77,995,018\n",
      "  Primary suspects kept: 17,530,708\n",
      "  Secondary/other removed: 60,464,310\n",
      "  Filter rate: 77.5%\n",
      "  Input size: 8.77 GB\n",
      "  Output size: 2.24 GB\n",
      "  Size reduction: 74.5%\n",
      "  Output file: /Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/processed_cleaned/faers_drugs_primary_suspects.csv\n",
      "\n",
      "💡 RESULT:\n",
      "Now you have ONE suspected culprit drug per case!\n",
      "Perfect for analyzing drug → menstrual disturbance relationships!\n",
      "\n",
      "📋 SAMPLE OF PRIMARY SUSPECT DRUGS\n",
      "==================================================\n",
      "Sample from filtered file:\n",
      "Rows: 10, Columns: 23\n",
      "\n",
      "Sample data:\n",
      "     caseid  drugname role_cod\n",
      "0  10004694   LIPITOR       PS\n",
      "1  10004820  DILANTIN       PS\n",
      "2  10004862   LIPITOR       PS\n",
      "3  10005135   LIPITOR       PS\n",
      "4  10005138   LIPITOR       PS\n",
      "\n",
      "Sample primary suspect drugs:\n",
      "  - LIPITOR\n",
      "  - DILANTIN\n",
      "  - SAMSCA\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FILE PATHS\n",
    "DRUG_INPUT_PATH = \"/Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/processed/faers_drugs_combined_2020_2024.csv\"\n",
    "DRUG_PRIMARY_PATH = \"/Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/processed_cleaned/faers_drugs_primary_suspects.csv\"\n",
    "\n",
    "def check_role_cod_column():\n",
    "    \"\"\"\n",
    "    Check if ROLE_COD column exists and what values it contains\n",
    "    \"\"\"\n",
    "    print(\"🔍 CHECKING ROLE_COD COLUMN\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not os.path.exists(DRUG_INPUT_PATH):\n",
    "        print(f\"❌ Drug file not found\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Check file size\n",
    "        file_size_gb = os.path.getsize(DRUG_INPUT_PATH) / (1024**3)\n",
    "        print(f\"📁 Drug file size: {file_size_gb:.2f} GB\")\n",
    "        \n",
    "        # Read small sample to check structure\n",
    "        print(f\"📊 Reading sample to check ROLE_COD...\")\n",
    "        sample_df = pd.read_csv(DRUG_INPUT_PATH, nrows=10000, low_memory=False)\n",
    "        \n",
    "        print(f\"Sample: {sample_df.shape[0]:,} rows, {sample_df.shape[1]} columns\")\n",
    "        \n",
    "        # Look for ROLE_COD column (or variations)\n",
    "        role_col = None\n",
    "        role_candidates = ['role_cod', 'rolecod', 'role_code', 'drug_role']\n",
    "        \n",
    "        for candidate in role_candidates:\n",
    "            if candidate in sample_df.columns:\n",
    "                role_col = candidate\n",
    "                break\n",
    "        \n",
    "        if not role_col:\n",
    "            print(f\"❌ No ROLE_COD column found\")\n",
    "            print(f\"Available columns: {list(sample_df.columns)}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"✅ Found role column: {role_col}\")\n",
    "        \n",
    "        # Show role distribution\n",
    "        role_counts = sample_df[role_col].value_counts()\n",
    "        print(f\"\\n📈 Role distribution in sample:\")\n",
    "        for role, count in role_counts.items():\n",
    "            pct = (count / len(sample_df)) * 100\n",
    "            description = {\n",
    "                'PS': 'Primary Suspect',\n",
    "                'SS': 'Secondary Suspect', \n",
    "                'C': 'Concomitant',\n",
    "                'I': 'Interacting'\n",
    "            }.get(role, 'Unknown')\n",
    "            print(f\"  {role}: {count:,} ({pct:.1f}%) - {description}\")\n",
    "        \n",
    "        # Count primary suspects\n",
    "        primary_count = (sample_df[role_col] == 'PS').sum()\n",
    "        primary_pct = (primary_count / len(sample_df)) * 100\n",
    "        \n",
    "        print(f\"\\n🎯 PRIMARY SUSPECTS:\")\n",
    "        print(f\"  Count in sample: {primary_count:,}\")\n",
    "        print(f\"  Percentage: {primary_pct:.1f}%\")\n",
    "        print(f\"  Estimated in full file: ~{(primary_pct/100) * file_size_gb:.1f} GB\")\n",
    "        \n",
    "        return role_col\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking ROLE_COD: {e}\")\n",
    "        return None\n",
    "\n",
    "def filter_primary_suspects_chunked(role_col, chunk_size=50000):\n",
    "    \"\"\"\n",
    "    Filter massive drug file to keep only Primary Suspect drugs\n",
    "    Uses chunked processing for 8GB+ files\n",
    "    \"\"\"\n",
    "    print(f\"\\n💊 FILTERING TO PRIMARY SUSPECTS ONLY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Strategy: Keep only ROLE_COD = 'PS' (Primary Suspect drugs)\")\n",
    "    print(f\"Chunk size: {chunk_size:,} rows\")\n",
    "    \n",
    "    try:\n",
    "        # Create output directory\n",
    "        os.makedirs(os.path.dirname(DRUG_PRIMARY_PATH), exist_ok=True)\n",
    "        \n",
    "        # Save header first\n",
    "        print(f\"📋 Setting up output file...\")\n",
    "        header_df = pd.read_csv(DRUG_INPUT_PATH, nrows=0)\n",
    "        header_df.to_csv(DRUG_PRIMARY_PATH, index=False)\n",
    "        \n",
    "        # Process file in chunks\n",
    "        print(f\"🔄 Processing file in chunks...\")\n",
    "        \n",
    "        total_processed = 0\n",
    "        primary_kept = 0\n",
    "        chunk_count = 0\n",
    "        \n",
    "        chunk_iterator = pd.read_csv(DRUG_INPUT_PATH, chunksize=chunk_size, low_memory=False)\n",
    "        \n",
    "        for chunk in chunk_iterator:\n",
    "            chunk_count += 1\n",
    "            \n",
    "            # Filter to primary suspects only\n",
    "            primary_chunk = chunk[chunk[role_col] == 'PS'].copy()\n",
    "            \n",
    "            # Save filtered chunk\n",
    "            if not primary_chunk.empty:\n",
    "                primary_chunk.to_csv(DRUG_PRIMARY_PATH, mode='a', header=False, index=False)\n",
    "                primary_kept += len(primary_chunk)\n",
    "            \n",
    "            total_processed += len(chunk)\n",
    "            \n",
    "            # Progress update every 100 chunks\n",
    "            if chunk_count % 100 == 0:\n",
    "                gb_processed = (total_processed * 4) / (1024**3)\n",
    "                print(f\"    Processed {chunk_count} chunks, ~{gb_processed:.1f}GB\")\n",
    "                print(f\"    Kept {primary_kept:,} primary suspects from {total_processed:,} total drugs...\")\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del chunk, primary_chunk\n",
    "            gc.collect()\n",
    "        \n",
    "        # Final statistics\n",
    "        filtered_out = total_processed - primary_kept\n",
    "        filter_rate = (filtered_out / total_processed) * 100\n",
    "        \n",
    "        # File size comparison\n",
    "        input_size_gb = os.path.getsize(DRUG_INPUT_PATH) / (1024**3)\n",
    "        output_size_gb = os.path.getsize(DRUG_PRIMARY_PATH) / (1024**3)\n",
    "        size_reduction = ((input_size_gb - output_size_gb) / input_size_gb) * 100\n",
    "        \n",
    "        print(f\"\\n🎉 PRIMARY SUSPECT FILTERING COMPLETE!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"📊 FILTERING RESULTS:\")\n",
    "        print(f\"  Total input drugs: {total_processed:,}\")\n",
    "        print(f\"  Primary suspects kept: {primary_kept:,}\")\n",
    "        print(f\"  Secondary/other removed: {filtered_out:,}\")\n",
    "        print(f\"  Filter rate: {filter_rate:.1f}%\")\n",
    "        print(f\"  Input size: {input_size_gb:.2f} GB\")\n",
    "        print(f\"  Output size: {output_size_gb:.2f} GB\")\n",
    "        print(f\"  Size reduction: {size_reduction:.1f}%\")\n",
    "        print(f\"  Output file: {DRUG_PRIMARY_PATH}\")\n",
    "        \n",
    "        print(f\"\\n💡 RESULT:\")\n",
    "        print(f\"Now you have ONE suspected culprit drug per case!\")\n",
    "        print(f\"Perfect for analyzing drug → menstrual disturbance relationships!\")\n",
    "        \n",
    "        return {\n",
    "            'total_drugs': total_processed,\n",
    "            'primary_suspects': primary_kept,\n",
    "            'filter_rate': filter_rate,\n",
    "            'size_reduction': size_reduction\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during filtering: {e}\")\n",
    "        return None\n",
    "\n",
    "def show_sample_primary_suspects():\n",
    "    \"\"\"\n",
    "    Show sample of filtered primary suspect drugs\n",
    "    \"\"\"\n",
    "    print(f\"\\n📋 SAMPLE OF PRIMARY SUSPECT DRUGS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not os.path.exists(DRUG_PRIMARY_PATH):\n",
    "        print(f\"❌ Primary suspects file not found. Run filtering first.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Load sample\n",
    "        sample_df = pd.read_csv(DRUG_PRIMARY_PATH, nrows=10)\n",
    "        \n",
    "        print(f\"Sample from filtered file:\")\n",
    "        print(f\"Rows: {sample_df.shape[0]}, Columns: {sample_df.shape[1]}\")\n",
    "        \n",
    "        # Show key columns\n",
    "        key_cols = ['caseid', 'drugname', 'role_cod']\n",
    "        available_cols = [col for col in key_cols if col in sample_df.columns]\n",
    "        \n",
    "        if available_cols:\n",
    "            print(f\"\\nSample data:\")\n",
    "            print(sample_df[available_cols].head())\n",
    "        \n",
    "        # Show unique drug names in sample\n",
    "        if 'drugname' in sample_df.columns:\n",
    "            unique_drugs = sample_df['drugname'].unique()\n",
    "            print(f\"\\nSample primary suspect drugs:\")\n",
    "            for drug in unique_drugs[:10]:\n",
    "                print(f\"  - {drug}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error showing sample: {e}\")\n",
    "\n",
    "def run_primary_suspect_filtering():\n",
    "    \"\"\"\n",
    "    Complete workflow to filter drug file to primary suspects only\n",
    "    \"\"\"\n",
    "    print(\"🎯 PRIMARY SUSPECT DRUG FILTERING\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Keep only the ONE drug most likely to cause adverse events per case\")\n",
    "    \n",
    "    # Step 1: Check if ROLE_COD exists\n",
    "    role_col = check_role_cod_column()\n",
    "    \n",
    "    if not role_col:\n",
    "        print(\"❌ Cannot proceed - no ROLE_COD column found\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Ask for confirmation\n",
    "    print(f\"\\n⚠️  READY TO FILTER 8.77 GB DRUG FILE\")\n",
    "    print(f\"This will take 20-40 minutes but create much cleaner data\")\n",
    "    print(f\"Ready to proceed? Uncomment line below:\")\n",
    "    print(f\"# result = filter_primary_suspects_chunked('{role_col}')\")\n",
    "    \n",
    "    return role_col\n",
    "\n",
    "# 🚀 MAIN EXECUTION\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"💊 PRIMARY SUSPECT DRUG FILTERING\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Strategy: Keep only ROLE_COD = 'PS' drugs\")\n",
    "    print(\"Result: One suspected culprit drug per case\")\n",
    "    \n",
    "    print(f\"\\n🎯 WHY THIS IS BETTER:\")\n",
    "    print(\"✅ Focuses on the actual problem drug\")\n",
    "    print(\"✅ Eliminates noise from unrelated medications\")\n",
    "    print(\"✅ Perfect for menstrual disturbance analysis\")\n",
    "    print(\"✅ Much smaller, cleaner dataset\")\n",
    "    \n",
    "    print(f\"\\n🚀 TO START:\")\n",
    "    print(\"# role_col = run_primary_suspect_filtering()\")\n",
    "    \n",
    "    # Uncomment to run:\n",
    "    role_col = run_primary_suspect_filtering()\n",
    "    result = filter_primary_suspects_chunked(role_col)\n",
    "    show_sample_primary_suspects()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d2a159",
   "metadata": {},
   "source": [
    "## Drug Names: This is a major task.\n",
    "\n",
    "Normalize: Convert all drug names to lowercase.\n",
    "\n",
    "Map to Generic Names: The same drug is reported under hundreds of brand names and misspellings. You need to map these to a standard generic name (the active ingredient). Use a drug dictionary like RxNorm for this. For example, \"Advil,\" \"Motrin,\" and \"Ibuprofen\" should all be mapped to \"ibuprofen.\" for drug df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff24f77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4150) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: requests in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (2.32.4)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from requests) (2025.7.14)\n",
      "Requirement already satisfied: six>=1.5 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ab961d",
   "metadata": {},
   "source": [
    "RXnorms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb84f685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['primaryid', 'caseid', 'drug_seq', 'role_cod', 'drugname', 'prod_ai',\n",
      "       'val_vbm', 'route', 'dose_vbm', 'cum_dose_chr', 'cum_dose_unit',\n",
      "       'dechal', 'rechal', 'lot_num', 'exp_dt', 'nda_num', 'dose_amt',\n",
      "       'dose_unit', 'dose_form', 'dose_freq', 'year', 'quarter',\n",
      "       'source_file'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 1. First, check what columns you have\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/processed_cleaned/faers_drugs_primary_suspects.csv', nrows=5)\n",
    "print(df.columns)\n",
    "\n",
    "# 2. Tell me what drug column name you have, and I'll help you run the optimized process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a50b03",
   "metadata": {},
   "source": [
    "RXnorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7af85f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "      STARTING FAERS DRUG STANDARDIZATION PIPELINE      \n",
      "      Input File: /Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/processed_cleaned/faers_drugs_primary_suspects.csv      \n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "STEP 1: EXTRACTING UNIQUE DRUGS\n",
      "============================================================\n",
      "Extraction Complete! Found 41,649 unique drugs in 27.20s.\n",
      "\n",
      "============================================================\n",
      "STEP 2: CALCULATING DRUG FREQUENCIES\n",
      "============================================================\n",
      "Frequency calculation complete in 38.87s.\n",
      "Top 10 most frequent drugs:\n",
      "                                drugname  frequency\n",
      "0                              DUPIXENT     625122\n",
      "1                                ZANTAC     594274\n",
      "2                                HUMIRA     394674\n",
      "3                              REVLIMID     319190\n",
      "4                              COSENTYX     219804\n",
      "5                             OXYCONTIN     199872\n",
      "6  PROACTIV MD ADAPALENE ACNE TREATMENT     190464\n",
      "7                               REPATHA     173372\n",
      "8                              ENTRESTO     151206\n",
      "9                            XELJANZ XR     145826\n",
      "\n",
      "============================================================\n",
      "STEP 3: MAPPING UNIQUE DRUGS TO RXNORM\n",
      "============================================================\n",
      "FAERSRxNormMapper initialized. Cache located at: 'faers_rxnorm_cache.db'\n",
      "  Progress: 50/41649 (0.1%) | Success: 100.0% | Rate: 2.5 drugs/sec\n",
      "  Progress: 100/41649 (0.2%) | Success: 100.0% | Rate: 2.6 drugs/sec\n",
      "  Progress: 150/41649 (0.4%) | Success: 100.0% | Rate: 2.6 drugs/sec\n",
      "  Progress: 200/41649 (0.5%) | Success: 99.5% | Rate: 2.6 drugs/sec\n",
      "  Progress: 250/41649 (0.6%) | Success: 99.6% | Rate: 2.6 drugs/sec\n",
      "  Progress: 300/41649 (0.7%) | Success: 99.3% | Rate: 2.6 drugs/sec\n",
      "  Progress: 350/41649 (0.8%) | Success: 99.1% | Rate: 2.6 drugs/sec\n",
      "  Progress: 400/41649 (1.0%) | Success: 99.2% | Rate: 2.6 drugs/sec\n",
      "  Progress: 450/41649 (1.1%) | Success: 99.3% | Rate: 2.6 drugs/sec\n",
      "  Progress: 500/41649 (1.2%) | Success: 99.4% | Rate: 2.6 drugs/sec\n",
      "  Progress: 550/41649 (1.3%) | Success: 99.5% | Rate: 2.6 drugs/sec\n",
      "  Progress: 600/41649 (1.4%) | Success: 99.3% | Rate: 2.6 drugs/sec\n",
      "  Progress: 650/41649 (1.6%) | Success: 99.4% | Rate: 2.6 drugs/sec\n",
      "  Progress: 700/41649 (1.7%) | Success: 99.4% | Rate: 2.6 drugs/sec\n",
      "  Progress: 750/41649 (1.8%) | Success: 99.2% | Rate: 2.6 drugs/sec\n",
      "  Progress: 800/41649 (1.9%) | Success: 99.2% | Rate: 2.6 drugs/sec\n",
      "  Progress: 850/41649 (2.0%) | Success: 99.2% | Rate: 2.6 drugs/sec\n",
      "  Progress: 900/41649 (2.2%) | Success: 99.2% | Rate: 2.6 drugs/sec\n",
      "  Progress: 950/41649 (2.3%) | Success: 99.3% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1000/41649 (2.4%) | Success: 99.3% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1050/41649 (2.5%) | Success: 99.2% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1100/41649 (2.6%) | Success: 99.3% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1150/41649 (2.8%) | Success: 99.3% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1200/41649 (2.9%) | Success: 99.3% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1250/41649 (3.0%) | Success: 99.4% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1300/41649 (3.1%) | Success: 99.4% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1350/41649 (3.2%) | Success: 99.3% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1400/41649 (3.4%) | Success: 99.3% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1450/41649 (3.5%) | Success: 99.3% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1500/41649 (3.6%) | Success: 99.3% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1550/41649 (3.7%) | Success: 99.3% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1600/41649 (3.8%) | Success: 99.2% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1650/41649 (4.0%) | Success: 99.2% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1700/41649 (4.1%) | Success: 99.2% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1750/41649 (4.2%) | Success: 99.3% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1800/41649 (4.3%) | Success: 99.3% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1850/41649 (4.4%) | Success: 99.3% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1900/41649 (4.6%) | Success: 99.3% | Rate: 2.6 drugs/sec\n",
      "  Progress: 1950/41649 (4.7%) | Success: 99.2% | Rate: 2.6 drugs/sec\n",
      "  Progress: 2000/41649 (4.8%) | Success: 99.2% | Rate: 2.6 drugs/sec\n",
      "  Progress: 2050/41649 (4.9%) | Success: 99.1% | Rate: 2.6 drugs/sec\n",
      "  Progress: 2100/41649 (5.0%) | Success: 99.1% | Rate: 2.6 drugs/sec\n",
      "  Progress: 2150/41649 (5.2%) | Success: 99.0% | Rate: 2.5 drugs/sec\n",
      "  Progress: 2200/41649 (5.3%) | Success: 99.0% | Rate: 2.5 drugs/sec\n",
      "  Progress: 2250/41649 (5.4%) | Success: 99.0% | Rate: 2.5 drugs/sec\n",
      "  Progress: 2300/41649 (5.5%) | Success: 99.0% | Rate: 2.5 drugs/sec\n",
      "  Progress: 2350/41649 (5.6%) | Success: 98.9% | Rate: 2.5 drugs/sec\n",
      "  Progress: 2400/41649 (5.8%) | Success: 98.8% | Rate: 2.5 drugs/sec\n",
      "  Progress: 2450/41649 (5.9%) | Success: 98.8% | Rate: 2.5 drugs/sec\n",
      "  Progress: 2500/41649 (6.0%) | Success: 98.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 2550/41649 (6.1%) | Success: 98.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 2600/41649 (6.2%) | Success: 98.6% | Rate: 2.5 drugs/sec\n",
      "  Progress: 2650/41649 (6.4%) | Success: 98.6% | Rate: 2.5 drugs/sec\n",
      "  Progress: 2700/41649 (6.5%) | Success: 98.6% | Rate: 2.5 drugs/sec\n",
      "  Progress: 2750/41649 (6.6%) | Success: 98.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 2800/41649 (6.7%) | Success: 98.6% | Rate: 2.5 drugs/sec\n",
      "  Progress: 2850/41649 (6.8%) | Success: 98.6% | Rate: 2.5 drugs/sec\n",
      "  Progress: 2900/41649 (7.0%) | Success: 98.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 2950/41649 (7.1%) | Success: 98.6% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3000/41649 (7.2%) | Success: 98.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3050/41649 (7.3%) | Success: 98.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3100/41649 (7.4%) | Success: 98.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3150/41649 (7.6%) | Success: 98.3% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3200/41649 (7.7%) | Success: 98.3% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3250/41649 (7.8%) | Success: 98.3% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3300/41649 (7.9%) | Success: 98.2% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3350/41649 (8.0%) | Success: 98.2% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3400/41649 (8.2%) | Success: 98.1% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3450/41649 (8.3%) | Success: 98.0% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3500/41649 (8.4%) | Success: 98.0% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3550/41649 (8.5%) | Success: 98.0% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3600/41649 (8.6%) | Success: 98.0% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3650/41649 (8.8%) | Success: 97.9% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3700/41649 (8.9%) | Success: 97.9% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3750/41649 (9.0%) | Success: 97.9% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3800/41649 (9.1%) | Success: 97.9% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3850/41649 (9.2%) | Success: 97.9% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3900/41649 (9.4%) | Success: 97.9% | Rate: 2.5 drugs/sec\n",
      "  Progress: 3950/41649 (9.5%) | Success: 97.9% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4000/41649 (9.6%) | Success: 97.9% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4050/41649 (9.7%) | Success: 97.8% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4100/41649 (9.8%) | Success: 97.8% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4150/41649 (10.0%) | Success: 97.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4200/41649 (10.1%) | Success: 97.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4250/41649 (10.2%) | Success: 97.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4300/41649 (10.3%) | Success: 97.8% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4350/41649 (10.4%) | Success: 97.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4400/41649 (10.6%) | Success: 97.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4450/41649 (10.7%) | Success: 97.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4500/41649 (10.8%) | Success: 97.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4550/41649 (10.9%) | Success: 97.6% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4600/41649 (11.0%) | Success: 97.6% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4650/41649 (11.2%) | Success: 97.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4700/41649 (11.3%) | Success: 97.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4750/41649 (11.4%) | Success: 97.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4800/41649 (11.5%) | Success: 97.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4850/41649 (11.6%) | Success: 97.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4900/41649 (11.8%) | Success: 97.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 4950/41649 (11.9%) | Success: 97.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5000/41649 (12.0%) | Success: 97.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5050/41649 (12.1%) | Success: 97.3% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5100/41649 (12.2%) | Success: 97.3% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5150/41649 (12.4%) | Success: 97.3% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5200/41649 (12.5%) | Success: 97.3% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5250/41649 (12.6%) | Success: 97.2% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5300/41649 (12.7%) | Success: 97.3% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5350/41649 (12.8%) | Success: 97.3% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5400/41649 (13.0%) | Success: 97.2% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5450/41649 (13.1%) | Success: 97.2% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5500/41649 (13.2%) | Success: 97.2% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5550/41649 (13.3%) | Success: 97.2% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5600/41649 (13.4%) | Success: 97.2% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5650/41649 (13.6%) | Success: 97.1% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5700/41649 (13.7%) | Success: 97.1% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5750/41649 (13.8%) | Success: 97.1% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5800/41649 (13.9%) | Success: 97.1% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5850/41649 (14.0%) | Success: 97.1% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5900/41649 (14.2%) | Success: 97.1% | Rate: 2.5 drugs/sec\n",
      "  Progress: 5950/41649 (14.3%) | Success: 97.0% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6000/41649 (14.4%) | Success: 97.0% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6050/41649 (14.5%) | Success: 97.0% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6100/41649 (14.6%) | Success: 97.0% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6150/41649 (14.8%) | Success: 97.0% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6200/41649 (14.9%) | Success: 97.0% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6250/41649 (15.0%) | Success: 96.9% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6300/41649 (15.1%) | Success: 96.9% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6350/41649 (15.2%) | Success: 96.9% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6400/41649 (15.4%) | Success: 96.9% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6450/41649 (15.5%) | Success: 96.9% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6500/41649 (15.6%) | Success: 96.9% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6550/41649 (15.7%) | Success: 96.8% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6600/41649 (15.8%) | Success: 96.8% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6650/41649 (16.0%) | Success: 96.8% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6700/41649 (16.1%) | Success: 96.8% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6750/41649 (16.2%) | Success: 96.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6800/41649 (16.3%) | Success: 96.8% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6850/41649 (16.4%) | Success: 96.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6900/41649 (16.6%) | Success: 96.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 6950/41649 (16.7%) | Success: 96.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7000/41649 (16.8%) | Success: 96.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7050/41649 (16.9%) | Success: 96.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7100/41649 (17.0%) | Success: 96.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7150/41649 (17.2%) | Success: 96.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7200/41649 (17.3%) | Success: 96.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7250/41649 (17.4%) | Success: 96.7% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7300/41649 (17.5%) | Success: 96.6% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7350/41649 (17.6%) | Success: 96.6% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7400/41649 (17.8%) | Success: 96.6% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7450/41649 (17.9%) | Success: 96.6% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7500/41649 (18.0%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7550/41649 (18.1%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7600/41649 (18.2%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7650/41649 (18.4%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7700/41649 (18.5%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7750/41649 (18.6%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7800/41649 (18.7%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7850/41649 (18.8%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7900/41649 (19.0%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 7950/41649 (19.1%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8000/41649 (19.2%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8050/41649 (19.3%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8100/41649 (19.4%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8150/41649 (19.6%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8200/41649 (19.7%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8250/41649 (19.8%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8300/41649 (19.9%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8350/41649 (20.0%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8400/41649 (20.2%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8450/41649 (20.3%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8500/41649 (20.4%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8550/41649 (20.5%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8600/41649 (20.6%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8650/41649 (20.8%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8700/41649 (20.9%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8750/41649 (21.0%) | Success: 96.5% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8800/41649 (21.1%) | Success: 96.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8850/41649 (21.2%) | Success: 96.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8900/41649 (21.4%) | Success: 96.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 8950/41649 (21.5%) | Success: 96.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 9000/41649 (21.6%) | Success: 96.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 9050/41649 (21.7%) | Success: 96.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 9100/41649 (21.8%) | Success: 96.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 9150/41649 (22.0%) | Success: 96.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 9200/41649 (22.1%) | Success: 96.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 9250/41649 (22.2%) | Success: 96.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 9300/41649 (22.3%) | Success: 96.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 9350/41649 (22.4%) | Success: 96.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 9400/41649 (22.6%) | Success: 96.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 9450/41649 (22.7%) | Success: 96.4% | Rate: 2.5 drugs/sec\n",
      "  Progress: 9500/41649 (22.8%) | Success: 96.4% | Rate: 2.5 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 05:31:43,815 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=BREVIBLOC&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 9550/41649 (22.9%) | Success: 96.4% | Rate: 2.0 drugs/sec\n",
      "  Progress: 9600/41649 (23.0%) | Success: 96.4% | Rate: 2.0 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 05:48:27,865 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=Dutasteride+Soft+Capsules&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 9650/41649 (23.2%) | Success: 96.4% | Rate: 1.7 drugs/sec\n",
      "  Progress: 9700/41649 (23.3%) | Success: 96.3% | Rate: 1.7 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 05:54:19,668 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=NITROFURANTOIN+MONO&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 9750/41649 (23.4%) | Success: 96.3% | Rate: 1.6 drugs/sec\n",
      "  Progress: 9800/41649 (23.5%) | Success: 96.3% | Rate: 1.6 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 06:25:34,870 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=LENDACIN&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 9850/41649 (23.7%) | Success: 96.3% | Rate: 1.2 drugs/sec\n",
      "  Progress: 9900/41649 (23.8%) | Success: 96.3% | Rate: 1.2 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 06:27:16,015 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=IBUPROFEN+SODIUM&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 9950/41649 (23.9%) | Success: 96.3% | Rate: 1.2 drugs/sec\n",
      "  Progress: 10000/41649 (24.0%) | Success: 96.3% | Rate: 1.2 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 06:43:34,729 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=Solifenacin+Succinate+Tablets+10+mg&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10050/41649 (24.1%) | Success: 96.3% | Rate: 1.1 drugs/sec\n",
      "  Progress: 10100/41649 (24.3%) | Success: 96.3% | Rate: 1.1 drugs/sec\n",
      "  Progress: 10150/41649 (24.4%) | Success: 96.3% | Rate: 1.1 drugs/sec\n",
      "  Progress: 10200/41649 (24.5%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 10250/41649 (24.6%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 10300/41649 (24.7%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 10350/41649 (24.9%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 10400/41649 (25.0%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 10450/41649 (25.1%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 10500/41649 (25.2%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 10550/41649 (25.3%) | Success: 96.2% | Rate: 1.1 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 07:04:44,170 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=CLOBETASOL+PROPIONATE+FOAM+0.05%25&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10600/41649 (25.5%) | Success: 96.2% | Rate: 1.0 drugs/sec\n",
      "  Progress: 10650/41649 (25.6%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 10700/41649 (25.7%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 10750/41649 (25.8%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 10800/41649 (25.9%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 10850/41649 (26.1%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 10900/41649 (26.2%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 10950/41649 (26.3%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 11000/41649 (26.4%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 11050/41649 (26.5%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 11100/41649 (26.7%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 11150/41649 (26.8%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 11200/41649 (26.9%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 11250/41649 (27.0%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 11300/41649 (27.1%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 11350/41649 (27.3%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 11400/41649 (27.4%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 11450/41649 (27.5%) | Success: 96.1% | Rate: 1.1 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 07:40:58,841 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=TRIAMCINOLONE%3FMOXIFLOXACIN+%2815%2F1%29+MG%2FML&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 11500/41649 (27.6%) | Success: 96.1% | Rate: 0.9 drugs/sec\n",
      "  Progress: 11550/41649 (27.7%) | Success: 96.1% | Rate: 0.9 drugs/sec\n",
      "  Progress: 11600/41649 (27.9%) | Success: 96.1% | Rate: 0.9 drugs/sec\n",
      "  Progress: 11650/41649 (28.0%) | Success: 96.1% | Rate: 0.9 drugs/sec\n",
      "  Progress: 11700/41649 (28.1%) | Success: 96.1% | Rate: 0.9 drugs/sec\n",
      "  Progress: 11750/41649 (28.2%) | Success: 96.1% | Rate: 0.9 drugs/sec\n",
      "  Progress: 11800/41649 (28.3%) | Success: 96.0% | Rate: 0.9 drugs/sec\n",
      "  Progress: 11850/41649 (28.5%) | Success: 96.0% | Rate: 0.9 drugs/sec\n",
      "  Progress: 11900/41649 (28.6%) | Success: 96.0% | Rate: 0.9 drugs/sec\n",
      "  Progress: 11950/41649 (28.7%) | Success: 96.0% | Rate: 0.9 drugs/sec\n",
      "  Progress: 12000/41649 (28.8%) | Success: 96.0% | Rate: 0.9 drugs/sec\n",
      "  Progress: 12050/41649 (28.9%) | Success: 96.1% | Rate: 0.9 drugs/sec\n",
      "  Progress: 12100/41649 (29.1%) | Success: 96.1% | Rate: 0.9 drugs/sec\n",
      "  Progress: 12150/41649 (29.2%) | Success: 96.1% | Rate: 1.0 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 07:55:31,946 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=LOQTORZI&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 12200/41649 (29.3%) | Success: 96.1% | Rate: 0.9 drugs/sec\n",
      "  Progress: 12250/41649 (29.4%) | Success: 96.1% | Rate: 0.9 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 08:15:59,897 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=MESALAZINE+400mg+Cap&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 12300/41649 (29.5%) | Success: 96.1% | Rate: 0.8 drugs/sec\n",
      "  Progress: 12350/41649 (29.7%) | Success: 96.1% | Rate: 0.8 drugs/sec\n",
      "  Progress: 12400/41649 (29.8%) | Success: 96.1% | Rate: 0.8 drugs/sec\n",
      "  Progress: 12450/41649 (29.9%) | Success: 96.0% | Rate: 0.8 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 08:34:29,968 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=Famotidine+Tablets&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 12500/41649 (30.0%) | Success: 96.0% | Rate: 0.8 drugs/sec\n",
      "  Progress: 12550/41649 (30.1%) | Success: 96.0% | Rate: 0.8 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 08:42:40,010 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=LEVODOPA%2C+CARBIDOPA&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 12600/41649 (30.3%) | Success: 96.0% | Rate: 0.8 drugs/sec\n",
      "  Progress: 12650/41649 (30.4%) | Success: 96.0% | Rate: 0.8 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 08:59:35,437 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=LIDOCAINE+2%25+%28MANUFACTURER+UNKNOWN%29&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 12700/41649 (30.5%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 12750/41649 (30.6%) | Success: 96.0% | Rate: 0.7 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 09:18:13,026 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=Quetiapin+retard&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 12800/41649 (30.7%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 12850/41649 (30.9%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 12900/41649 (31.0%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 12950/41649 (31.1%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13000/41649 (31.2%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13050/41649 (31.3%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13100/41649 (31.5%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13150/41649 (31.6%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13200/41649 (31.7%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13250/41649 (31.8%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13300/41649 (31.9%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13350/41649 (32.1%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13400/41649 (32.2%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13450/41649 (32.3%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13500/41649 (32.4%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13550/41649 (32.5%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13600/41649 (32.7%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13650/41649 (32.8%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13700/41649 (32.9%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13750/41649 (33.0%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13800/41649 (33.1%) | Success: 96.0% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13850/41649 (33.3%) | Success: 95.9% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13900/41649 (33.4%) | Success: 95.9% | Rate: 0.7 drugs/sec\n",
      "  Progress: 13950/41649 (33.5%) | Success: 95.9% | Rate: 0.7 drugs/sec\n",
      "  Progress: 14000/41649 (33.6%) | Success: 95.9% | Rate: 0.7 drugs/sec\n",
      "  Progress: 14050/41649 (33.7%) | Success: 95.9% | Rate: 0.7 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 09:56:20,902 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=Methylphenidate+Hydrochloride+Extended%3FRelease+Tablets+USP+%28non%3Fspecif&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 14100/41649 (33.9%) | Success: 95.9% | Rate: 0.7 drugs/sec\n",
      "  Progress: 14150/41649 (34.0%) | Success: 95.9% | Rate: 0.7 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 10:14:55,848 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=Omeprazole+20mg+Capsules%2C+Dr.+Reddy%27s&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 14200/41649 (34.1%) | Success: 95.9% | Rate: 0.7 drugs/sec\n",
      "  Progress: 14250/41649 (34.2%) | Success: 95.9% | Rate: 0.7 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 10:16:23,648 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=APO%3FCYCLOBENZAPRINE&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 14300/41649 (34.3%) | Success: 95.9% | Rate: 0.7 drugs/sec\n",
      "  Progress: 14350/41649 (34.5%) | Success: 95.9% | Rate: 0.7 drugs/sec\n",
      "  Progress: 14400/41649 (34.6%) | Success: 95.9% | Rate: 0.7 drugs/sec\n",
      "  Progress: 14450/41649 (34.7%) | Success: 95.9% | Rate: 0.7 drugs/sec\n",
      "  Progress: 14500/41649 (34.8%) | Success: 95.9% | Rate: 0.7 drugs/sec\n",
      "  Progress: 14550/41649 (34.9%) | Success: 95.9% | Rate: 0.7 drugs/sec\n",
      "  Progress: 14600/41649 (35.1%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 14650/41649 (35.2%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 14700/41649 (35.3%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 14750/41649 (35.4%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 14800/41649 (35.5%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 14850/41649 (35.7%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 14900/41649 (35.8%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 14950/41649 (35.9%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15000/41649 (36.0%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15050/41649 (36.1%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15100/41649 (36.3%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15150/41649 (36.4%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15200/41649 (36.5%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15250/41649 (36.6%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15300/41649 (36.7%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15350/41649 (36.9%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15400/41649 (37.0%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15450/41649 (37.1%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15500/41649 (37.2%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15550/41649 (37.3%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15600/41649 (37.5%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15650/41649 (37.6%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15700/41649 (37.7%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15750/41649 (37.8%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15800/41649 (37.9%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15850/41649 (38.1%) | Success: 95.8% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15900/41649 (38.2%) | Success: 95.7% | Rate: 0.7 drugs/sec\n",
      "  Progress: 15950/41649 (38.3%) | Success: 95.7% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16000/41649 (38.4%) | Success: 95.7% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16050/41649 (38.5%) | Success: 95.7% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16100/41649 (38.7%) | Success: 95.7% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16150/41649 (38.8%) | Success: 95.7% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16200/41649 (38.9%) | Success: 95.7% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16250/41649 (39.0%) | Success: 95.7% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16300/41649 (39.1%) | Success: 95.7% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16350/41649 (39.3%) | Success: 95.7% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16400/41649 (39.4%) | Success: 95.7% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16450/41649 (39.5%) | Success: 95.7% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16500/41649 (39.6%) | Success: 95.6% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16550/41649 (39.7%) | Success: 95.7% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16600/41649 (39.9%) | Success: 95.7% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16650/41649 (40.0%) | Success: 95.6% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16700/41649 (40.1%) | Success: 95.6% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16750/41649 (40.2%) | Success: 95.6% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16800/41649 (40.3%) | Success: 95.6% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16850/41649 (40.5%) | Success: 95.6% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16900/41649 (40.6%) | Success: 95.6% | Rate: 0.7 drugs/sec\n",
      "  Progress: 16950/41649 (40.7%) | Success: 95.6% | Rate: 0.7 drugs/sec\n",
      "  Progress: 17000/41649 (40.8%) | Success: 95.6% | Rate: 0.7 drugs/sec\n",
      "  Progress: 17050/41649 (40.9%) | Success: 95.6% | Rate: 0.7 drugs/sec\n",
      "  Progress: 17100/41649 (41.1%) | Success: 95.6% | Rate: 0.7 drugs/sec\n",
      "  Progress: 17150/41649 (41.2%) | Success: 95.7% | Rate: 0.7 drugs/sec\n",
      "  Progress: 17200/41649 (41.3%) | Success: 95.7% | Rate: 0.7 drugs/sec\n",
      "  Progress: 17250/41649 (41.4%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 17300/41649 (41.5%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 17350/41649 (41.7%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 17400/41649 (41.8%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 17450/41649 (41.9%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 17500/41649 (42.0%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 17550/41649 (42.1%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 17600/41649 (42.3%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 17650/41649 (42.4%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 17700/41649 (42.5%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 17750/41649 (42.6%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 17800/41649 (42.7%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 17850/41649 (42.9%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 17900/41649 (43.0%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 17950/41649 (43.1%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18000/41649 (43.2%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18050/41649 (43.3%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18100/41649 (43.5%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18150/41649 (43.6%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18200/41649 (43.7%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18250/41649 (43.8%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18300/41649 (43.9%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18350/41649 (44.1%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18400/41649 (44.2%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18450/41649 (44.3%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18500/41649 (44.4%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18550/41649 (44.5%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18600/41649 (44.7%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18650/41649 (44.8%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18700/41649 (44.9%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18750/41649 (45.0%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18800/41649 (45.1%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18850/41649 (45.3%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18900/41649 (45.4%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 18950/41649 (45.5%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19000/41649 (45.6%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19050/41649 (45.7%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19100/41649 (45.9%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19150/41649 (46.0%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19200/41649 (46.1%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19250/41649 (46.2%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19300/41649 (46.3%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19350/41649 (46.5%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19400/41649 (46.6%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19450/41649 (46.7%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19500/41649 (46.8%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19550/41649 (46.9%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19600/41649 (47.1%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19650/41649 (47.2%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19700/41649 (47.3%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19750/41649 (47.4%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19800/41649 (47.5%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19850/41649 (47.7%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19900/41649 (47.8%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 19950/41649 (47.9%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 20000/41649 (48.0%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 20050/41649 (48.1%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 20100/41649 (48.3%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 20150/41649 (48.4%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 20200/41649 (48.5%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 20250/41649 (48.6%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 20300/41649 (48.7%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 20350/41649 (48.9%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 20400/41649 (49.0%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 20450/41649 (49.1%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 20500/41649 (49.2%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 20550/41649 (49.3%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 20600/41649 (49.5%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 20650/41649 (49.6%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 20700/41649 (49.7%) | Success: 95.7% | Rate: 0.8 drugs/sec\n",
      "  Progress: 20750/41649 (49.8%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 20800/41649 (49.9%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 20850/41649 (50.1%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 20900/41649 (50.2%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 20950/41649 (50.3%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21000/41649 (50.4%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21050/41649 (50.5%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21100/41649 (50.7%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21150/41649 (50.8%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21200/41649 (50.9%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21250/41649 (51.0%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21300/41649 (51.1%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21350/41649 (51.3%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21400/41649 (51.4%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21450/41649 (51.5%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21500/41649 (51.6%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21550/41649 (51.7%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21600/41649 (51.9%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21650/41649 (52.0%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21700/41649 (52.1%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21750/41649 (52.2%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21800/41649 (52.3%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21850/41649 (52.5%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21900/41649 (52.6%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 21950/41649 (52.7%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22000/41649 (52.8%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22050/41649 (52.9%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22100/41649 (53.1%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22150/41649 (53.2%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22200/41649 (53.3%) | Success: 95.7% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22250/41649 (53.4%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22300/41649 (53.5%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22350/41649 (53.7%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22400/41649 (53.8%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22450/41649 (53.9%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22500/41649 (54.0%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22550/41649 (54.1%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22600/41649 (54.3%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22650/41649 (54.4%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22700/41649 (54.5%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22750/41649 (54.6%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22800/41649 (54.7%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22850/41649 (54.9%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22900/41649 (55.0%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 22950/41649 (55.1%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23000/41649 (55.2%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23050/41649 (55.3%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23100/41649 (55.5%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23150/41649 (55.6%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23200/41649 (55.7%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23250/41649 (55.8%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23300/41649 (55.9%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23350/41649 (56.1%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23400/41649 (56.2%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23450/41649 (56.3%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23500/41649 (56.4%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23550/41649 (56.5%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23600/41649 (56.7%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23650/41649 (56.8%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23700/41649 (56.9%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23750/41649 (57.0%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23800/41649 (57.1%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23850/41649 (57.3%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23900/41649 (57.4%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 23950/41649 (57.5%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 24000/41649 (57.6%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 24050/41649 (57.7%) | Success: 95.8% | Rate: 0.9 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 11:26:43,810 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=PROMETHAZINE+TABLET+OMHULD+25MG+%2F+BRAND+NAME+NOT+SPECIFIED&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 24100/41649 (57.9%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 24150/41649 (58.0%) | Success: 95.8% | Rate: 0.9 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 11:42:32,080 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=SMART+WOMEN%27S+CHOICE+%28CONTAINER+ITSELF+IS+UNMARKED%29&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 24200/41649 (58.1%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 24250/41649 (58.2%) | Success: 95.8% | Rate: 0.9 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 11:58:22,167 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=SODIUM+PYROPHOSPHATE&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 24300/41649 (58.3%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 24350/41649 (58.5%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 24400/41649 (58.6%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 24450/41649 (58.7%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 24500/41649 (58.8%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 24550/41649 (58.9%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 24600/41649 (59.1%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 24650/41649 (59.2%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 24700/41649 (59.3%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 24750/41649 (59.4%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 24800/41649 (59.5%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 24850/41649 (59.7%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 24900/41649 (59.8%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 24950/41649 (59.9%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25000/41649 (60.0%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25050/41649 (60.1%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25100/41649 (60.3%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25150/41649 (60.4%) | Success: 95.8% | Rate: 0.9 drugs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 12:08:58,913 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='rxnav.nlm.nih.gov', port=443): Read timed out. (read timeout=10)\")': /REST/approximateTerm.json?term=Pregabalin+beta+75+mg+Hartkapseln&maxEntries=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 25200/41649 (60.5%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25250/41649 (60.6%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25300/41649 (60.7%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25350/41649 (60.9%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25400/41649 (61.0%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25450/41649 (61.1%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25500/41649 (61.2%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25550/41649 (61.3%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25600/41649 (61.5%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25650/41649 (61.6%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25700/41649 (61.7%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25750/41649 (61.8%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25800/41649 (61.9%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25850/41649 (62.1%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25900/41649 (62.2%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 25950/41649 (62.3%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26000/41649 (62.4%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26050/41649 (62.5%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26100/41649 (62.7%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26150/41649 (62.8%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26200/41649 (62.9%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26250/41649 (63.0%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26300/41649 (63.1%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26350/41649 (63.3%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26400/41649 (63.4%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26450/41649 (63.5%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26500/41649 (63.6%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26550/41649 (63.7%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26600/41649 (63.9%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26650/41649 (64.0%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26700/41649 (64.1%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26750/41649 (64.2%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26800/41649 (64.3%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26850/41649 (64.5%) | Success: 95.8% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26900/41649 (64.6%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 26950/41649 (64.7%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27000/41649 (64.8%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27050/41649 (64.9%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27100/41649 (65.1%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27150/41649 (65.2%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27200/41649 (65.3%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27250/41649 (65.4%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27300/41649 (65.5%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27350/41649 (65.7%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27400/41649 (65.8%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27450/41649 (65.9%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27500/41649 (66.0%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27550/41649 (66.1%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27600/41649 (66.3%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27650/41649 (66.4%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27700/41649 (66.5%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27750/41649 (66.6%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27800/41649 (66.7%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27850/41649 (66.9%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27900/41649 (67.0%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 27950/41649 (67.1%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 28000/41649 (67.2%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 28050/41649 (67.3%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 28100/41649 (67.5%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 28150/41649 (67.6%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 28200/41649 (67.7%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 28250/41649 (67.8%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 28300/41649 (67.9%) | Success: 95.9% | Rate: 0.9 drugs/sec\n",
      "  Progress: 28350/41649 (68.1%) | Success: 95.9% | Rate: 1.0 drugs/sec\n",
      "  Progress: 28400/41649 (68.2%) | Success: 95.9% | Rate: 1.0 drugs/sec\n",
      "  Progress: 28450/41649 (68.3%) | Success: 95.9% | Rate: 1.0 drugs/sec\n",
      "  Progress: 28500/41649 (68.4%) | Success: 95.9% | Rate: 1.0 drugs/sec\n",
      "  Progress: 28550/41649 (68.5%) | Success: 95.9% | Rate: 1.0 drugs/sec\n",
      "  Progress: 28600/41649 (68.7%) | Success: 95.9% | Rate: 1.0 drugs/sec\n",
      "  Progress: 28650/41649 (68.8%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 28700/41649 (68.9%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 28750/41649 (69.0%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 28800/41649 (69.1%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 28850/41649 (69.3%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 28900/41649 (69.4%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 28950/41649 (69.5%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29000/41649 (69.6%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29050/41649 (69.7%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29100/41649 (69.9%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29150/41649 (70.0%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29200/41649 (70.1%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29250/41649 (70.2%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29300/41649 (70.3%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29350/41649 (70.5%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29400/41649 (70.6%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29450/41649 (70.7%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29500/41649 (70.8%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29550/41649 (71.0%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29600/41649 (71.1%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29650/41649 (71.2%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29700/41649 (71.3%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29750/41649 (71.4%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29800/41649 (71.6%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29850/41649 (71.7%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29900/41649 (71.8%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 29950/41649 (71.9%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30000/41649 (72.0%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30050/41649 (72.2%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30100/41649 (72.3%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30150/41649 (72.4%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30200/41649 (72.5%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30250/41649 (72.6%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30300/41649 (72.8%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30350/41649 (72.9%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30400/41649 (73.0%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30450/41649 (73.1%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30500/41649 (73.2%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30550/41649 (73.4%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30600/41649 (73.5%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30650/41649 (73.6%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30700/41649 (73.7%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30750/41649 (73.8%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30800/41649 (74.0%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30850/41649 (74.1%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30900/41649 (74.2%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 30950/41649 (74.3%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31000/41649 (74.4%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31050/41649 (74.6%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31100/41649 (74.7%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31150/41649 (74.8%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31200/41649 (74.9%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31250/41649 (75.0%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31300/41649 (75.2%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31350/41649 (75.3%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31400/41649 (75.4%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31450/41649 (75.5%) | Success: 96.0% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31500/41649 (75.6%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31550/41649 (75.8%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31600/41649 (75.9%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31650/41649 (76.0%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31700/41649 (76.1%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31750/41649 (76.2%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31800/41649 (76.4%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31850/41649 (76.5%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31900/41649 (76.6%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 31950/41649 (76.7%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32000/41649 (76.8%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32050/41649 (77.0%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32100/41649 (77.1%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32150/41649 (77.2%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32200/41649 (77.3%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32250/41649 (77.4%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32300/41649 (77.6%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32350/41649 (77.7%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32400/41649 (77.8%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32450/41649 (77.9%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32500/41649 (78.0%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32550/41649 (78.2%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32600/41649 (78.3%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32650/41649 (78.4%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32700/41649 (78.5%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32750/41649 (78.6%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32800/41649 (78.8%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32850/41649 (78.9%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32900/41649 (79.0%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 32950/41649 (79.1%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 33000/41649 (79.2%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 33050/41649 (79.4%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 33100/41649 (79.5%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 33150/41649 (79.6%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 33200/41649 (79.7%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 33250/41649 (79.8%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 33300/41649 (80.0%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 33350/41649 (80.1%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 33400/41649 (80.2%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 33450/41649 (80.3%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 33500/41649 (80.4%) | Success: 96.1% | Rate: 1.0 drugs/sec\n",
      "  Progress: 33550/41649 (80.6%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 33600/41649 (80.7%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 33650/41649 (80.8%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 33700/41649 (80.9%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 33750/41649 (81.0%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 33800/41649 (81.2%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 33850/41649 (81.3%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 33900/41649 (81.4%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 33950/41649 (81.5%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34000/41649 (81.6%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34050/41649 (81.8%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34100/41649 (81.9%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34150/41649 (82.0%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34200/41649 (82.1%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34250/41649 (82.2%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34300/41649 (82.4%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34350/41649 (82.5%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34400/41649 (82.6%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34450/41649 (82.7%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34500/41649 (82.8%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34550/41649 (83.0%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34600/41649 (83.1%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34650/41649 (83.2%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34700/41649 (83.3%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34750/41649 (83.4%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34800/41649 (83.6%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34850/41649 (83.7%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34900/41649 (83.8%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 34950/41649 (83.9%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35000/41649 (84.0%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35050/41649 (84.2%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35100/41649 (84.3%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35150/41649 (84.4%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35200/41649 (84.5%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35250/41649 (84.6%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35300/41649 (84.8%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35350/41649 (84.9%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35400/41649 (85.0%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35450/41649 (85.1%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35500/41649 (85.2%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35550/41649 (85.4%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35600/41649 (85.5%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35650/41649 (85.6%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35700/41649 (85.7%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35750/41649 (85.8%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35800/41649 (86.0%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35850/41649 (86.1%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35900/41649 (86.2%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 35950/41649 (86.3%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36000/41649 (86.4%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36050/41649 (86.6%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36100/41649 (86.7%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36150/41649 (86.8%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36200/41649 (86.9%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36250/41649 (87.0%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36300/41649 (87.2%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36350/41649 (87.3%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36400/41649 (87.4%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36450/41649 (87.5%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36500/41649 (87.6%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36550/41649 (87.8%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36600/41649 (87.9%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36650/41649 (88.0%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36700/41649 (88.1%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36750/41649 (88.2%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36800/41649 (88.4%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36850/41649 (88.5%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36900/41649 (88.6%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 36950/41649 (88.7%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37000/41649 (88.8%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37050/41649 (89.0%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37100/41649 (89.1%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37150/41649 (89.2%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37200/41649 (89.3%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37250/41649 (89.4%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37300/41649 (89.6%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37350/41649 (89.7%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37400/41649 (89.8%) | Success: 96.1% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37450/41649 (89.9%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37500/41649 (90.0%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37550/41649 (90.2%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37600/41649 (90.3%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37650/41649 (90.4%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37700/41649 (90.5%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37750/41649 (90.6%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37800/41649 (90.8%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37850/41649 (90.9%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37900/41649 (91.0%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 37950/41649 (91.1%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38000/41649 (91.2%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38050/41649 (91.4%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38100/41649 (91.5%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38150/41649 (91.6%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38200/41649 (91.7%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38250/41649 (91.8%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38300/41649 (92.0%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38350/41649 (92.1%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38400/41649 (92.2%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38450/41649 (92.3%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38500/41649 (92.4%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38550/41649 (92.6%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38600/41649 (92.7%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38650/41649 (92.8%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38700/41649 (92.9%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38750/41649 (93.0%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38800/41649 (93.2%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38850/41649 (93.3%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38900/41649 (93.4%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 38950/41649 (93.5%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 39000/41649 (93.6%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 39050/41649 (93.8%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 39100/41649 (93.9%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 39150/41649 (94.0%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 39200/41649 (94.1%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 39250/41649 (94.2%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 39300/41649 (94.4%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 39350/41649 (94.5%) | Success: 96.2% | Rate: 1.1 drugs/sec\n",
      "  Progress: 39400/41649 (94.6%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 39450/41649 (94.7%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 39500/41649 (94.8%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 39550/41649 (95.0%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 39600/41649 (95.1%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 39650/41649 (95.2%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 39700/41649 (95.3%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 39750/41649 (95.4%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 39800/41649 (95.6%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 39850/41649 (95.7%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 39900/41649 (95.8%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 39950/41649 (95.9%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40000/41649 (96.0%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40050/41649 (96.2%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40100/41649 (96.3%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40150/41649 (96.4%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40200/41649 (96.5%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40250/41649 (96.6%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40300/41649 (96.8%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40350/41649 (96.9%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40400/41649 (97.0%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40450/41649 (97.1%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40500/41649 (97.2%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40550/41649 (97.4%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40600/41649 (97.5%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40650/41649 (97.6%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40700/41649 (97.7%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40750/41649 (97.8%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40800/41649 (98.0%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40850/41649 (98.1%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40900/41649 (98.2%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 40950/41649 (98.3%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 41000/41649 (98.4%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 41050/41649 (98.6%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 41100/41649 (98.7%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 41150/41649 (98.8%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 41200/41649 (98.9%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 41250/41649 (99.0%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 41300/41649 (99.2%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 41350/41649 (99.3%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 41400/41649 (99.4%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 41450/41649 (99.5%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 41500/41649 (99.6%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 41550/41649 (99.8%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 41600/41649 (99.9%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "  Progress: 41649/41649 (100.0%) | Success: 96.2% | Rate: 1.2 drugs/sec\n",
      "Cache connection closed.\n",
      "\n",
      "Mapping complete! Final results saved to: faers_unique_drugs_mapped_final.csv\n",
      "\n",
      "============================================================\n",
      "STEP 4: APPLYING MAPPINGS TO FULL FILE\n",
      "============================================================\n",
      "Processing Complete! Full file with mappings saved to: faers_drug_rxnorm_standardized.csv\n",
      "\n",
      "\n",
      "✅ PIPELINE COMPLETE!\n",
      "The final standardized file is available at: 'faers_drug_rxnorm_standardized.csv'\n",
      "You can now proceed with analysis on this file.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# COMPLETE FAERS DRUG STANDARDIZATION PIPELINE - FOR A SINGLE JUPYTER NOTEBOOK CELL\n",
    "# ================================================================================\n",
    "# This script contains all necessary classes and functions to perform the full\n",
    "# FAERS drug name standardization workflow.\n",
    "#\n",
    "# Workflow:\n",
    "# 1. Imports and Class Definition: All setup is at the top.\n",
    "# 2. Helper Functions: The logic for each step of the pipeline.\n",
    "# 3. Main Execution: The main block at the bottom runs the entire process.\n",
    "#\n",
    "# To Run: Simply execute this cell in your Jupyter Notebook.\n",
    "# ================================================================================\n",
    "\n",
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import sqlite3\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# ================================================================================\n",
    "# CLASS DEFINITION: The tool for mapping drug names\n",
    "# ================================================================================\n",
    "\n",
    "class FAERSRxNormMapper:\n",
    "    \"\"\"\n",
    "    Maps drug names to their standardized RxNorm counterparts using the NIH RxNav API.\n",
    "    Caches results in a local SQLite database to improve performance and reduce API load.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cache_db: str = \"faers_rxnorm_cache.db\"):\n",
    "        \"\"\"Initializes the mapper, sets up the API session, and connects to the cache.\"\"\"\n",
    "        self.base_url = \"https://rxnav.nlm.nih.gov/REST\"\n",
    "        self.cache_db = cache_db\n",
    "        self.cache_conn = None\n",
    "        self._connect_to_cache()\n",
    "        self.session = self._create_robust_session()\n",
    "        print(f\"FAERSRxNormMapper initialized. Cache located at: '{self.cache_db}'\")\n",
    "\n",
    "    def _connect_to_cache(self):\n",
    "        \"\"\"Establishes a connection to the SQLite database and creates the cache table.\"\"\"\n",
    "        try:\n",
    "            self.cache_conn = sqlite3.connect(self.cache_db, check_same_thread=False)\n",
    "            cursor = self.cache_conn.cursor()\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS rxnorm_cache (\n",
    "                    drug_name TEXT PRIMARY KEY,\n",
    "                    standard_name TEXT,\n",
    "                    rxcui TEXT,\n",
    "                    score REAL,\n",
    "                    mapping_method TEXT\n",
    "                )\n",
    "            ''')\n",
    "            self.cache_conn.commit()\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Database error: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _create_robust_session(self):\n",
    "        \"\"\"Creates a requests.Session with automatic retries for resilience.\"\"\"\n",
    "        session = requests.Session()\n",
    "        retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[500, 502, 503, 504])\n",
    "        adapter = HTTPAdapter(max_retries=retries)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        return session\n",
    "\n",
    "    def _query_cache(self, drug_name: str):\n",
    "        \"\"\"Checks the local cache for a previously mapped drug name.\"\"\"\n",
    "        try:\n",
    "            cursor = self.cache_conn.cursor()\n",
    "            cursor.execute(\"SELECT standard_name, rxcui, score, mapping_method FROM rxnorm_cache WHERE drug_name = ?\", (drug_name,))\n",
    "            result = cursor.fetchone()\n",
    "            if result:\n",
    "                return {'standard_name': result[0], 'rxcui': result[1], 'score': result[2], 'mapping_method': result[3]}\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Cache query error for '{drug_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "    def _update_cache(self, drug_name: str, result: dict):\n",
    "        \"\"\"Stores a new mapping result in the local cache.\"\"\"\n",
    "        try:\n",
    "            cursor = self.cache_conn.cursor()\n",
    "            cursor.execute(\"INSERT OR REPLACE INTO rxnorm_cache (drug_name, standard_name, rxcui, score, mapping_method) VALUES (?, ?, ?, ?, ?)\",\n",
    "                           (drug_name, result.get('standard_name'), result.get('rxcui'), result.get('score'), result.get('mapping_method')))\n",
    "            self.cache_conn.commit()\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Cache update error for '{drug_name}': {e}\")\n",
    "\n",
    "    def _query_rxnorm_api(self, drug_name: str):\n",
    "        \"\"\"Queries the RxNorm API to get the standardized name and RxCUI.\"\"\"\n",
    "        url = f\"{self.base_url}/approximateTerm.json\"\n",
    "        params = {'term': drug_name, 'maxEntries': 1}\n",
    "        try:\n",
    "            response = self.session.get(url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            candidates = data.get('approximateGroup', {}).get('candidate', [])\n",
    "            if candidates:\n",
    "                best_match = candidates[0]\n",
    "                return {'standard_name': best_match.get('rxcuiName'), 'rxcui': best_match.get('rxcui'), 'score': best_match.get('score'), 'mapping_method': 'approximateTerm'}\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API request failed for '{drug_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "    def get_standardized_drug_name(self, drug_name: str):\n",
    "        \"\"\"Public method to get a standardized drug name, using cache first.\"\"\"\n",
    "        if not drug_name or pd.isna(drug_name):\n",
    "            return None\n",
    "        \n",
    "        cached_result = self._query_cache(drug_name)\n",
    "        if cached_result:\n",
    "            cached_result['mapping_method'] = 'cache'\n",
    "            return cached_result\n",
    "        \n",
    "        api_result = self._query_rxnorm_api(drug_name)\n",
    "        \n",
    "        if api_result:\n",
    "            self._update_cache(drug_name, api_result)\n",
    "        else:\n",
    "            self._update_cache(drug_name, {'mapping_method': 'api_fail'})\n",
    "        \n",
    "        time.sleep(0.03)  # Respect API rate limits\n",
    "        return api_result\n",
    "\n",
    "    def close_connection(self):\n",
    "        \"\"\"Closes the connection to the cache database.\"\"\"\n",
    "        if self.cache_conn:\n",
    "            self.cache_conn.close()\n",
    "            print(\"Cache connection closed.\")\n",
    "\n",
    "# ================================================================================\n",
    "# PIPELINE HELPER FUNCTIONS\n",
    "# ================================================================================\n",
    "\n",
    "def create_sample_faers_file(file_path: str, num_rows: int = 50000):\n",
    "    \"\"\"Generates a sample FAERS drug file for demonstration.\"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Sample file '{file_path}' already exists. Skipping generation.\")\n",
    "        return\n",
    "    print(f\"Generating a sample FAERS file with {num_rows:,} rows at '{file_path}'...\")\n",
    "    drug_pool = ['ASPIRIN', 'LIPITOR', 'METFORMIN', 'LISINOPRIL', 'IBUPROFEN', 'ADVIL', 'TYLENOL', 'ATORVASTATIN', 'Glucophage', 'Prozac', 'Xanax', 'OxyContin', 'VITAMIN C', np.nan]\n",
    "    p = np.linspace(1, 0.1, len(drug_pool)); p /= p.sum()\n",
    "    data = {'primaryid': [f\"CASE{100000 + i}\" for i in range(num_rows)], 'drugname': np.random.choice(drug_pool, size=num_rows, p=p)}\n",
    "    df = pd.DataFrame(data)\n",
    "    df['caseid'] = df['primaryid']\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Sample file '{file_path}' created successfully.\")\n",
    "\n",
    "def extract_unique_faers_drugs(file_path: str, chunksize: int = 50000):\n",
    "    \"\"\"Extracts unique drug names from the FAERS drug file.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\nSTEP 1: EXTRACTING UNIQUE DRUGS\\n\" + \"=\"*60)\n",
    "    start_time = time.time()\n",
    "    unique_drugs = set()\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize, usecols=['drugname'], low_memory=False):\n",
    "        unique_drugs.update(chunk['drugname'].dropna().str.strip().unique())\n",
    "    print(f\"Extraction Complete! Found {len(unique_drugs):,} unique drugs in {time.time() - start_time:.2f}s.\")\n",
    "    return pd.DataFrame({'drugname': sorted(list(unique_drugs))})\n",
    "\n",
    "def calculate_drug_frequencies(file_path: str, unique_drugs_df: pd.DataFrame, chunksize: int = 50000):\n",
    "    \"\"\"Calculates the frequency of each unique drug.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\nSTEP 2: CALCULATING DRUG FREQUENCIES\\n\" + \"=\"*60)\n",
    "    start_time = time.time()\n",
    "    drug_counts = pd.Series(dtype=int)\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize, usecols=['drugname'], low_memory=False):\n",
    "        drug_counts = drug_counts.add(chunk['drugname'].dropna().str.strip().value_counts(), fill_value=0)\n",
    "    unique_drugs_df['frequency'] = unique_drugs_df['drugname'].map(drug_counts).astype(int)\n",
    "    unique_drugs_df = unique_drugs_df.sort_values('frequency', ascending=False).reset_index(drop=True)\n",
    "    print(f\"Frequency calculation complete in {time.time() - start_time:.2f}s.\")\n",
    "    print(\"Top 10 most frequent drugs:\\n\", unique_drugs_df.head(10).to_string())\n",
    "    return unique_drugs_df\n",
    "\n",
    "def map_unique_drugs_batch(unique_drugs_df: pd.DataFrame, start_from: int = 0, batch_size: int = 50):\n",
    "    \"\"\"Maps unique drugs to RxNorm with progress tracking and checkpointing.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\nSTEP 3: MAPPING UNIQUE DRUGS TO RXNORM\\n\" + \"=\"*60)\n",
    "    mapper = FAERSRxNormMapper()\n",
    "    for col in ['rxnorm_name', 'rxcui', 'mapping_score', 'mapping_method']:\n",
    "        if col not in unique_drugs_df.columns:\n",
    "            unique_drugs_df[col] = None\n",
    "    total_drugs = len(unique_drugs_df)\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        for idx, row in unique_drugs_df.iloc[start_from:].iterrows():\n",
    "            if pd.notna(unique_drugs_df.loc[idx, 'rxcui']): continue\n",
    "            result = mapper.get_standardized_drug_name(row['drugname'])\n",
    "            if result:\n",
    "                unique_drugs_df.loc[idx, ['rxnorm_name', 'rxcui', 'mapping_score', 'mapping_method']] = result.values()\n",
    "            if (idx + 1) % batch_size == 0 or (idx + 1) == total_drugs:\n",
    "                processed = idx + 1\n",
    "                rate = processed / (time.time() - start_time) if (time.time() - start_time) > 0 else 0\n",
    "                success_rate = unique_drugs_df['rxcui'].notna().sum() / processed * 100\n",
    "                print(f\"  Progress: {processed}/{total_drugs} ({processed/total_drugs:.1%}) | Success: {success_rate:.1f}% | Rate: {rate:.1f} drugs/sec\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted. Saving progress...\")\n",
    "    finally:\n",
    "        mapper.close_connection()\n",
    "    final_file = 'faers_unique_drugs_mapped_final.csv'\n",
    "    unique_drugs_df.to_csv(final_file, index=False)\n",
    "    print(f\"\\nMapping complete! Final results saved to: {final_file}\")\n",
    "    return unique_drugs_df\n",
    "\n",
    "def apply_mappings_to_faers(original_file: str, mapping_df: pd.DataFrame, output_file: str, chunksize: int = 50000):\n",
    "    \"\"\"Applies the RxNorm mappings back to the full FAERS file.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\nSTEP 4: APPLYING MAPPINGS TO FULL FILE\\n\" + \"=\"*60)\n",
    "    mapping_df.set_index('drugname', inplace=True)\n",
    "    is_first_chunk = True\n",
    "    for chunk in pd.read_csv(original_file, chunksize=chunksize, low_memory=False):\n",
    "        # Clean drugname before merging\n",
    "        chunk['drugname_clean'] = chunk['drugname'].str.strip()\n",
    "        merged_chunk = chunk.merge(mapping_df, left_on='drugname_clean', right_index=True, how='left').drop(columns=['drugname_clean'])\n",
    "        \n",
    "        mode = 'w' if is_first_chunk else 'a'\n",
    "        header = is_first_chunk\n",
    "        merged_chunk.to_csv(output_file, index=False, mode=mode, header=header)\n",
    "        is_first_chunk = False\n",
    "    print(f\"Processing Complete! Full file with mappings saved to: {output_file}\")\n",
    "\n",
    "# ================================================================================\n",
    "# MAIN EXECUTION BLOCK\n",
    "# ================================================================================\n",
    "\n",
    "# --- Configuration ---\n",
    "# Updated to use the specific file path provided by the user.\n",
    "FAERS_FILE = \"/Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/processed_cleaned/faers_drugs_primary_suspects.csv\"\n",
    "FINAL_OUTPUT_FILE = \"/Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/processed_cleaned/faers_drug_rxnorm_standardized.csv\"\n",
    "\n",
    "# --- Run Pipeline ---\n",
    "print(\"=\"*80)\n",
    "print(\"      STARTING FAERS DRUG STANDARDIZATION PIPELINE      \")\n",
    "print(f\"      Input File: {FAERS_FILE}      \")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# The sample file creation is commented out as we are now using a real file path.\n",
    "# create_sample_faers_file(FAERS_FILE) \n",
    "\n",
    "# Step 1: Extract unique drugs\n",
    "unique_drugs_df = extract_unique_faers_drugs(FAERS_FILE)\n",
    "\n",
    "# Step 2: Calculate frequencies (sorts by most common for efficient mapping)\n",
    "unique_drugs_df = calculate_drug_frequencies(FAERS_FILE, unique_drugs_df)\n",
    "\n",
    "# Step 3: Map to RxNorm (this is the most time-consuming step)\n",
    "mapped_drugs_df = map_unique_drugs_batch(unique_drugs_df)\n",
    "\n",
    "# Step 4: Apply mappings back to the original full file\n",
    "apply_mappings_to_faers(\n",
    "    original_file=FAERS_FILE,\n",
    "    mapping_df=mapped_drugs_df,\n",
    "    output_file=FINAL_OUTPUT_FILE\n",
    ")\n",
    "\n",
    "print(\"\\n\\n✅ PIPELINE COMPLETE!\")\n",
    "print(f\"The final standardized file is available at: '{FINAL_OUTPUT_FILE}'\")\n",
    "print(\"You can now proceed with analysis on this file.\")\n",
    "print(\"=\"*80)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
