{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: chromadb in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (1.0.15)\n",
      "Requirement already satisfied: sentence-transformers in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (5.0.0)\n",
      "Requirement already satisfied: tqdm in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (2.11.7)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (1.4.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (4.14.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (1.35.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (0.21.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (1.64.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (3.11.0)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (13.7.1)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from chromadb) (4.25.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.53.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: scipy in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (0.33.4)\n",
      "Requirement already satisfied: Pillow in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: packaging>=19.1 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: anyio in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: filelock in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
      "Requirement already satisfied: requests in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: coloredlogs in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (6.31.1)\n",
      "Requirement already satisfied: sympy in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.35.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.35.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.56b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: networkx in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.0.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.11.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/deliciamagdaline/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas chromadb sentence-transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configure pandas and tqdm\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 MEMORY-EFFICIENT FAERS PROCESSOR\n",
      "================================================================================\n",
      "💾 Memory usage: 62.9%\n",
      "\n",
      "================================================================================\n",
      "🚀 PROCESSING: DEMO_DF\n",
      "================================================================================\n",
      "Found 96 unique files for pattern demo*.txt\n",
      "Found 0 unique files for pattern DEMO*.txt\n",
      "📊 Found 96 files to process\n",
      "\n",
      "🔄 Processing 96 files in chunks of 10\n",
      "📁 Output: faers_demographics_combined.csv\n",
      "\n",
      "📦 Processing chunk 1/10 (10 files)\n",
      "💾 Memory usage: 62.9%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0bc4e96ee24266bdd00fc4040e17b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ demo13q1.txt: 223016 rows, 25 cols\n",
      "  ✅ demo13q1.txt: 223016 rows, 25 cols\n",
      "  ✅ demo13q2.txt: 171764 rows, 25 cols\n",
      "  ✅ demo13q2.txt: 171764 rows, 25 cols\n",
      "  ✅ demo13q3.txt: 185569 rows, 25 cols\n",
      "  ✅ demo13q3.txt: 185569 rows, 25 cols\n",
      "  ✅ demo13q4.txt: 232247 rows, 25 cols\n",
      "  ✅ demo13q4.txt: 232247 rows, 25 cols\n",
      "  ✅ demo14q1.txt: 260057 rows, 25 cols\n",
      "  ✅ demo14q1.txt: 260057 rows, 25 cols\n",
      "  🔗 Combining 10 dataframes from chunk 1\n",
      "  ✅ Chunk 1: 2145306 rows appended (Total: 2145306)\n",
      "  💾 Memory after chunk 1: 62.5%\n",
      "\n",
      "📦 Processing chunk 2/10 (10 files)\n",
      "💾 Memory usage: 62.5%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae119b091ea84dd08ba858bd69ae06f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ demo14q2.txt: 223845 rows, 25 cols\n",
      "  ✅ demo14q2.txt: 223845 rows, 25 cols\n",
      "  ✅ demo14q3.txt: 211308 rows, 28 cols\n",
      "  ✅ demo14q3.txt: 211308 rows, 28 cols\n",
      "  ✅ demo14q4.txt: 207964 rows, 28 cols\n",
      "  ✅ demo14q4.txt: 207964 rows, 28 cols\n",
      "  ✅ demo15q1.txt: 317071 rows, 28 cols\n",
      "  ✅ demo15q1.txt: 317071 rows, 28 cols\n",
      "  ✅ demo15q2.txt: 289270 rows, 28 cols\n",
      "  ✅ demo15q2.txt: 289270 rows, 28 cols\n",
      "  🔗 Combining 10 dataframes from chunk 2\n",
      "  ✅ Chunk 2: 2498916 rows appended (Total: 4644222)\n",
      "  💾 Memory after chunk 2: 60.8%\n",
      "\n",
      "📦 Processing chunk 3/10 (10 files)\n",
      "💾 Memory usage: 60.8%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df386e4eff1c4790bbbe43614a7db5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ demo15q3.txt: 398860 rows, 28 cols\n",
      "  ✅ demo15q3.txt: 398860 rows, 28 cols\n",
      "  ✅ demo15q4.txt: 314704 rows, 28 cols\n",
      "  ✅ demo15q4.txt: 314704 rows, 28 cols\n",
      "  ✅ demo16q1.txt: 365682 rows, 28 cols\n",
      "  ✅ demo16q1.txt: 365682 rows, 28 cols\n",
      "  ✅ demo16q2.txt: 316056 rows, 28 cols\n",
      "  ✅ demo16q2.txt: 316056 rows, 28 cols\n",
      "  ✅ demo16q3.txt: 313613 rows, 28 cols\n",
      "  ✅ demo16q3.txt: 313613 rows, 28 cols\n",
      "  🔗 Combining 10 dataframes from chunk 3\n",
      "  ✅ Chunk 3: 3417830 rows appended (Total: 8062052)\n",
      "  💾 Memory after chunk 3: 54.7%\n",
      "\n",
      "📦 Processing chunk 4/10 (10 files)\n",
      "💾 Memory usage: 54.7%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94053e26054460b98ae19c2215ed623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ demo16q4.txt: 309534 rows, 28 cols\n",
      "  ✅ demo16q4.txt: 309534 rows, 28 cols\n",
      "  ✅ demo17q1.txt: 352913 rows, 28 cols\n",
      "  ✅ demo17q1.txt: 352913 rows, 28 cols\n",
      "  ✅ demo17q2.txt: 337398 rows, 28 cols\n",
      "  ✅ demo17q2.txt: 337398 rows, 28 cols\n",
      "  ✅ demo17q3.txt: 351388 rows, 28 cols\n",
      "  ✅ demo17q3.txt: 351388 rows, 28 cols\n",
      "  ✅ demo17q4.txt: 327848 rows, 28 cols\n",
      "  ✅ demo17q4.txt: 327848 rows, 28 cols\n",
      "  🔗 Combining 10 dataframes from chunk 4\n",
      "  ✅ Chunk 4: 3358162 rows appended (Total: 11420214)\n",
      "  💾 Memory after chunk 4: 54.8%\n",
      "\n",
      "📦 Processing chunk 5/10 (10 files)\n",
      "💾 Memory usage: 54.8%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7514e19cf7fe4933b5c59b4ecf41d765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 5:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ demo18q1_new.txt: 412702 rows, 28 cols\n",
      "  ✅ demo18q1_new.txt: 412702 rows, 28 cols\n",
      "  ✅ demo18q2.txt: 457169 rows, 28 cols\n",
      "  ✅ demo18q2.txt: 457169 rows, 28 cols\n",
      "  ✅ demo18q3.txt: 420915 rows, 28 cols\n",
      "  ✅ demo18q3.txt: 420915 rows, 28 cols\n",
      "  ✅ demo18q4.txt: 394066 rows, 28 cols\n",
      "  ✅ demo18q4.txt: 394066 rows, 28 cols\n",
      "  ✅ demo19q1.txt: 413734 rows, 28 cols\n",
      "  ✅ demo19q1.txt: 413734 rows, 28 cols\n",
      "  🔗 Combining 10 dataframes from chunk 5\n",
      "  ✅ Chunk 5: 4197172 rows appended (Total: 15617386)\n",
      "  💾 Memory after chunk 5: 57.3%\n",
      "\n",
      "📦 Processing chunk 6/10 (10 files)\n",
      "💾 Memory usage: 57.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4e9f6c62c74836b359ab2ac08a22e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 6:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ demo19q2.txt: 441108 rows, 28 cols\n",
      "  ✅ demo19q2.txt: 441108 rows, 28 cols\n",
      "  ✅ demo19q3.txt: 452873 rows, 28 cols\n",
      "  ✅ demo19q3.txt: 452873 rows, 28 cols\n",
      "  ✅ demo19q4.txt: 419581 rows, 28 cols\n",
      "  ✅ demo19q4.txt: 419581 rows, 28 cols\n",
      "  ✅ demo20q1.txt: 460327 rows, 28 cols\n",
      "  ✅ demo20q1.txt: 460327 rows, 28 cols\n",
      "  ✅ demo20q2.txt: 429227 rows, 28 cols\n",
      "  ✅ demo20q2.txt: 429227 rows, 28 cols\n",
      "  🔗 Combining 10 dataframes from chunk 6\n",
      "  ✅ Chunk 6: 4406232 rows appended (Total: 20023618)\n",
      "  💾 Memory after chunk 6: 56.6%\n",
      "\n",
      "📦 Processing chunk 7/10 (10 files)\n",
      "💾 Memory usage: 56.6%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fae355d011b4ee79ae75f4126ddaf90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 7:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ demo20q3.txt: 431667 rows, 28 cols\n",
      "  ✅ demo20q3.txt: 431667 rows, 28 cols\n",
      "  ✅ demo20q4.txt: 436148 rows, 28 cols\n",
      "  ✅ demo20q4.txt: 436148 rows, 28 cols\n",
      "  ✅ demo21q1.txt: 463741 rows, 28 cols\n",
      "  ✅ demo21q1.txt: 463741 rows, 28 cols\n",
      "  ✅ demo21q2.txt: 479945 rows, 28 cols\n",
      "  ✅ demo21q2.txt: 479945 rows, 28 cols\n",
      "  ✅ demo21q3.txt: 504160 rows, 28 cols\n",
      "  ✅ demo21q3.txt: 504160 rows, 28 cols\n",
      "  🔗 Combining 10 dataframes from chunk 7\n",
      "  ✅ Chunk 7: 4631322 rows appended (Total: 24654940)\n",
      "  💾 Memory after chunk 7: 58.7%\n",
      "\n",
      "📦 Processing chunk 8/10 (10 files)\n",
      "💾 Memory usage: 58.7%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5a44b2679a42d1920868011cc8af4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 8:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ demo21q4.txt: 412542 rows, 28 cols\n",
      "  ✅ demo21q4.txt: 412542 rows, 28 cols\n",
      "  ✅ demo22q1.txt: 461623 rows, 28 cols\n",
      "  ✅ demo22q1.txt: 461623 rows, 28 cols\n",
      "  ✅ demo22q2.txt: 435618 rows, 28 cols\n",
      "  ✅ demo22q2.txt: 435618 rows, 28 cols\n",
      "  ✅ demo22q3.txt: 446511 rows, 28 cols\n",
      "  ✅ demo22q3.txt: 446511 rows, 28 cols\n",
      "  ✅ demo22q4.txt: 483643 rows, 28 cols\n",
      "  ✅ demo22q4.txt: 483643 rows, 28 cols\n",
      "  🔗 Combining 10 dataframes from chunk 8\n",
      "  ✅ Chunk 8: 4479874 rows appended (Total: 29134814)\n",
      "  💾 Memory after chunk 8: 55.9%\n",
      "\n",
      "📦 Processing chunk 9/10 (10 files)\n",
      "💾 Memory usage: 55.9%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9c9d2d09a04a6d84f313d57a74502c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 9:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ demo23q1.txt: 432144 rows, 28 cols\n",
      "  ✅ demo23q1.txt: 432144 rows, 28 cols\n",
      "  ✅ demo23q2.txt: 418592 rows, 28 cols\n",
      "  ✅ demo23q2.txt: 418592 rows, 28 cols\n",
      "  ✅ demo23q3.txt: 407522 rows, 28 cols\n",
      "  ✅ demo23q3.txt: 407522 rows, 28 cols\n",
      "  ✅ demo23q4.txt: 415379 rows, 28 cols\n",
      "  ✅ demo23q4.txt: 415379 rows, 28 cols\n",
      "  ✅ demo24q1.txt: 406184 rows, 28 cols\n",
      "  ✅ demo24q1.txt: 406184 rows, 28 cols\n",
      "  🔗 Combining 10 dataframes from chunk 9\n",
      "  ✅ Chunk 9: 4159642 rows appended (Total: 33294456)\n",
      "  💾 Memory after chunk 9: 54.5%\n",
      "\n",
      "📦 Processing chunk 10/10 (6 files)\n",
      "💾 Memory usage: 54.5%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02554bb047db42808c455795307e7d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 10:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ demo24q2.txt: 397119 rows, 28 cols\n",
      "  ✅ demo24q2.txt: 397119 rows, 28 cols\n",
      "  ✅ demo24q3.txt: 405513 rows, 28 cols\n",
      "  ✅ demo24q3.txt: 405513 rows, 28 cols\n",
      "  ✅ demo24q4.txt: 410849 rows, 28 cols\n",
      "  ✅ demo24q4.txt: 410849 rows, 28 cols\n",
      "  🔗 Combining 6 dataframes from chunk 10\n",
      "  ✅ Chunk 10: 2426962 rows appended (Total: 35721418)\n",
      "  💾 Memory after chunk 10: 58.4%\n",
      "\n",
      "🎉 Successfully created faers_demographics_combined.csv with 35,721,418 total rows\n",
      "\n",
      "================================================================================\n",
      "🚀 PROCESSING: DRUG_DF\n",
      "================================================================================\n",
      "Found 96 unique files for pattern drug*.txt\n",
      "Found 0 unique files for pattern DRUG*.txt\n",
      "📊 Found 96 files to process\n",
      "\n",
      "🔄 Processing 96 files in chunks of 10\n",
      "📁 Output: faers_drugs_combined.csv\n",
      "\n",
      "📦 Processing chunk 1/10 (10 files)\n",
      "💾 Memory usage: 57.5%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e60cad6c7aa4349a9329b9bd3deb173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ drug13q1.txt: 737725 rows, 22 cols\n",
      "  ✅ drug13q1.txt: 737725 rows, 22 cols\n",
      "  ✅ drug13q2.txt: 564548 rows, 22 cols\n",
      "  ✅ drug13q2.txt: 564548 rows, 22 cols\n",
      "  ✅ drug13q3.txt: 607921 rows, 22 cols\n",
      "  ✅ drug13q3.txt: 607921 rows, 22 cols\n",
      "  ✅ drug13q4.txt: 738501 rows, 22 cols\n",
      "  ✅ drug13q4.txt: 738501 rows, 22 cols\n",
      "  ✅ drug14q1.txt: 901608 rows, 22 cols\n",
      "  ✅ drug14q1.txt: 901608 rows, 22 cols\n",
      "  🔗 Combining 10 dataframes from chunk 1\n",
      "  ✅ Chunk 1: 7100606 rows appended (Total: 7100606)\n",
      "  💾 Memory after chunk 1: 51.8%\n",
      "\n",
      "📦 Processing chunk 2/10 (10 files)\n",
      "💾 Memory usage: 51.8%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba3e10014734cc48860fb991295ea4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ drug14q2.txt: 683829 rows, 22 cols\n",
      "  ✅ drug14q2.txt: 683829 rows, 22 cols\n",
      "  ✅ drug14q3.txt: 688140 rows, 23 cols\n",
      "  ✅ drug14q3.txt: 688140 rows, 23 cols\n",
      "  ✅ drug14q4.txt: 682292 rows, 23 cols\n",
      "  ✅ drug14q4.txt: 682292 rows, 23 cols\n",
      "  ✅ drug15q1.txt: 1082833 rows, 23 cols\n",
      "  ✅ drug15q1.txt: 1082833 rows, 23 cols\n",
      "  ✅ drug15q2.txt: 967120 rows, 23 cols\n",
      "  ✅ drug15q2.txt: 967120 rows, 23 cols\n",
      "  🔗 Combining 10 dataframes from chunk 2\n",
      "  ✅ Chunk 2: 8208428 rows appended (Total: 15309034)\n",
      "  💾 Memory after chunk 2: 53.0%\n",
      "\n",
      "📦 Processing chunk 3/10 (10 files)\n",
      "💾 Memory usage: 53.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68b5b82be7a41a88beae0ab031c6a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ drug15q3.txt: 1294625 rows, 23 cols\n",
      "  ✅ drug15q3.txt: 1294625 rows, 23 cols\n",
      "  ✅ drug15q4.txt: 1064332 rows, 23 cols\n",
      "  ✅ drug15q4.txt: 1064332 rows, 23 cols\n",
      "  ✅ drug16q1.txt: 1202390 rows, 23 cols\n",
      "  ✅ drug16q1.txt: 1202390 rows, 23 cols\n",
      "  ✅ drug16q2.txt: 1144148 rows, 23 cols\n",
      "  ✅ drug16q2.txt: 1144148 rows, 23 cols\n",
      "  ✅ drug16q3.txt: 1257054 rows, 23 cols\n",
      "  ✅ drug16q3.txt: 1257054 rows, 23 cols\n",
      "  🔗 Combining 10 dataframes from chunk 3\n",
      "  ✅ Chunk 3: 11925098 rows appended (Total: 27234132)\n",
      "  💾 Memory after chunk 3: 51.1%\n",
      "\n",
      "📦 Processing chunk 4/10 (10 files)\n",
      "💾 Memory usage: 51.1%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091312b9ef4f4a3181133c4aea75ff41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ drug16q4.txt: 1147547 rows, 23 cols\n",
      "  ✅ drug16q4.txt: 1147547 rows, 23 cols\n",
      "  ✅ drug17q1.txt: 1234393 rows, 23 cols\n",
      "  ✅ drug17q1.txt: 1234393 rows, 23 cols\n",
      "  ✅ drug17q2.txt: 1188078 rows, 23 cols\n",
      "  ✅ drug17q2.txt: 1188078 rows, 23 cols\n",
      "  ✅ drug17q3.txt: 1220118 rows, 23 cols\n",
      "  ✅ drug17q3.txt: 1220118 rows, 23 cols\n",
      "  ✅ drug17q4.txt: 1213842 rows, 23 cols\n",
      "  ✅ drug17q4.txt: 1213842 rows, 23 cols\n",
      "  🔗 Combining 10 dataframes from chunk 4\n",
      "  ✅ Chunk 4: 12007956 rows appended (Total: 39242088)\n",
      "  💾 Memory after chunk 4: 51.6%\n",
      "\n",
      "📦 Processing chunk 5/10 (10 files)\n",
      "💾 Memory usage: 51.5%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee79aa2113b492abc369fb666ee4d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 5:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ drug18q1.txt: 1686461 rows, 23 cols\n",
      "  ✅ drug18q1.txt: 1686461 rows, 23 cols\n",
      "  ✅ drug18q2.txt: 1987872 rows, 23 cols\n",
      "  ✅ drug18q2.txt: 1987872 rows, 23 cols\n",
      "  ✅ drug18q3.txt: 1651966 rows, 23 cols\n",
      "  ✅ drug18q3.txt: 1651966 rows, 23 cols\n",
      "  ✅ drug18q4.txt: 1546835 rows, 23 cols\n",
      "  ✅ drug18q4.txt: 1546835 rows, 23 cols\n",
      "  ✅ drug19q1.txt: 1648987 rows, 23 cols\n",
      "  ✅ drug19q1.txt: 1648987 rows, 23 cols\n",
      "  🔗 Combining 10 dataframes from chunk 5\n",
      "  ✅ Chunk 5: 17044242 rows appended (Total: 56286330)\n",
      "  💾 Memory after chunk 5: 43.2%\n",
      "\n",
      "📦 Processing chunk 6/10 (10 files)\n",
      "💾 Memory usage: 43.2%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfa4b05c7ad4fc6b9db0c7d25d554ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 6:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ drug19q2.txt: 1863010 rows, 23 cols\n",
      "  ✅ drug19q2.txt: 1863010 rows, 23 cols\n",
      "  ✅ drug19q3.txt: 1982442 rows, 23 cols\n",
      "  ✅ drug19q3.txt: 1982442 rows, 23 cols\n",
      "  ✅ drug19q4.txt: 1714177 rows, 23 cols\n",
      "  ✅ drug19q4.txt: 1714177 rows, 23 cols\n",
      "  ✅ drug20q1.txt: 1943532 rows, 23 cols\n",
      "  ✅ drug20q1.txt: 1943532 rows, 23 cols\n",
      "  ✅ drug20q2.txt: 1825414 rows, 23 cols\n",
      "  ✅ drug20q2.txt: 1825414 rows, 23 cols\n",
      "  🔗 Combining 10 dataframes from chunk 6\n",
      "  ✅ Chunk 6: 18657150 rows appended (Total: 74943480)\n",
      "  💾 Memory after chunk 6: 45.0%\n",
      "\n",
      "📦 Processing chunk 7/10 (10 files)\n",
      "💾 Memory usage: 45.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5392c6ba42eb44bc9351c159b66ad148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 7:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ drug20q3.txt: 1895153 rows, 23 cols\n",
      "  ✅ drug20q3.txt: 1895153 rows, 23 cols\n",
      "  ✅ drug20q4.txt: 1918927 rows, 23 cols\n",
      "  ✅ drug20q4.txt: 1918927 rows, 23 cols\n",
      "  ✅ drug21q1.txt: 2208416 rows, 23 cols\n",
      "  ✅ drug21q1.txt: 2208416 rows, 23 cols\n",
      "  ✅ drug21q2.txt: 2291903 rows, 23 cols\n",
      "  ✅ drug21q2.txt: 2291903 rows, 23 cols\n",
      "  ✅ drug21q3.txt: 2260570 rows, 23 cols\n",
      "  ✅ drug21q3.txt: 2260570 rows, 23 cols\n",
      "  🔗 Combining 10 dataframes from chunk 7\n",
      "  ✅ Chunk 7: 21149938 rows appended (Total: 96093418)\n",
      "  💾 Memory after chunk 7: 46.2%\n",
      "\n",
      "📦 Processing chunk 8/10 (10 files)\n",
      "💾 Memory usage: 46.1%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9392b944b2546f0aef0bef5ae74aaa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 8:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ drug21q4.txt: 1778675 rows, 23 cols\n",
      "  ✅ drug21q4.txt: 1778675 rows, 23 cols\n",
      "  ✅ drug22q1.txt: 1994171 rows, 23 cols\n",
      "  ✅ drug22q1.txt: 1994171 rows, 23 cols\n",
      "  ✅ drug22q2.txt: 1828103 rows, 23 cols\n",
      "  ✅ drug22q2.txt: 1828103 rows, 23 cols\n",
      "  ✅ drug22q3.txt: 1835461 rows, 23 cols\n",
      "  ✅ drug22q3.txt: 1835461 rows, 23 cols\n",
      "  ✅ drug22q4.txt: 2006967 rows, 23 cols\n",
      "  ✅ drug22q4.txt: 2006967 rows, 23 cols\n",
      "  🔗 Combining 10 dataframes from chunk 8\n",
      "  ✅ Chunk 8: 18886754 rows appended (Total: 114980172)\n",
      "  💾 Memory after chunk 8: 56.2%\n",
      "\n",
      "📦 Processing chunk 9/10 (10 files)\n",
      "💾 Memory usage: 56.2%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92b7245261949c28323472978696d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 9:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ drug23q1.txt: 1899503 rows, 23 cols\n",
      "  ✅ drug23q1.txt: 1899503 rows, 23 cols\n",
      "  ✅ drug23q2.txt: 1885096 rows, 23 cols\n",
      "  ✅ drug23q2.txt: 1885096 rows, 23 cols\n",
      "  ✅ drug23q3.txt: 1768391 rows, 23 cols\n",
      "  ✅ drug23q3.txt: 1768391 rows, 23 cols\n",
      "  ✅ drug23q4.txt: 1920732 rows, 23 cols\n",
      "  ✅ drug23q4.txt: 1920732 rows, 23 cols\n",
      "  ✅ drug24q1.txt: 1909327 rows, 23 cols\n",
      "  ✅ drug24q1.txt: 1909327 rows, 23 cols\n",
      "  🔗 Combining 10 dataframes from chunk 9\n",
      "  ✅ Chunk 9: 18766098 rows appended (Total: 133746270)\n",
      "  💾 Memory after chunk 9: 49.6%\n",
      "\n",
      "📦 Processing chunk 10/10 (6 files)\n",
      "💾 Memory usage: 49.5%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5978d8ddd848e6a8da088f46c632d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 10:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ drug24q2.txt: 1888937 rows, 23 cols\n",
      "  ✅ drug24q2.txt: 1888937 rows, 23 cols\n",
      "  ✅ drug24q3.txt: 1907293 rows, 23 cols\n",
      "  ✅ drug24q3.txt: 1907293 rows, 23 cols\n",
      "  ✅ drug24q4.txt: 2030938 rows, 23 cols\n",
      "  ✅ drug24q4.txt: 2030938 rows, 23 cols\n",
      "  🔗 Combining 6 dataframes from chunk 10\n",
      "  ✅ Chunk 10: 11654336 rows appended (Total: 145400606)\n",
      "  💾 Memory after chunk 10: 44.6%\n",
      "\n",
      "🎉 Successfully created faers_drugs_combined.csv with 145,400,606 total rows\n",
      "\n",
      "================================================================================\n",
      "🚀 PROCESSING: REAC_DF\n",
      "================================================================================\n",
      "Found 96 unique files for pattern reac*.txt\n",
      "Found 0 unique files for pattern REAC*.txt\n",
      "📊 Found 96 files to process\n",
      "\n",
      "🔄 Processing 96 files in chunks of 10\n",
      "📁 Output: faers_reactions_combined.csv\n",
      "\n",
      "📦 Processing chunk 1/10 (10 files)\n",
      "💾 Memory usage: 41.4%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11525c05f7724d49b13ed81c246f8a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ reac13q1.txt: 658002 rows, 6 cols\n",
      "  ✅ reac13q1.txt: 658002 rows, 6 cols\n",
      "  ✅ reac13q2.txt: 544835 rows, 6 cols\n",
      "  ✅ reac13q2.txt: 544835 rows, 6 cols\n",
      "  ✅ reac13q3.txt: 584833 rows, 6 cols\n",
      "  ✅ reac13q3.txt: 584833 rows, 6 cols\n",
      "  ✅ reac13q4.txt: 690690 rows, 6 cols\n",
      "  ✅ reac13q4.txt: 690690 rows, 6 cols\n",
      "  ✅ reac14q1.txt: 754668 rows, 6 cols\n",
      "  ✅ reac14q1.txt: 754668 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 1\n",
      "  ✅ Chunk 1: 6466056 rows appended (Total: 6466056)\n",
      "  💾 Memory after chunk 1: 44.0%\n",
      "\n",
      "📦 Processing chunk 2/10 (10 files)\n",
      "💾 Memory usage: 43.9%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3baa637600a442738bf812844c4635db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ reac14q2.txt: 652959 rows, 6 cols\n",
      "  ✅ reac14q2.txt: 652959 rows, 6 cols\n",
      "  ✅ reac14q3.txt: 667778 rows, 7 cols\n",
      "  ✅ reac14q3.txt: 667778 rows, 7 cols\n",
      "  ✅ reac14q4.txt: 635465 rows, 7 cols\n",
      "  ✅ reac14q4.txt: 635465 rows, 7 cols\n",
      "  ✅ reac15q1.txt: 872848 rows, 7 cols\n",
      "  ✅ reac15q1.txt: 872848 rows, 7 cols\n",
      "  ✅ reac15q2.txt: 845134 rows, 7 cols\n",
      "  ✅ reac15q2.txt: 845134 rows, 7 cols\n",
      "  🔗 Combining 10 dataframes from chunk 2\n",
      "  ✅ Chunk 2: 7348368 rows appended (Total: 13814424)\n",
      "  💾 Memory after chunk 2: 48.8%\n",
      "\n",
      "📦 Processing chunk 3/10 (10 files)\n",
      "💾 Memory usage: 48.7%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83ac7b000394993bdc34c1dc4dc435b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ reac15q3.txt: 1133196 rows, 7 cols\n",
      "  ✅ reac15q3.txt: 1133196 rows, 7 cols\n",
      "  ✅ reac15q4.txt: 968664 rows, 7 cols\n",
      "  ✅ reac15q4.txt: 968664 rows, 7 cols\n",
      "  ✅ reac16q1.txt: 1013744 rows, 7 cols\n",
      "  ✅ reac16q1.txt: 1013744 rows, 7 cols\n",
      "  ✅ reac16q2.txt: 914638 rows, 7 cols\n",
      "  ✅ reac16q2.txt: 914638 rows, 7 cols\n",
      "  ✅ reac16q3.txt: 936744 rows, 7 cols\n",
      "  ✅ reac16q3.txt: 936744 rows, 7 cols\n",
      "  🔗 Combining 10 dataframes from chunk 3\n",
      "  ✅ Chunk 3: 9933972 rows appended (Total: 23748396)\n",
      "  💾 Memory after chunk 3: 59.6%\n",
      "\n",
      "📦 Processing chunk 4/10 (10 files)\n",
      "💾 Memory usage: 59.6%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7dca7590f4493fa5086fdcf3616a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ reac16q4.txt: 916308 rows, 7 cols\n",
      "  ✅ reac16q4.txt: 916308 rows, 7 cols\n",
      "  ✅ reac17q1.txt: 1041509 rows, 7 cols\n",
      "  ✅ reac17q1.txt: 1041509 rows, 7 cols\n",
      "  ✅ reac17q2.txt: 987728 rows, 7 cols\n",
      "  ✅ reac17q2.txt: 987728 rows, 7 cols\n",
      "  ✅ reac17q3.txt: 1025153 rows, 7 cols\n",
      "  ✅ reac17q3.txt: 1025153 rows, 7 cols\n",
      "  ✅ reac17q4.txt: 961640 rows, 7 cols\n",
      "  ✅ reac17q4.txt: 961640 rows, 7 cols\n",
      "  🔗 Combining 10 dataframes from chunk 4\n",
      "  ✅ Chunk 4: 9864676 rows appended (Total: 33613072)\n",
      "  💾 Memory after chunk 4: 57.7%\n",
      "\n",
      "📦 Processing chunk 5/10 (10 files)\n",
      "💾 Memory usage: 57.7%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1d9cf8a0314445bb174ddb16803573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 5:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ reac18q1.txt: 1206259 rows, 7 cols\n",
      "  ✅ reac18q1.txt: 1206259 rows, 7 cols\n",
      "  ✅ reac18q2.txt: 1392439 rows, 7 cols\n",
      "  ✅ reac18q2.txt: 1392439 rows, 7 cols\n",
      "  ✅ reac18q3.txt: 1329530 rows, 7 cols\n",
      "  ✅ reac18q3.txt: 1329530 rows, 7 cols\n",
      "  ✅ reac18q4.txt: 1250978 rows, 7 cols\n",
      "  ✅ reac18q4.txt: 1250978 rows, 7 cols\n",
      "  ✅ reac19q1.txt: 1303532 rows, 7 cols\n",
      "  ✅ reac19q1.txt: 1303532 rows, 7 cols\n",
      "  🔗 Combining 10 dataframes from chunk 5\n",
      "  ✅ Chunk 5: 12965476 rows appended (Total: 46578548)\n",
      "  💾 Memory after chunk 5: 65.5%\n",
      "\n",
      "📦 Processing chunk 6/10 (10 files)\n",
      "💾 Memory usage: 65.4%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7b6f10cfbc49dcbbde1a95244a6a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 6:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ reac19q2.txt: 1408482 rows, 7 cols\n",
      "  ✅ reac19q2.txt: 1408482 rows, 7 cols\n",
      "  ✅ reac19q3.txt: 1504790 rows, 7 cols\n",
      "  ✅ reac19q3.txt: 1504790 rows, 7 cols\n",
      "  ✅ reac19q4.txt: 1359922 rows, 7 cols\n",
      "  ✅ reac19q4.txt: 1359922 rows, 7 cols\n",
      "  ✅ reac20q1.txt: 1517264 rows, 7 cols\n",
      "  ✅ reac20q1.txt: 1517264 rows, 7 cols\n",
      "  ✅ reac20q2.txt: 1437285 rows, 7 cols\n",
      "  ✅ reac20q2.txt: 1437285 rows, 7 cols\n",
      "  🔗 Combining 10 dataframes from chunk 6\n",
      "  ✅ Chunk 6: 14455486 rows appended (Total: 61034034)\n",
      "  💾 Memory after chunk 6: 57.0%\n",
      "\n",
      "📦 Processing chunk 7/10 (10 files)\n",
      "💾 Memory usage: 57.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890e7e6fa5c3471787256560d9528622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 7:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ reac20q3.txt: 1454044 rows, 7 cols\n",
      "  ✅ reac20q3.txt: 1454044 rows, 7 cols\n",
      "  ✅ reac20q4.txt: 1522657 rows, 7 cols\n",
      "  ✅ reac20q4.txt: 1522657 rows, 7 cols\n",
      "  ✅ reac21q1.txt: 1505167 rows, 7 cols\n",
      "  ✅ reac21q1.txt: 1505167 rows, 7 cols\n",
      "  ✅ reac21q2.txt: 1526544 rows, 7 cols\n",
      "  ✅ reac21q2.txt: 1526544 rows, 7 cols\n",
      "  ✅ reac21q3.txt: 1544374 rows, 7 cols\n",
      "  ✅ reac21q3.txt: 1544374 rows, 7 cols\n",
      "  🔗 Combining 10 dataframes from chunk 7\n",
      "  ✅ Chunk 7: 15105572 rows appended (Total: 76139606)\n",
      "  💾 Memory after chunk 7: 57.4%\n",
      "\n",
      "📦 Processing chunk 8/10 (10 files)\n",
      "💾 Memory usage: 57.4%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e9475eac174819b0f86d4c815abf78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 8:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ reac21q4.txt: 1355734 rows, 7 cols\n",
      "  ✅ reac21q4.txt: 1355734 rows, 7 cols\n",
      "  ✅ reac22q1.txt: 1543059 rows, 7 cols\n",
      "  ✅ reac22q1.txt: 1543059 rows, 7 cols\n",
      "  ✅ reac22q2.txt: 1464627 rows, 7 cols\n",
      "  ✅ reac22q2.txt: 1464627 rows, 7 cols\n",
      "  ✅ reac22q3.txt: 1449509 rows, 7 cols\n",
      "  ✅ reac22q3.txt: 1449509 rows, 7 cols\n",
      "  ✅ reac22q4.txt: 1617584 rows, 7 cols\n",
      "  ✅ reac22q4.txt: 1617584 rows, 7 cols\n",
      "  🔗 Combining 10 dataframes from chunk 8\n",
      "  ✅ Chunk 8: 14861026 rows appended (Total: 91000632)\n",
      "  💾 Memory after chunk 8: 55.6%\n",
      "\n",
      "📦 Processing chunk 9/10 (10 files)\n",
      "💾 Memory usage: 55.5%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000faaa97be64c72968b55c85599aa06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 9:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ reac23q1.txt: 1491473 rows, 7 cols\n",
      "  ✅ reac23q1.txt: 1491473 rows, 7 cols\n",
      "  ✅ reac23q2.txt: 1478973 rows, 7 cols\n",
      "  ✅ reac23q2.txt: 1478973 rows, 7 cols\n",
      "  ✅ reac23q3.txt: 1373338 rows, 7 cols\n",
      "  ✅ reac23q3.txt: 1373338 rows, 7 cols\n",
      "  ✅ reac23q4.txt: 1500033 rows, 7 cols\n",
      "  ✅ reac23q4.txt: 1500033 rows, 7 cols\n",
      "  ✅ reac24q1.txt: 1445416 rows, 7 cols\n",
      "  ✅ reac24q1.txt: 1445416 rows, 7 cols\n",
      "  🔗 Combining 10 dataframes from chunk 9\n",
      "  ✅ Chunk 9: 14578466 rows appended (Total: 105579098)\n",
      "  💾 Memory after chunk 9: 58.0%\n",
      "\n",
      "📦 Processing chunk 10/10 (6 files)\n",
      "💾 Memory usage: 58.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f451406d4274115b10f96e0f0dcba41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 10:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ reac24q2.txt: 1445044 rows, 7 cols\n",
      "  ✅ reac24q2.txt: 1445044 rows, 7 cols\n",
      "  ✅ reac24q3.txt: 1431718 rows, 7 cols\n",
      "  ✅ reac24q3.txt: 1431718 rows, 7 cols\n",
      "  ✅ reac24q4.txt: 1472750 rows, 7 cols\n",
      "  ✅ reac24q4.txt: 1472750 rows, 7 cols\n",
      "  🔗 Combining 6 dataframes from chunk 10\n",
      "  ✅ Chunk 10: 8699024 rows appended (Total: 114278122)\n",
      "  💾 Memory after chunk 10: 57.0%\n",
      "\n",
      "🎉 Successfully created faers_reactions_combined.csv with 114,278,122 total rows\n",
      "\n",
      "================================================================================\n",
      "🚀 PROCESSING: INDI_DF\n",
      "================================================================================\n",
      "Found 0 unique files for pattern indi*.txt\n",
      "Found 96 unique files for pattern INDI*.txt\n",
      "📊 Found 96 files to process\n",
      "\n",
      "🔄 Processing 96 files in chunks of 10\n",
      "📁 Output: faers_indications_combined.csv\n",
      "\n",
      "📦 Processing chunk 1/10 (10 files)\n",
      "💾 Memory usage: 56.4%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76fa2c3cc034678b65e49b7d8ab537f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ INDI13Q1.txt: 453964 rows, 7 cols\n",
      "  ✅ INDI13Q1.txt: 453964 rows, 7 cols\n",
      "  ✅ INDI13Q2.txt: 330405 rows, 7 cols\n",
      "  ✅ INDI13Q2.txt: 330405 rows, 7 cols\n",
      "  ✅ INDI13Q3.txt: 369227 rows, 7 cols\n",
      "  ✅ INDI13Q3.txt: 369227 rows, 7 cols\n",
      "  ✅ INDI13Q4.txt: 432190 rows, 7 cols\n",
      "  ✅ INDI13Q4.txt: 432190 rows, 7 cols\n",
      "  ✅ INDI14Q1.txt: 581696 rows, 7 cols\n",
      "  ✅ INDI14Q1.txt: 581696 rows, 7 cols\n",
      "  🔗 Combining 10 dataframes from chunk 1\n",
      "  ✅ Chunk 1: 4334964 rows appended (Total: 4334964)\n",
      "  💾 Memory after chunk 1: 63.4%\n",
      "\n",
      "📦 Processing chunk 2/10 (10 files)\n",
      "💾 Memory usage: 63.4%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9eab4eb1b01462185641410348eb146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ INDI14Q2.txt: 420506 rows, 7 cols\n",
      "  ✅ INDI14Q2.txt: 420506 rows, 7 cols\n",
      "  ✅ INDI14Q3.txt: 423391 rows, 7 cols\n",
      "  ✅ INDI14Q3.txt: 423391 rows, 7 cols\n",
      "  ✅ INDI14Q4.txt: 436621 rows, 7 cols\n",
      "  ✅ INDI14Q4.txt: 436621 rows, 7 cols\n",
      "  ✅ INDI15Q1.txt: 748742 rows, 7 cols\n",
      "  ✅ INDI15Q1.txt: 748742 rows, 7 cols\n",
      "  ✅ INDI15Q2.txt: 659529 rows, 7 cols\n",
      "  ✅ INDI15Q2.txt: 659529 rows, 7 cols\n",
      "  🔗 Combining 10 dataframes from chunk 2\n",
      "  ✅ Chunk 2: 5377578 rows appended (Total: 9712542)\n",
      "  💾 Memory after chunk 2: 64.1%\n",
      "\n",
      "📦 Processing chunk 3/10 (10 files)\n",
      "💾 Memory usage: 64.1%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f088c5e0b0745efbcee732718b67472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ INDI15Q3.txt: 811279 rows, 7 cols\n",
      "  ✅ INDI15Q3.txt: 811279 rows, 7 cols\n",
      "  ✅ INDI15Q4.txt: 718340 rows, 7 cols\n",
      "  ✅ INDI15Q4.txt: 718340 rows, 7 cols\n",
      "  ✅ INDI16Q1.txt: 878267 rows, 7 cols\n",
      "  ✅ INDI16Q1.txt: 878267 rows, 7 cols\n",
      "  ✅ INDI16Q2.txt: 798489 rows, 7 cols\n",
      "  ✅ INDI16Q2.txt: 798489 rows, 7 cols\n",
      "  ✅ INDI16Q3.txt: 816898 rows, 7 cols\n",
      "  ✅ INDI16Q3.txt: 816898 rows, 7 cols\n",
      "  🔗 Combining 10 dataframes from chunk 3\n",
      "  ✅ Chunk 3: 8046546 rows appended (Total: 17759088)\n",
      "  💾 Memory after chunk 3: 65.6%\n",
      "\n",
      "📦 Processing chunk 4/10 (10 files)\n",
      "💾 Memory usage: 65.6%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1cb088b4334d94b0500ccdff5c44f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ INDI16Q4.txt: 755832 rows, 7 cols\n",
      "  ✅ INDI16Q4.txt: 755832 rows, 7 cols\n",
      "  ✅ INDI17Q1.txt: 859833 rows, 7 cols\n",
      "  ✅ INDI17Q1.txt: 859833 rows, 7 cols\n",
      "  ✅ INDI17Q2.txt: 816254 rows, 7 cols\n",
      "  ✅ INDI17Q2.txt: 816254 rows, 7 cols\n",
      "  ✅ INDI17Q3.txt: 844974 rows, 7 cols\n",
      "  ✅ INDI17Q3.txt: 844974 rows, 7 cols\n",
      "  ✅ INDI17Q4.txt: 863690 rows, 7 cols\n",
      "  ✅ INDI17Q4.txt: 863690 rows, 7 cols\n",
      "  🔗 Combining 10 dataframes from chunk 4\n",
      "  ✅ Chunk 4: 8281166 rows appended (Total: 26040254)\n",
      "  💾 Memory after chunk 4: 66.6%\n",
      "\n",
      "📦 Processing chunk 5/10 (10 files)\n",
      "💾 Memory usage: 66.5%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9360792b304c471e85fe58ce327017ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 5:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ INDI18Q1.txt: 1253118 rows, 7 cols\n",
      "  ✅ INDI18Q1.txt: 1253118 rows, 7 cols\n",
      "  ✅ INDI18Q2.txt: 1428287 rows, 7 cols\n",
      "  ✅ INDI18Q2.txt: 1428287 rows, 7 cols\n",
      "  ✅ INDI18Q3.txt: 1134135 rows, 7 cols\n",
      "  ✅ INDI18Q3.txt: 1134135 rows, 7 cols\n",
      "  ✅ INDI18Q4.txt: 1064664 rows, 7 cols\n",
      "  ✅ INDI18Q4.txt: 1064664 rows, 7 cols\n",
      "  ✅ INDI19Q1.txt: 1106754 rows, 7 cols\n",
      "  ✅ INDI19Q1.txt: 1106754 rows, 7 cols\n",
      "  🔗 Combining 10 dataframes from chunk 5\n",
      "  ✅ Chunk 5: 11973916 rows appended (Total: 38014170)\n",
      "  💾 Memory after chunk 5: 67.1%\n",
      "\n",
      "📦 Processing chunk 6/10 (10 files)\n",
      "💾 Memory usage: 67.1%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24e0c93a6a74097b0e121b3d08cef8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 6:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ INDI19Q2.txt: 1221162 rows, 7 cols\n",
      "  ✅ INDI19Q2.txt: 1221162 rows, 7 cols\n",
      "  ✅ INDI19Q3.txt: 1353011 rows, 7 cols\n",
      "  ✅ INDI19Q3.txt: 1353011 rows, 7 cols\n",
      "  ✅ INDI19Q4.txt: 1146938 rows, 7 cols\n",
      "  ✅ INDI19Q4.txt: 1146938 rows, 7 cols\n",
      "  ✅ INDI20Q1.txt: 1348658 rows, 7 cols\n",
      "  ✅ INDI20Q1.txt: 1348658 rows, 7 cols\n",
      "  ✅ INDI20Q2.txt: 1294010 rows, 7 cols\n",
      "  ✅ INDI20Q2.txt: 1294010 rows, 7 cols\n",
      "  🔗 Combining 10 dataframes from chunk 6\n",
      "  ✅ Chunk 6: 12727558 rows appended (Total: 50741728)\n",
      "  💾 Memory after chunk 6: 47.4%\n",
      "\n",
      "📦 Processing chunk 7/10 (10 files)\n",
      "💾 Memory usage: 47.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562b9eccfe8a441ca704c6b515a9cc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 7:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ INDI20Q3.txt: 1276348 rows, 7 cols\n",
      "  ✅ INDI20Q3.txt: 1276348 rows, 7 cols\n",
      "  ✅ INDI20Q4.txt: 1297446 rows, 7 cols\n",
      "  ✅ INDI20Q4.txt: 1297446 rows, 7 cols\n",
      "  ✅ INDI21Q1.txt: 1603039 rows, 7 cols\n",
      "  ✅ INDI21Q1.txt: 1603039 rows, 7 cols\n",
      "  ✅ INDI21Q2.txt: 1659378 rows, 7 cols\n",
      "  ✅ INDI21Q2.txt: 1659378 rows, 7 cols\n",
      "  ✅ INDI21Q3.txt: 1588679 rows, 7 cols\n",
      "  ✅ INDI21Q3.txt: 1588679 rows, 7 cols\n",
      "  🔗 Combining 10 dataframes from chunk 7\n",
      "  ✅ Chunk 7: 14849780 rows appended (Total: 65591508)\n",
      "  💾 Memory after chunk 7: 57.0%\n",
      "\n",
      "📦 Processing chunk 8/10 (10 files)\n",
      "💾 Memory usage: 57.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382f09dd01f04565b12390fcd542f978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 8:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ INDI21Q4.txt: 1234766 rows, 7 cols\n",
      "  ✅ INDI21Q4.txt: 1234766 rows, 7 cols\n",
      "  ✅ INDI22Q1.txt: 1347146 rows, 7 cols\n",
      "  ✅ INDI22Q1.txt: 1347146 rows, 7 cols\n",
      "  ✅ INDI22Q2.txt: 1150299 rows, 7 cols\n",
      "  ✅ INDI22Q2.txt: 1150299 rows, 7 cols\n",
      "  ✅ INDI22Q3.txt: 1159203 rows, 7 cols\n",
      "  ✅ INDI22Q3.txt: 1159203 rows, 7 cols\n",
      "  ✅ INDI22Q4.txt: 1321489 rows, 7 cols\n",
      "  ✅ INDI22Q4.txt: 1321489 rows, 7 cols\n",
      "  🔗 Combining 10 dataframes from chunk 8\n",
      "  ✅ Chunk 8: 12425806 rows appended (Total: 78017314)\n",
      "  💾 Memory after chunk 8: 61.0%\n",
      "\n",
      "📦 Processing chunk 9/10 (10 files)\n",
      "💾 Memory usage: 61.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618e9646325247029bfb2f4b358ab7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 9:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ INDI23Q1.txt: 1176237 rows, 7 cols\n",
      "  ✅ INDI23Q1.txt: 1176237 rows, 7 cols\n",
      "  ❌ Failed to read: INDI23Q2.txt\n",
      "  ✅ INDI23Q2.txt: 1165782 rows, 7 cols\n",
      "  ✅ INDI23Q3.txt: 1063761 rows, 7 cols\n",
      "  ✅ INDI23Q3.txt: 1063761 rows, 7 cols\n",
      "  ✅ INDI23Q4.txt: 1115961 rows, 7 cols\n",
      "  ✅ INDI23Q4.txt: 1115961 rows, 7 cols\n",
      "  ✅ INDI24Q1.txt: 1186115 rows, 7 cols\n",
      "  ✅ INDI24Q1.txt: 1186115 rows, 7 cols\n",
      "  🔗 Combining 9 dataframes from chunk 9\n",
      "  ✅ Chunk 9: 10249930 rows appended (Total: 88267244)\n",
      "  💾 Memory after chunk 9: 63.3%\n",
      "\n",
      "📦 Processing chunk 10/10 (6 files)\n",
      "💾 Memory usage: 63.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e442b3caeeb421d914cf3e6e87046f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 10:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ INDI24Q2.txt: 1187626 rows, 7 cols\n",
      "  ✅ INDI24Q2.txt: 1187626 rows, 7 cols\n",
      "  ✅ INDI24Q3.txt: 1177133 rows, 7 cols\n",
      "  ✅ INDI24Q3.txt: 1177133 rows, 7 cols\n",
      "  ✅ INDI24Q4.txt: 1219759 rows, 7 cols\n",
      "  ✅ INDI24Q4.txt: 1219759 rows, 7 cols\n",
      "  🔗 Combining 6 dataframes from chunk 10\n",
      "  ✅ Chunk 10: 7169036 rows appended (Total: 95436280)\n",
      "  💾 Memory after chunk 10: 64.4%\n",
      "\n",
      "🎉 Successfully created faers_indications_combined.csv with 95,436,280 total rows\n",
      "\n",
      "================================================================================\n",
      "🚀 PROCESSING: OUTC_DF\n",
      "================================================================================\n",
      "Found 0 unique files for pattern outc*.txt\n",
      "Found 96 unique files for pattern OUTC*.txt\n",
      "📊 Found 96 files to process\n",
      "\n",
      "🔄 Processing 96 files in chunks of 10\n",
      "📁 Output: faers_outcomes_combined.csv\n",
      "\n",
      "📦 Processing chunk 1/10 (10 files)\n",
      "💾 Memory usage: 64.1%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e020737d15d44096b58cbf08853f4d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ OUTC13Q1.txt: 153733 rows, 6 cols\n",
      "  ✅ OUTC13Q1.txt: 153733 rows, 6 cols\n",
      "  ✅ OUTC13Q2.txt: 138634 rows, 6 cols\n",
      "  ✅ OUTC13Q2.txt: 138634 rows, 6 cols\n",
      "  ✅ OUTC13Q3.txt: 147472 rows, 6 cols\n",
      "  ✅ OUTC13Q3.txt: 147472 rows, 6 cols\n",
      "  ✅ OUTC13Q4.txt: 154451 rows, 6 cols\n",
      "  ✅ OUTC13Q4.txt: 154451 rows, 6 cols\n",
      "  ✅ OUTC14Q1.txt: 170437 rows, 6 cols\n",
      "  ✅ OUTC14Q1.txt: 170437 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 1\n",
      "  ✅ Chunk 1: 1529454 rows appended (Total: 1529454)\n",
      "  💾 Memory after chunk 1: 64.3%\n",
      "\n",
      "📦 Processing chunk 2/10 (10 files)\n",
      "💾 Memory usage: 64.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e040f5423af345239aa7c471c6abe1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ OUTC14Q2.txt: 178144 rows, 6 cols\n",
      "  ✅ OUTC14Q2.txt: 178144 rows, 6 cols\n",
      "  ✅ OUTC14Q3.txt: 169754 rows, 6 cols\n",
      "  ✅ OUTC14Q3.txt: 169754 rows, 6 cols\n",
      "  ✅ OUTC14Q4.txt: 167401 rows, 6 cols\n",
      "  ✅ OUTC14Q4.txt: 167401 rows, 6 cols\n",
      "  ✅ OUTC15Q1.txt: 238008 rows, 6 cols\n",
      "  ✅ OUTC15Q1.txt: 238008 rows, 6 cols\n",
      "  ✅ OUTC15Q2.txt: 200423 rows, 6 cols\n",
      "  ✅ OUTC15Q2.txt: 200423 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 2\n",
      "  ✅ Chunk 2: 1907460 rows appended (Total: 3436914)\n",
      "  💾 Memory after chunk 2: 65.0%\n",
      "\n",
      "📦 Processing chunk 3/10 (10 files)\n",
      "💾 Memory usage: 65.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75357e8c41a43c0b99f0622ca282e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ OUTC15Q3.txt: 213108 rows, 6 cols\n",
      "  ✅ OUTC15Q3.txt: 213108 rows, 6 cols\n",
      "  ✅ OUTC15Q4.txt: 205080 rows, 6 cols\n",
      "  ✅ OUTC15Q4.txt: 205080 rows, 6 cols\n",
      "  ✅ OUTC16Q1.txt: 210280 rows, 6 cols\n",
      "  ✅ OUTC16Q1.txt: 210280 rows, 6 cols\n",
      "  ✅ OUTC16Q2.txt: 218745 rows, 6 cols\n",
      "  ✅ OUTC16Q2.txt: 218745 rows, 6 cols\n",
      "  ✅ OUTC16Q3.txt: 221346 rows, 6 cols\n",
      "  ✅ OUTC16Q3.txt: 221346 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 3\n",
      "  ✅ Chunk 3: 2137118 rows appended (Total: 5574032)\n",
      "  💾 Memory after chunk 3: 65.3%\n",
      "\n",
      "📦 Processing chunk 4/10 (10 files)\n",
      "💾 Memory usage: 65.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05540646c4214b56b31701feed9a0cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ OUTC16Q4.txt: 220468 rows, 6 cols\n",
      "  ✅ OUTC16Q4.txt: 220468 rows, 6 cols\n",
      "  ✅ OUTC17Q1.txt: 242274 rows, 6 cols\n",
      "  ✅ OUTC17Q1.txt: 242274 rows, 6 cols\n",
      "  ✅ OUTC17Q2.txt: 232053 rows, 6 cols\n",
      "  ✅ OUTC17Q2.txt: 232053 rows, 6 cols\n",
      "  ✅ OUTC17Q3.txt: 233715 rows, 6 cols\n",
      "  ✅ OUTC17Q3.txt: 233715 rows, 6 cols\n",
      "  ✅ OUTC17Q4.txt: 243176 rows, 6 cols\n",
      "  ✅ OUTC17Q4.txt: 243176 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 4\n",
      "  ✅ Chunk 4: 2343372 rows appended (Total: 7917404)\n",
      "  💾 Memory after chunk 4: 65.5%\n",
      "\n",
      "📦 Processing chunk 5/10 (10 files)\n",
      "💾 Memory usage: 65.4%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01c79a5e6de4fc8b53a5f26ff6a7776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 5:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ OUTC18Q1.txt: 298710 rows, 6 cols\n",
      "  ✅ OUTC18Q1.txt: 298710 rows, 6 cols\n",
      "  ✅ OUTC18Q2.txt: 331947 rows, 6 cols\n",
      "  ✅ OUTC18Q2.txt: 331947 rows, 6 cols\n",
      "  ✅ OUTC18Q3.txt: 295817 rows, 6 cols\n",
      "  ✅ OUTC18Q3.txt: 295817 rows, 6 cols\n",
      "  ✅ OUTC18Q4.txt: 299135 rows, 6 cols\n",
      "  ✅ OUTC18Q4.txt: 299135 rows, 6 cols\n",
      "  ✅ OUTC19Q1.txt: 310662 rows, 6 cols\n",
      "  ✅ OUTC19Q1.txt: 310662 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 5\n",
      "  ✅ Chunk 5: 3072542 rows appended (Total: 10989946)\n",
      "  💾 Memory after chunk 5: 58.9%\n",
      "\n",
      "📦 Processing chunk 6/10 (10 files)\n",
      "💾 Memory usage: 58.9%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7ef5cc9caa4d6f891d326fef58c4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 6:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ OUTC19Q2.txt: 324738 rows, 6 cols\n",
      "  ✅ OUTC19Q2.txt: 324738 rows, 6 cols\n",
      "  ✅ OUTC19Q3.txt: 323787 rows, 6 cols\n",
      "  ✅ OUTC19Q3.txt: 323787 rows, 6 cols\n",
      "  ✅ OUTC19Q4.txt: 321603 rows, 6 cols\n",
      "  ✅ OUTC19Q4.txt: 321603 rows, 6 cols\n",
      "  ✅ OUTC20Q1.txt: 335470 rows, 6 cols\n",
      "  ✅ OUTC20Q1.txt: 335470 rows, 6 cols\n",
      "  ✅ OUTC20Q2.txt: 307509 rows, 6 cols\n",
      "  ✅ OUTC20Q2.txt: 307509 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 6\n",
      "  ✅ Chunk 6: 3226214 rows appended (Total: 14216160)\n",
      "  💾 Memory after chunk 6: 61.4%\n",
      "\n",
      "📦 Processing chunk 7/10 (10 files)\n",
      "💾 Memory usage: 61.4%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a53acc0eaf43c79d1e3a048f17520e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 7:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ OUTC20Q3.txt: 358815 rows, 6 cols\n",
      "  ✅ OUTC20Q3.txt: 358815 rows, 6 cols\n",
      "  ✅ OUTC20Q4.txt: 365575 rows, 6 cols\n",
      "  ✅ OUTC20Q4.txt: 365575 rows, 6 cols\n",
      "  ✅ OUTC21Q1.txt: 371698 rows, 6 cols\n",
      "  ✅ OUTC21Q1.txt: 371698 rows, 6 cols\n",
      "  ✅ OUTC21Q2.txt: 383928 rows, 6 cols\n",
      "  ✅ OUTC21Q2.txt: 383928 rows, 6 cols\n",
      "  ✅ OUTC21Q3.txt: 420729 rows, 6 cols\n",
      "  ✅ OUTC21Q3.txt: 420729 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 7\n",
      "  ✅ Chunk 7: 3801490 rows appended (Total: 18017650)\n",
      "  💾 Memory after chunk 7: 62.0%\n",
      "\n",
      "📦 Processing chunk 8/10 (10 files)\n",
      "💾 Memory usage: 62.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037796aecdda441a9d1121ef20dd0e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 8:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ OUTC21Q4.txt: 337168 rows, 6 cols\n",
      "  ✅ OUTC21Q4.txt: 337168 rows, 6 cols\n",
      "  ✅ OUTC22Q1.txt: 375497 rows, 6 cols\n",
      "  ✅ OUTC22Q1.txt: 375497 rows, 6 cols\n",
      "  ✅ OUTC22Q2.txt: 325309 rows, 6 cols\n",
      "  ✅ OUTC22Q2.txt: 325309 rows, 6 cols\n",
      "  ✅ OUTC22Q3.txt: 345763 rows, 6 cols\n",
      "  ✅ OUTC22Q3.txt: 345763 rows, 6 cols\n",
      "  ✅ OUTC22Q4.txt: 334611 rows, 6 cols\n",
      "  ✅ OUTC22Q4.txt: 334611 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 8\n",
      "  ✅ Chunk 8: 3436696 rows appended (Total: 21454346)\n",
      "  💾 Memory after chunk 8: 62.3%\n",
      "\n",
      "📦 Processing chunk 9/10 (10 files)\n",
      "💾 Memory usage: 62.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58000c1ec234a4493ba2d7ea0f38fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 9:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ OUTC23Q1.txt: 309217 rows, 6 cols\n",
      "  ✅ OUTC23Q1.txt: 309217 rows, 6 cols\n",
      "  ✅ OUTC23Q2.txt: 303513 rows, 6 cols\n",
      "  ✅ OUTC23Q2.txt: 303513 rows, 6 cols\n",
      "  ✅ OUTC23Q3.txt: 307396 rows, 6 cols\n",
      "  ✅ OUTC23Q3.txt: 307396 rows, 6 cols\n",
      "  ✅ OUTC23Q4.txt: 327797 rows, 6 cols\n",
      "  ✅ OUTC23Q4.txt: 327797 rows, 6 cols\n",
      "  ✅ OUTC24Q1.txt: 295044 rows, 6 cols\n",
      "  ✅ OUTC24Q1.txt: 295044 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 9\n",
      "  ✅ Chunk 9: 3085934 rows appended (Total: 24540280)\n",
      "  💾 Memory after chunk 9: 62.8%\n",
      "\n",
      "📦 Processing chunk 10/10 (6 files)\n",
      "💾 Memory usage: 62.8%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6804f16b69bd43e8b9ceb949f8000204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 10:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ OUTC24Q2.txt: 291572 rows, 6 cols\n",
      "  ✅ OUTC24Q2.txt: 291572 rows, 6 cols\n",
      "  ✅ OUTC24Q3.txt: 288275 rows, 6 cols\n",
      "  ✅ OUTC24Q3.txt: 288275 rows, 6 cols\n",
      "  ✅ OUTC24Q4.txt: 308960 rows, 6 cols\n",
      "  ✅ OUTC24Q4.txt: 308960 rows, 6 cols\n",
      "  🔗 Combining 6 dataframes from chunk 10\n",
      "  ✅ Chunk 10: 1777614 rows appended (Total: 26317894)\n",
      "  💾 Memory after chunk 10: 62.9%\n",
      "\n",
      "🎉 Successfully created faers_outcomes_combined.csv with 26,317,894 total rows\n",
      "\n",
      "================================================================================\n",
      "🚀 PROCESSING: RPSR_DF\n",
      "================================================================================\n",
      "Found 0 unique files for pattern rpsr*.txt\n",
      "Found 96 unique files for pattern RPSR*.txt\n",
      "📊 Found 96 files to process\n",
      "\n",
      "🔄 Processing 96 files in chunks of 10\n",
      "📁 Output: faers_reports_combined.csv\n",
      "\n",
      "📦 Processing chunk 1/10 (10 files)\n",
      "💾 Memory usage: 62.8%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e974b20cdd894516ac137baaf6efa293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ RPSR13Q1.txt: 54000 rows, 6 cols\n",
      "  ✅ RPSR13Q1.txt: 54000 rows, 6 cols\n",
      "  ✅ RPSR13Q2.txt: 27142 rows, 6 cols\n",
      "  ✅ RPSR13Q2.txt: 27142 rows, 6 cols\n",
      "  ✅ RPSR13Q3.txt: 24861 rows, 6 cols\n",
      "  ✅ RPSR13Q3.txt: 24861 rows, 6 cols\n",
      "  ✅ RPSR13Q4.txt: 26575 rows, 6 cols\n",
      "  ✅ RPSR13Q4.txt: 26575 rows, 6 cols\n",
      "  ✅ RPSR14Q1.txt: 37839 rows, 6 cols\n",
      "  ✅ RPSR14Q1.txt: 37839 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 1\n",
      "  ✅ Chunk 1: 340834 rows appended (Total: 340834)\n",
      "  💾 Memory after chunk 1: 62.9%\n",
      "\n",
      "📦 Processing chunk 2/10 (10 files)\n",
      "💾 Memory usage: 62.9%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84f60dc58084c6f93b28c4cb4563126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ RPSR14Q2.txt: 22556 rows, 6 cols\n",
      "  ✅ RPSR14Q2.txt: 22556 rows, 6 cols\n",
      "  ✅ RPSR14Q3.txt: 24337 rows, 6 cols\n",
      "  ✅ RPSR14Q3.txt: 24337 rows, 6 cols\n",
      "  ✅ RPSR14Q4.txt: 23333 rows, 6 cols\n",
      "  ✅ RPSR14Q4.txt: 23333 rows, 6 cols\n",
      "  ✅ RPSR15Q1.txt: 28234 rows, 6 cols\n",
      "  ✅ RPSR15Q1.txt: 28234 rows, 6 cols\n",
      "  ✅ RPSR15Q2.txt: 20463 rows, 6 cols\n",
      "  ✅ RPSR15Q2.txt: 20463 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 2\n",
      "  ✅ Chunk 2: 237846 rows appended (Total: 578680)\n",
      "  💾 Memory after chunk 2: 62.9%\n",
      "\n",
      "📦 Processing chunk 3/10 (10 files)\n",
      "💾 Memory usage: 62.9%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c35485e57164e59ae49b269b6c67b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ RPSR15Q3.txt: 12075 rows, 6 cols\n",
      "  ✅ RPSR15Q3.txt: 12075 rows, 6 cols\n",
      "  ✅ RPSR15Q4.txt: 5688 rows, 6 cols\n",
      "  ✅ RPSR15Q4.txt: 5688 rows, 6 cols\n",
      "  ✅ RPSR16Q1.txt: 6480 rows, 6 cols\n",
      "  ✅ RPSR16Q1.txt: 6480 rows, 6 cols\n",
      "  ✅ RPSR16Q2.txt: 9437 rows, 6 cols\n",
      "  ✅ RPSR16Q2.txt: 9437 rows, 6 cols\n",
      "  ✅ RPSR16Q3.txt: 7356 rows, 6 cols\n",
      "  ✅ RPSR16Q3.txt: 7356 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 3\n",
      "  ✅ Chunk 3: 82072 rows appended (Total: 660752)\n",
      "  💾 Memory after chunk 3: 63.2%\n",
      "\n",
      "📦 Processing chunk 4/10 (10 files)\n",
      "💾 Memory usage: 63.2%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d4f2f8dcde48ce875f94f64de6b0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ RPSR16Q4.txt: 10886 rows, 6 cols\n",
      "  ✅ RPSR16Q4.txt: 10886 rows, 6 cols\n",
      "  ✅ RPSR17Q1.txt: 13426 rows, 6 cols\n",
      "  ✅ RPSR17Q1.txt: 13426 rows, 6 cols\n",
      "  ✅ RPSR17Q2.txt: 15768 rows, 6 cols\n",
      "  ✅ RPSR17Q2.txt: 15768 rows, 6 cols\n",
      "  ✅ RPSR17Q3.txt: 15478 rows, 6 cols\n",
      "  ✅ RPSR17Q3.txt: 15478 rows, 6 cols\n",
      "  ✅ RPSR17Q4.txt: 15363 rows, 6 cols\n",
      "  ✅ RPSR17Q4.txt: 15363 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 4\n",
      "  ✅ Chunk 4: 141842 rows appended (Total: 802594)\n",
      "  💾 Memory after chunk 4: 63.9%\n",
      "\n",
      "📦 Processing chunk 5/10 (10 files)\n",
      "💾 Memory usage: 63.9%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd851efedaf400380e682d08cd3414e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 5:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ RPSR18Q1.txt: 19433 rows, 6 cols\n",
      "  ✅ RPSR18Q1.txt: 19433 rows, 6 cols\n",
      "  ✅ RPSR18Q2.txt: 15860 rows, 6 cols\n",
      "  ✅ RPSR18Q2.txt: 15860 rows, 6 cols\n",
      "  ✅ RPSR18Q3.txt: 23783 rows, 6 cols\n",
      "  ✅ RPSR18Q3.txt: 23783 rows, 6 cols\n",
      "  ✅ RPSR18Q4.txt: 21075 rows, 6 cols\n",
      "  ✅ RPSR18Q4.txt: 21075 rows, 6 cols\n",
      "  ✅ RPSR19Q1.txt: 16018 rows, 6 cols\n",
      "  ✅ RPSR19Q1.txt: 16018 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 5\n",
      "  ✅ Chunk 5: 192338 rows appended (Total: 994932)\n",
      "  💾 Memory after chunk 5: 64.1%\n",
      "\n",
      "📦 Processing chunk 6/10 (10 files)\n",
      "💾 Memory usage: 64.1%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd98ea5e8179412a87a7fe135b9965b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 6:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ RPSR19Q2.txt: 23643 rows, 6 cols\n",
      "  ✅ RPSR19Q2.txt: 23643 rows, 6 cols\n",
      "  ✅ RPSR19Q3.txt: 20652 rows, 6 cols\n",
      "  ✅ RPSR19Q3.txt: 20652 rows, 6 cols\n",
      "  ✅ RPSR19Q4.txt: 23774 rows, 6 cols\n",
      "  ✅ RPSR19Q4.txt: 23774 rows, 6 cols\n",
      "  ✅ RPSR20Q1.txt: 15492 rows, 6 cols\n",
      "  ✅ RPSR20Q1.txt: 15492 rows, 6 cols\n",
      "  ✅ RPSR20Q2.txt: 13094 rows, 6 cols\n",
      "  ✅ RPSR20Q2.txt: 13094 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 6\n",
      "  ✅ Chunk 6: 193310 rows appended (Total: 1188242)\n",
      "  💾 Memory after chunk 6: 63.8%\n",
      "\n",
      "📦 Processing chunk 7/10 (10 files)\n",
      "💾 Memory usage: 63.8%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24689227bf2643edb4f3bcd9b8474d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 7:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ RPSR20Q3.txt: 17281 rows, 6 cols\n",
      "  ✅ RPSR20Q3.txt: 17281 rows, 6 cols\n",
      "  ✅ RPSR20Q4.txt: 14477 rows, 6 cols\n",
      "  ✅ RPSR20Q4.txt: 14477 rows, 6 cols\n",
      "  ✅ RPSR21Q1.txt: 14046 rows, 6 cols\n",
      "  ✅ RPSR21Q1.txt: 14046 rows, 6 cols\n",
      "  ✅ RPSR21Q2.txt: 13123 rows, 6 cols\n",
      "  ✅ RPSR21Q2.txt: 13123 rows, 6 cols\n",
      "  ✅ RPSR21Q3.txt: 16855 rows, 6 cols\n",
      "  ✅ RPSR21Q3.txt: 16855 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 7\n",
      "  ✅ Chunk 7: 151564 rows appended (Total: 1339806)\n",
      "  💾 Memory after chunk 7: 64.2%\n",
      "\n",
      "📦 Processing chunk 8/10 (10 files)\n",
      "💾 Memory usage: 64.2%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a004a48008334723bf3763995a937ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 8:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ RPSR21Q4.txt: 4936 rows, 6 cols\n",
      "  ✅ RPSR21Q4.txt: 4936 rows, 6 cols\n",
      "  ✅ RPSR22Q1.txt: 13091 rows, 6 cols\n",
      "  ✅ RPSR22Q1.txt: 13091 rows, 6 cols\n",
      "  ✅ RPSR22Q2.txt: 13867 rows, 6 cols\n",
      "  ✅ RPSR22Q2.txt: 13867 rows, 6 cols\n",
      "  ✅ RPSR22Q3.txt: 14727 rows, 6 cols\n",
      "  ✅ RPSR22Q3.txt: 14727 rows, 6 cols\n",
      "  ✅ RPSR22Q4.txt: 14398 rows, 6 cols\n",
      "  ✅ RPSR22Q4.txt: 14398 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 8\n",
      "  ✅ Chunk 8: 122038 rows appended (Total: 1461844)\n",
      "  💾 Memory after chunk 8: 64.2%\n",
      "\n",
      "📦 Processing chunk 9/10 (10 files)\n",
      "💾 Memory usage: 64.2%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4298be57334520ad7559ea9929cbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 9:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ RPSR23Q1.txt: 13851 rows, 6 cols\n",
      "  ✅ RPSR23Q1.txt: 13851 rows, 6 cols\n",
      "  ✅ RPSR23Q2.txt: 13884 rows, 6 cols\n",
      "  ✅ RPSR23Q2.txt: 13884 rows, 6 cols\n",
      "  ✅ RPSR23Q3.txt: 11524 rows, 6 cols\n",
      "  ✅ RPSR23Q3.txt: 11524 rows, 6 cols\n",
      "  ✅ RPSR23Q4.txt: 13238 rows, 6 cols\n",
      "  ✅ RPSR23Q4.txt: 13238 rows, 6 cols\n",
      "  ✅ RPSR24Q1.txt: 12381 rows, 6 cols\n",
      "  ✅ RPSR24Q1.txt: 12381 rows, 6 cols\n",
      "  🔗 Combining 10 dataframes from chunk 9\n",
      "  ✅ Chunk 9: 129756 rows appended (Total: 1591600)\n",
      "  💾 Memory after chunk 9: 64.3%\n",
      "\n",
      "📦 Processing chunk 10/10 (6 files)\n",
      "💾 Memory usage: 64.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f627ac1ab534f41b403696a694680fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 10:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ RPSR24Q2.txt: 11517 rows, 6 cols\n",
      "  ✅ RPSR24Q2.txt: 11517 rows, 6 cols\n",
      "  ✅ RPSR24Q3.txt: 10087 rows, 6 cols\n",
      "  ✅ RPSR24Q3.txt: 10087 rows, 6 cols\n",
      "  ✅ RPSR24Q4.txt: 11627 rows, 6 cols\n",
      "  ✅ RPSR24Q4.txt: 11627 rows, 6 cols\n",
      "  🔗 Combining 6 dataframes from chunk 10\n",
      "  ✅ Chunk 10: 66462 rows appended (Total: 1658062)\n",
      "  💾 Memory after chunk 10: 64.3%\n",
      "\n",
      "🎉 Successfully created faers_reports_combined.csv with 1,658,062 total rows\n",
      "\n",
      "================================================================================\n",
      "🚀 PROCESSING: THER_DF\n",
      "================================================================================\n",
      "Found 0 unique files for pattern ther*.txt\n",
      "Found 96 unique files for pattern THER*.txt\n",
      "📊 Found 96 files to process\n",
      "\n",
      "🔄 Processing 96 files in chunks of 10\n",
      "📁 Output: faers_therapy_combined.csv\n",
      "\n",
      "📦 Processing chunk 1/10 (10 files)\n",
      "💾 Memory usage: 64.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c576d277dd44129a29f35bfcd74f938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ THER13Q1.txt: 281940 rows, 10 cols\n",
      "  ✅ THER13Q1.txt: 281940 rows, 10 cols\n",
      "  ✅ THER13Q2.txt: 237513 rows, 10 cols\n",
      "  ✅ THER13Q2.txt: 237513 rows, 10 cols\n",
      "  ✅ THER13Q3.txt: 251263 rows, 10 cols\n",
      "  ✅ THER13Q3.txt: 251263 rows, 10 cols\n",
      "  ✅ THER13Q4.txt: 291315 rows, 10 cols\n",
      "  ✅ THER13Q4.txt: 291315 rows, 10 cols\n",
      "  ✅ THER14Q1.txt: 329817 rows, 10 cols\n",
      "  ✅ THER14Q1.txt: 329817 rows, 10 cols\n",
      "  🔗 Combining 10 dataframes from chunk 1\n",
      "  ✅ Chunk 1: 2783696 rows appended (Total: 2783696)\n",
      "  💾 Memory after chunk 1: 63.4%\n",
      "\n",
      "📦 Processing chunk 2/10 (10 files)\n",
      "💾 Memory usage: 63.4%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4886da5bfc04452a3c45c9b88c08806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ THER14Q2.txt: 266586 rows, 10 cols\n",
      "  ✅ THER14Q2.txt: 266586 rows, 10 cols\n",
      "  ✅ THER14Q3.txt: 270268 rows, 10 cols\n",
      "  ✅ THER14Q3.txt: 270268 rows, 10 cols\n",
      "  ✅ THER14Q4.txt: 270468 rows, 10 cols\n",
      "  ✅ THER14Q4.txt: 270468 rows, 10 cols\n",
      "  ✅ THER15Q1.txt: 374935 rows, 10 cols\n",
      "  ✅ THER15Q1.txt: 374935 rows, 10 cols\n",
      "  ✅ THER15Q2.txt: 373850 rows, 10 cols\n",
      "  ✅ THER15Q2.txt: 373850 rows, 10 cols\n",
      "  🔗 Combining 10 dataframes from chunk 2\n",
      "  ✅ Chunk 2: 3112214 rows appended (Total: 5895910)\n",
      "  💾 Memory after chunk 2: 63.6%\n",
      "\n",
      "📦 Processing chunk 3/10 (10 files)\n",
      "💾 Memory usage: 63.6%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f18e86d8f5b4e98821446c7286c9173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ THER15Q3.txt: 494584 rows, 10 cols\n",
      "  ✅ THER15Q3.txt: 494584 rows, 10 cols\n",
      "  ✅ THER15Q4.txt: 401725 rows, 10 cols\n",
      "  ✅ THER15Q4.txt: 401725 rows, 10 cols\n",
      "  ✅ THER16Q1.txt: 458946 rows, 10 cols\n",
      "  ✅ THER16Q1.txt: 458946 rows, 10 cols\n",
      "  ✅ THER16Q2.txt: 464701 rows, 10 cols\n",
      "  ✅ THER16Q2.txt: 464701 rows, 10 cols\n",
      "  ✅ THER16Q3.txt: 483082 rows, 10 cols\n",
      "  ✅ THER16Q3.txt: 483082 rows, 10 cols\n",
      "  🔗 Combining 10 dataframes from chunk 3\n",
      "  ✅ Chunk 3: 4606076 rows appended (Total: 10501986)\n",
      "  💾 Memory after chunk 3: 63.4%\n",
      "\n",
      "📦 Processing chunk 4/10 (10 files)\n",
      "💾 Memory usage: 63.4%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf4330b404a4ded89c9154c3d83de23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ THER16Q4.txt: 433545 rows, 10 cols\n",
      "  ✅ THER16Q4.txt: 433545 rows, 10 cols\n",
      "  ✅ THER17Q1.txt: 455411 rows, 10 cols\n",
      "  ✅ THER17Q1.txt: 455411 rows, 10 cols\n",
      "  ✅ THER17Q2.txt: 454078 rows, 10 cols\n",
      "  ✅ THER17Q2.txt: 454078 rows, 10 cols\n",
      "  ✅ THER17Q3.txt: 443213 rows, 10 cols\n",
      "  ✅ THER17Q3.txt: 443213 rows, 10 cols\n",
      "  ✅ THER17Q4.txt: 442433 rows, 10 cols\n",
      "  ✅ THER17Q4.txt: 442433 rows, 10 cols\n",
      "  🔗 Combining 10 dataframes from chunk 4\n",
      "  ✅ Chunk 4: 4457360 rows appended (Total: 14959346)\n",
      "  💾 Memory after chunk 4: 63.8%\n",
      "\n",
      "📦 Processing chunk 5/10 (10 files)\n",
      "💾 Memory usage: 63.8%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c773ed299df54c82929aae7036cca78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 5:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ THER18Q1.txt: 551394 rows, 10 cols\n",
      "  ✅ THER18Q1.txt: 551394 rows, 10 cols\n",
      "  ✅ THER18Q2.txt: 688053 rows, 10 cols\n",
      "  ✅ THER18Q2.txt: 688053 rows, 10 cols\n",
      "  ✅ THER18Q3.txt: 663319 rows, 10 cols\n",
      "  ✅ THER18Q3.txt: 663319 rows, 10 cols\n",
      "  ✅ THER18Q4.txt: 620308 rows, 10 cols\n",
      "  ✅ THER18Q4.txt: 620308 rows, 10 cols\n",
      "  ✅ THER19Q1.txt: 627355 rows, 10 cols\n",
      "  ✅ THER19Q1.txt: 627355 rows, 10 cols\n",
      "  🔗 Combining 10 dataframes from chunk 5\n",
      "  ✅ Chunk 5: 6300858 rows appended (Total: 21260204)\n",
      "  💾 Memory after chunk 5: 63.3%\n",
      "\n",
      "📦 Processing chunk 6/10 (10 files)\n",
      "💾 Memory usage: 63.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441416d350434b0e953ac4a770463511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 6:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ THER19Q2.txt: 747535 rows, 10 cols\n",
      "  ✅ THER19Q2.txt: 747535 rows, 10 cols\n",
      "  ✅ THER19Q3.txt: 776039 rows, 10 cols\n",
      "  ✅ THER19Q3.txt: 776039 rows, 10 cols\n",
      "  ✅ THER19Q4.txt: 643525 rows, 10 cols\n",
      "  ✅ THER19Q4.txt: 643525 rows, 10 cols\n",
      "  ✅ THER20Q1.txt: 728199 rows, 10 cols\n",
      "  ✅ THER20Q1.txt: 728199 rows, 10 cols\n",
      "  ✅ THER20Q2.txt: 636959 rows, 10 cols\n",
      "  ✅ THER20Q2.txt: 636959 rows, 10 cols\n",
      "  🔗 Combining 10 dataframes from chunk 6\n",
      "  ✅ Chunk 6: 7064514 rows appended (Total: 28324718)\n",
      "  💾 Memory after chunk 6: 63.3%\n",
      "\n",
      "📦 Processing chunk 7/10 (10 files)\n",
      "💾 Memory usage: 63.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf6b570da8b45188f0b698dc00fcfd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 7:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ THER20Q3.txt: 646462 rows, 10 cols\n",
      "  ✅ THER20Q3.txt: 646462 rows, 10 cols\n",
      "  ✅ THER20Q4.txt: 681411 rows, 10 cols\n",
      "  ✅ THER20Q4.txt: 681411 rows, 10 cols\n",
      "  ✅ THER21Q1.txt: 786472 rows, 10 cols\n",
      "  ✅ THER21Q1.txt: 786472 rows, 10 cols\n",
      "  ✅ THER21Q2.txt: 789968 rows, 10 cols\n",
      "  ✅ THER21Q2.txt: 789968 rows, 10 cols\n",
      "  ✅ THER21Q3.txt: 848661 rows, 10 cols\n",
      "  ✅ THER21Q3.txt: 848661 rows, 10 cols\n",
      "  🔗 Combining 10 dataframes from chunk 7\n",
      "  ✅ Chunk 7: 7505948 rows appended (Total: 35830666)\n",
      "  💾 Memory after chunk 7: 63.3%\n",
      "\n",
      "📦 Processing chunk 8/10 (10 files)\n",
      "💾 Memory usage: 63.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289d7431f35f4b62ab52b458470bf29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 8:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ THER21Q4.txt: 671045 rows, 10 cols\n",
      "  ✅ THER21Q4.txt: 671045 rows, 10 cols\n",
      "  ✅ THER22Q1.txt: 748899 rows, 10 cols\n",
      "  ✅ THER22Q1.txt: 748899 rows, 10 cols\n",
      "  ✅ THER22Q2.txt: 690828 rows, 10 cols\n",
      "  ✅ THER22Q2.txt: 690828 rows, 10 cols\n",
      "  ✅ THER22Q3.txt: 717902 rows, 10 cols\n",
      "  ✅ THER22Q3.txt: 717902 rows, 10 cols\n",
      "  ✅ THER22Q4.txt: 726767 rows, 10 cols\n",
      "  ✅ THER22Q4.txt: 726767 rows, 10 cols\n",
      "  🔗 Combining 10 dataframes from chunk 8\n",
      "  ✅ Chunk 8: 7110882 rows appended (Total: 42941548)\n",
      "  💾 Memory after chunk 8: 53.8%\n",
      "\n",
      "📦 Processing chunk 9/10 (10 files)\n",
      "💾 Memory usage: 53.8%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f591c4bb5a2f42f39b66bd4564a27dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 9:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ THER23Q1.txt: 683408 rows, 10 cols\n",
      "  ✅ THER23Q1.txt: 683408 rows, 10 cols\n",
      "  ✅ THER23Q2.txt: 678128 rows, 10 cols\n",
      "  ✅ THER23Q2.txt: 678128 rows, 10 cols\n",
      "  ✅ THER23Q3.txt: 593027 rows, 10 cols\n",
      "  ✅ THER23Q3.txt: 593027 rows, 10 cols\n",
      "  ✅ THER23Q4.txt: 633087 rows, 10 cols\n",
      "  ✅ THER23Q4.txt: 633087 rows, 10 cols\n",
      "  ✅ THER24Q1.txt: 594449 rows, 10 cols\n",
      "  ✅ THER24Q1.txt: 594449 rows, 10 cols\n",
      "  🔗 Combining 10 dataframes from chunk 9\n",
      "  ✅ Chunk 9: 6364198 rows appended (Total: 49305746)\n",
      "  💾 Memory after chunk 9: 56.3%\n",
      "\n",
      "📦 Processing chunk 10/10 (6 files)\n",
      "💾 Memory usage: 56.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d355b6e19974ea0b2bf39453c2bde15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 10:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ THER24Q2.txt: 539334 rows, 10 cols\n",
      "  ✅ THER24Q2.txt: 539334 rows, 10 cols\n",
      "  ✅ THER24Q3.txt: 532854 rows, 10 cols\n",
      "  ✅ THER24Q3.txt: 532854 rows, 10 cols\n",
      "  ✅ THER24Q4.txt: 561889 rows, 10 cols\n",
      "  ✅ THER24Q4.txt: 561889 rows, 10 cols\n",
      "  🔗 Combining 6 dataframes from chunk 10\n",
      "  ✅ Chunk 10: 3268154 rows appended (Total: 52573900)\n",
      "  💾 Memory after chunk 10: 57.8%\n",
      "\n",
      "🎉 Successfully created faers_therapy_combined.csv with 52,573,900 total rows\n",
      "\n",
      "🎉 ALL PROCESSING COMPLETE!\n",
      "\n",
      "📊 FINAL SUMMARY:\n",
      "================================================================================\n",
      "✅ demo_df: faers_demographics_combined.csv (5507.9 MB)\n",
      "✅ drug_df: faers_drugs_combined.csv (16206.8 MB)\n",
      "✅ reac_df: faers_reactions_combined.csv (6326.7 MB)\n",
      "✅ indi_df: faers_indications_combined.csv (6124.1 MB)\n",
      "✅ outc_df: faers_outcomes_combined.csv (1076.3 MB)\n",
      "✅ rpsr_df: faers_reports_combined.csv (67.9 MB)\n",
      "✅ ther_df: faers_therapy_combined.csv (2837.6 MB)\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# MEMORY-EFFICIENT FAERS DATA PROCESSOR (CRASH-RESISTANT VERSION)\n",
    "# ===================================================================\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import psutil\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "base_directory = \"/Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/raw\"\n",
    "CHUNK_SIZE = 10  # Process files in chunks of 10\n",
    "MEMORY_THRESHOLD = 85  # Stop if memory usage exceeds 85%\n",
    "\n",
    "def check_memory():\n",
    "    \"\"\"Check current memory usage\"\"\"\n",
    "    memory_percent = psutil.virtual_memory().percent\n",
    "    print(f\"💾 Memory usage: {memory_percent:.1f}%\")\n",
    "    return memory_percent\n",
    "\n",
    "def find_files(pattern):\n",
    "    \"\"\"Find files with case-insensitive search\"\"\"\n",
    "    search_patterns = [\n",
    "        f\"{base_directory}/*/ascii/{pattern}\",\n",
    "        f\"{base_directory}/*/ASCII/{pattern}\",\n",
    "    ]\n",
    "    \n",
    "    all_files = []\n",
    "    for search_path in search_patterns:\n",
    "        files = glob.glob(search_path)\n",
    "        all_files.extend(files)\n",
    "    \n",
    "    # Remove duplicates and sort by year/quarter for consistent processing\n",
    "    unique_files = sorted(list(set(all_files)))\n",
    "    print(f\"Found {len(unique_files)} unique files for pattern {pattern}\")\n",
    "    return unique_files\n",
    "\n",
    "def process_single_file(file_path):\n",
    "    \"\"\"Process a single file with memory management\"\"\"\n",
    "    try:\n",
    "        # Extract metadata\n",
    "        dir_name = os.path.basename(os.path.dirname(os.path.dirname(file_path)))\n",
    "        match = re.match(r'(\\d{4})(q\\d)', dir_name)\n",
    "        year, quarter = (match.groups() if match else (None, None))\n",
    "        \n",
    "        # Try multiple delimiters and encodings\n",
    "        for delimiter in ['$', '\\t', '|']:\n",
    "            for encoding in ['ISO-8859-1', 'utf-8', 'cp1252']:\n",
    "                try:\n",
    "                    df = pd.read_csv(\n",
    "                        file_path, \n",
    "                        delimiter=delimiter, \n",
    "                        encoding=encoding,\n",
    "                        on_bad_lines=\"skip\", \n",
    "                        low_memory=False,\n",
    "                        dtype=str  # Read everything as string to save memory\n",
    "                    )\n",
    "                    \n",
    "                    # Check if we got reasonable data\n",
    "                    if len(df) > 0 and len(df.columns) > 1:\n",
    "                        df['year'] = year\n",
    "                        df['quarter'] = quarter\n",
    "                        df['source_file'] = os.path.basename(file_path)\n",
    "                        \n",
    "                        print(f\"  ✅ {os.path.basename(file_path)}: {len(df)} rows, {len(df.columns)} cols\")\n",
    "                        return df\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        print(f\"  ❌ Failed to read: {os.path.basename(file_path)}\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def combine_files_chunked(file_list, output_filename, chunk_size=CHUNK_SIZE):\n",
    "    \"\"\"Combine files in chunks to avoid memory crashes\"\"\"\n",
    "    if not file_list:\n",
    "        print(f\"ERROR: No files found for {output_filename}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n🔄 Processing {len(file_list)} files in chunks of {chunk_size}\")\n",
    "    print(f\"📁 Output: {output_filename}\")\n",
    "    \n",
    "    # Remove existing output file if it exists\n",
    "    if os.path.exists(output_filename):\n",
    "        os.remove(output_filename)\n",
    "        print(f\"🗑️  Removed existing {output_filename}\")\n",
    "    \n",
    "    total_rows = 0\n",
    "    header_written = False\n",
    "    \n",
    "    # Process files in chunks\n",
    "    for i in range(0, len(file_list), chunk_size):\n",
    "        chunk_files = file_list[i:i+chunk_size]\n",
    "        chunk_num = (i // chunk_size) + 1\n",
    "        total_chunks = (len(file_list) + chunk_size - 1) // chunk_size\n",
    "        \n",
    "        print(f\"\\n📦 Processing chunk {chunk_num}/{total_chunks} ({len(chunk_files)} files)\")\n",
    "        \n",
    "        # Check memory before processing chunk\n",
    "        memory_percent = check_memory()\n",
    "        if memory_percent > MEMORY_THRESHOLD:\n",
    "            print(f\"⚠️  Memory usage too high ({memory_percent:.1f}%). Running garbage collection...\")\n",
    "            gc.collect()\n",
    "            time.sleep(2)\n",
    "        \n",
    "        chunk_dfs = []\n",
    "        \n",
    "        # Process each file in the chunk\n",
    "        for file_path in tqdm(chunk_files, desc=f\"Chunk {chunk_num}\"):\n",
    "            df = process_single_file(file_path)\n",
    "            if df is not None:\n",
    "                chunk_dfs.append(df)\n",
    "        \n",
    "        # Combine chunk dataframes\n",
    "        if chunk_dfs:\n",
    "            print(f\"  🔗 Combining {len(chunk_dfs)} dataframes from chunk {chunk_num}\")\n",
    "            chunk_combined = pd.concat(chunk_dfs, ignore_index=True, sort=False)\n",
    "            \n",
    "            # Append to output file\n",
    "            mode = 'w' if not header_written else 'a'\n",
    "            header = not header_written\n",
    "            \n",
    "            chunk_combined.to_csv(output_filename, mode=mode, header=header, index=False)\n",
    "            \n",
    "            chunk_rows = len(chunk_combined)\n",
    "            total_rows += chunk_rows\n",
    "            header_written = True\n",
    "            \n",
    "            print(f\"  ✅ Chunk {chunk_num}: {chunk_rows} rows appended (Total: {total_rows})\")\n",
    "            \n",
    "            # Clear memory\n",
    "            del chunk_combined\n",
    "            del chunk_dfs\n",
    "            gc.collect()\n",
    "        \n",
    "        print(f\"  💾 Memory after chunk {chunk_num}: {psutil.virtual_memory().percent:.1f}%\")\n",
    "    \n",
    "    if total_rows > 0:\n",
    "        print(f\"\\n🎉 Successfully created {output_filename} with {total_rows:,} total rows\")\n",
    "        return pd.read_csv(output_filename, nrows=0)  # Return just headers for validation\n",
    "    else:\n",
    "        print(f\"\\n❌ No data processed for {output_filename}\")\n",
    "        return None\n",
    "\n",
    "def process_data_type(data_type, patterns, resume_from=None):\n",
    "    \"\"\"Process a single data type with resume capability\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🚀 PROCESSING: {data_type.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Check if already processed\n",
    "    output_files = {\n",
    "        'demo_df': 'faers_demographics_combined.csv',\n",
    "        'drug_df': 'faers_drugs_combined.csv',\n",
    "        'reac_df': 'faers_reactions_combined.csv',\n",
    "        'indi_df': 'faers_indications_combined.csv',\n",
    "        'outc_df': 'faers_outcomes_combined.csv',\n",
    "        'rpsr_df': 'faers_reports_combined.csv',\n",
    "        'ther_df': 'faers_therapy_combined.csv'\n",
    "    }\n",
    "    \n",
    "    output_filename = output_files.get(data_type, f'faers_{data_type}_combined.csv')\n",
    "    \n",
    "    if os.path.exists(output_filename) and resume_from != data_type:\n",
    "        print(f\"✅ {output_filename} already exists. Skipping...\")\n",
    "        try:\n",
    "            df = pd.read_csv(output_filename, nrows=100)  # Just check first 100 rows\n",
    "            print(f\"   File appears valid with {len(df.columns)} columns\")\n",
    "            return df\n",
    "        except:\n",
    "            print(f\"   File appears corrupted. Will reprocess...\")\n",
    "    \n",
    "    # Find files\n",
    "    all_files = []\n",
    "    for pattern in patterns:\n",
    "        files = find_files(pattern)\n",
    "        all_files.extend(files)\n",
    "    \n",
    "    unique_files = sorted(list(set(all_files)))\n",
    "    \n",
    "    if not unique_files:\n",
    "        print(f\"❌ No files found for {data_type}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"📊 Found {len(unique_files)} files to process\")\n",
    "    \n",
    "    # Process files\n",
    "    return combine_files_chunked(unique_files, output_filename)\n",
    "\n",
    "# --- MAIN EXECUTION WITH RESUME CAPABILITY ---\n",
    "def main(resume_from=None):\n",
    "    \"\"\"\n",
    "    Main function with resume capability\n",
    "    resume_from: Skip to this data type (e.g., 'drug_df' to resume from drugs)\n",
    "    \"\"\"\n",
    "    print(\"🚀 MEMORY-EFFICIENT FAERS PROCESSOR\")\n",
    "    print(\"=\"*80)\n",
    "    check_memory()\n",
    "    \n",
    "    file_types = {\n",
    "        'demo_df': ['demo*.txt', 'DEMO*.txt'],\n",
    "        'drug_df': ['drug*.txt', 'DRUG*.txt'],\n",
    "        'reac_df': ['reac*.txt', 'REAC*.txt'],\n",
    "        'indi_df': ['indi*.txt', 'INDI*.txt'],\n",
    "        'outc_df': ['outc*.txt', 'OUTC*.txt'],\n",
    "        'rpsr_df': ['rpsr*.txt', 'RPSR*.txt'],\n",
    "        'ther_df': ['ther*.txt', 'THER*.txt']\n",
    "    }\n",
    "    \n",
    "    data_frames = {}\n",
    "    processing_order = list(file_types.keys())\n",
    "    \n",
    "    # Find start point if resuming\n",
    "    start_idx = 0\n",
    "    if resume_from and resume_from in processing_order:\n",
    "        start_idx = processing_order.index(resume_from)\n",
    "        print(f\"🔄 RESUMING from {resume_from}\")\n",
    "    \n",
    "    # Process each data type\n",
    "    for data_type in processing_order[start_idx:]:\n",
    "        patterns = file_types[data_type]\n",
    "        \n",
    "        try:\n",
    "            result = process_data_type(data_type, patterns, resume_from)\n",
    "            if result is not None:\n",
    "                data_frames[data_type] = result\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n⚠️  Processing interrupted at {data_type}\")\n",
    "            print(f\"To resume, run: main(resume_from='{data_type}')\")\n",
    "            return data_frames\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error processing {data_type}: {e}\")\n",
    "            print(f\"To resume, run: main(resume_from='{data_type}')\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n🎉 ALL PROCESSING COMPLETE!\")\n",
    "    return data_frames\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    # To resume from a specific point, use: main(resume_from='drug_df')\n",
    "    data_frames = main()\n",
    "    \n",
    "    print(\"\\n📊 FINAL SUMMARY:\")\n",
    "    print(\"=\"*80)\n",
    "    for name, df in data_frames.items():\n",
    "        if df is not None:\n",
    "            # Get actual file size\n",
    "            output_files = {\n",
    "                'demo_df': 'faers_demographics_combined.csv',\n",
    "                'drug_df': 'faers_drugs_combined.csv',\n",
    "                'reac_df': 'faers_reactions_combined.csv',\n",
    "                'indi_df': 'faers_indications_combined.csv',\n",
    "                'outc_df': 'faers_outcomes_combined.csv',\n",
    "                'rpsr_df': 'faers_reports_combined.csv',\n",
    "                'ther_df': 'faers_therapy_combined.csv'\n",
    "            }\n",
    "            filename = output_files[name]\n",
    "            if os.path.exists(filename):\n",
    "                size_mb = os.path.getsize(filename) / 1024 / 1024\n",
    "                print(f\"✅ {name}: {filename} ({size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print(f\"❌ {name}: Not processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 MEMORY-EFFICIENT FAERS PROCESSOR\n",
      "📅 TARGET YEARS: [2020, 2021, 2022, 2023, 2024]\n",
      "================================================================================\n",
      "💾 Memory usage: 85.9%\n",
      "\n",
      "================================================================================\n",
      "🚀 PROCESSING: DEMO_DF (2020-2024)\n",
      "================================================================================\n",
      "Found 96 total files for pattern demo*.txt\n",
      "  ⏭️  Skipping: demo13q1.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: demo13q1.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: demo13q2.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: demo13q2.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: demo13q3.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: demo13q3.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: demo13q4.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: demo13q4.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: demo14q1.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: demo14q1.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: demo14q2.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: demo14q2.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: demo14q3.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: demo14q3.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: demo14q4.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: demo14q4.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: demo15q1.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: demo15q1.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: demo15q2.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: demo15q2.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: demo15q3.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: demo15q3.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: demo15q4.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: demo15q4.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: demo16q1.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: demo16q1.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: demo16q2.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: demo16q2.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: demo16q3.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: demo16q3.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: demo16q4.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: demo16q4.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: demo17q1.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: demo17q1.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: demo17q2.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: demo17q2.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: demo17q3.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: demo17q3.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: demo17q4.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: demo17q4.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: demo18q1_new.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: demo18q1_new.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: demo18q2.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: demo18q2.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: demo18q3.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: demo18q3.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: demo18q4.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: demo18q4.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: demo19q1.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: demo19q1.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: demo19q2.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: demo19q2.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: demo19q3.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: demo19q3.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: demo19q4.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: demo19q4.txt (2019) - outside target range\n",
      "  ✅ Including: demo20q1.txt (2020)\n",
      "  ✅ Including: demo20q1.txt (2020)\n",
      "  ✅ Including: demo20q2.txt (2020)\n",
      "  ✅ Including: demo20q2.txt (2020)\n",
      "  ✅ Including: demo20q3.txt (2020)\n",
      "  ✅ Including: demo20q3.txt (2020)\n",
      "  ✅ Including: demo20q4.txt (2020)\n",
      "  ✅ Including: demo20q4.txt (2020)\n",
      "  ✅ Including: demo21q1.txt (2021)\n",
      "  ✅ Including: demo21q1.txt (2021)\n",
      "  ✅ Including: demo21q2.txt (2021)\n",
      "  ✅ Including: demo21q2.txt (2021)\n",
      "  ✅ Including: demo21q3.txt (2021)\n",
      "  ✅ Including: demo21q3.txt (2021)\n",
      "  ✅ Including: demo21q4.txt (2021)\n",
      "  ✅ Including: demo21q4.txt (2021)\n",
      "  ✅ Including: demo22q1.txt (2022)\n",
      "  ✅ Including: demo22q1.txt (2022)\n",
      "  ✅ Including: demo22q2.txt (2022)\n",
      "  ✅ Including: demo22q2.txt (2022)\n",
      "  ✅ Including: demo22q3.txt (2022)\n",
      "  ✅ Including: demo22q3.txt (2022)\n",
      "  ✅ Including: demo22q4.txt (2022)\n",
      "  ✅ Including: demo22q4.txt (2022)\n",
      "  ✅ Including: demo23q1.txt (2023)\n",
      "  ✅ Including: demo23q1.txt (2023)\n",
      "  ✅ Including: demo23q2.txt (2023)\n",
      "  ✅ Including: demo23q2.txt (2023)\n",
      "  ✅ Including: demo23q3.txt (2023)\n",
      "  ✅ Including: demo23q3.txt (2023)\n",
      "  ✅ Including: demo23q4.txt (2023)\n",
      "  ✅ Including: demo23q4.txt (2023)\n",
      "  ✅ Including: demo24q1.txt (2024)\n",
      "  ✅ Including: demo24q1.txt (2024)\n",
      "  ✅ Including: demo24q2.txt (2024)\n",
      "  ✅ Including: demo24q2.txt (2024)\n",
      "  ✅ Including: demo24q3.txt (2024)\n",
      "  ✅ Including: demo24q3.txt (2024)\n",
      "  ✅ Including: demo24q4.txt (2024)\n",
      "  ✅ Including: demo24q4.txt (2024)\n",
      "\n",
      "📊 Filtered 96 files → 40 files (2020-2024)\n",
      "Found 0 total files for pattern DEMO*.txt\n",
      "\n",
      "📊 Filtered 0 files → 0 files (2020-2024)\n",
      "📊 Found 40 files to process for [2020, 2021, 2022, 2023, 2024]\n",
      "\n",
      "🔄 Processing 40 files in chunks of 10\n",
      "📁 Output: faers_demographics_combined.csv\n",
      "\n",
      "📦 Processing chunk 1/4 (10 files)\n",
      "💾 Memory usage: 85.9%\n",
      "⚠️  Memory usage too high (85.9%). Running garbage collection...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e333082fa564cc7903f3a963de4d669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ demo20q1.txt: 460327 rows, 28 cols (2020)\n",
      "  ✅ demo20q1.txt: 460327 rows, 28 cols (2020)\n",
      "  ✅ demo20q2.txt: 429227 rows, 28 cols (2020)\n",
      "  ✅ demo20q2.txt: 429227 rows, 28 cols (2020)\n",
      "  ✅ demo20q3.txt: 431667 rows, 28 cols (2020)\n",
      "  ✅ demo20q3.txt: 431667 rows, 28 cols (2020)\n",
      "  ✅ demo20q4.txt: 436148 rows, 28 cols (2020)\n",
      "  ✅ demo20q4.txt: 436148 rows, 28 cols (2020)\n",
      "  ✅ demo21q1.txt: 463741 rows, 28 cols (2021)\n",
      "  ✅ demo21q1.txt: 463741 rows, 28 cols (2021)\n",
      "  🔗 Combining 10 dataframes from chunk 1\n",
      "  ✅ Chunk 1: 4442220 rows appended (Total: 4442220)\n",
      "  💾 Memory after chunk 1: 61.4%\n",
      "\n",
      "📦 Processing chunk 2/4 (10 files)\n",
      "💾 Memory usage: 61.4%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0627b81e024dfcb54c32cb212eb203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ demo21q2.txt: 479945 rows, 28 cols (2021)\n",
      "  ✅ demo21q2.txt: 479945 rows, 28 cols (2021)\n",
      "  ✅ demo21q3.txt: 504160 rows, 28 cols (2021)\n",
      "  ✅ demo21q3.txt: 504160 rows, 28 cols (2021)\n",
      "  ✅ demo21q4.txt: 412542 rows, 28 cols (2021)\n",
      "  ✅ demo21q4.txt: 412542 rows, 28 cols (2021)\n",
      "  ✅ demo22q1.txt: 461623 rows, 28 cols (2022)\n",
      "  ✅ demo22q1.txt: 461623 rows, 28 cols (2022)\n",
      "  ✅ demo22q2.txt: 435618 rows, 28 cols (2022)\n",
      "  ✅ demo22q2.txt: 435618 rows, 28 cols (2022)\n",
      "  🔗 Combining 10 dataframes from chunk 2\n",
      "  ✅ Chunk 2: 4587776 rows appended (Total: 9029996)\n",
      "  💾 Memory after chunk 2: 55.6%\n",
      "\n",
      "📦 Processing chunk 3/4 (10 files)\n",
      "💾 Memory usage: 55.6%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe43369048b94757a1d8d547274a747c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ demo22q3.txt: 446511 rows, 28 cols (2022)\n",
      "  ✅ demo22q3.txt: 446511 rows, 28 cols (2022)\n",
      "  ✅ demo22q4.txt: 483643 rows, 28 cols (2022)\n",
      "  ✅ demo22q4.txt: 483643 rows, 28 cols (2022)\n",
      "  ✅ demo23q1.txt: 432144 rows, 28 cols (2023)\n",
      "  ✅ demo23q1.txt: 432144 rows, 28 cols (2023)\n",
      "  ✅ demo23q2.txt: 418592 rows, 28 cols (2023)\n",
      "  ✅ demo23q2.txt: 418592 rows, 28 cols (2023)\n",
      "  ✅ demo23q3.txt: 407522 rows, 28 cols (2023)\n",
      "  ✅ demo23q3.txt: 407522 rows, 28 cols (2023)\n",
      "  🔗 Combining 10 dataframes from chunk 3\n",
      "  ✅ Chunk 3: 4376824 rows appended (Total: 13406820)\n",
      "  💾 Memory after chunk 3: 55.6%\n",
      "\n",
      "📦 Processing chunk 4/4 (10 files)\n",
      "💾 Memory usage: 55.6%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ea4e93c7bc47a88237a1f8704273ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ demo23q4.txt: 415379 rows, 28 cols (2023)\n",
      "  ✅ demo23q4.txt: 415379 rows, 28 cols (2023)\n",
      "  ✅ demo24q1.txt: 406184 rows, 28 cols (2024)\n",
      "  ✅ demo24q1.txt: 406184 rows, 28 cols (2024)\n",
      "  ✅ demo24q2.txt: 397119 rows, 28 cols (2024)\n",
      "  ✅ demo24q2.txt: 397119 rows, 28 cols (2024)\n",
      "  ✅ demo24q3.txt: 405513 rows, 28 cols (2024)\n",
      "  ✅ demo24q3.txt: 405513 rows, 28 cols (2024)\n",
      "  ✅ demo24q4.txt: 410849 rows, 28 cols (2024)\n",
      "  ✅ demo24q4.txt: 410849 rows, 28 cols (2024)\n",
      "  🔗 Combining 10 dataframes from chunk 4\n",
      "  ✅ Chunk 4: 4070088 rows appended (Total: 17476908)\n",
      "  💾 Memory after chunk 4: 66.0%\n",
      "\n",
      "🎉 Successfully created faers_demographics_combined_2020_2024.csv with 17,476,908 total rows\n",
      "\n",
      "================================================================================\n",
      "🚀 PROCESSING: DRUG_DF (2020-2024)\n",
      "================================================================================\n",
      "Found 96 total files for pattern drug*.txt\n",
      "  ⏭️  Skipping: drug13q1.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: drug13q1.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: drug13q2.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: drug13q2.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: drug13q3.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: drug13q3.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: drug13q4.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: drug13q4.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: drug14q1.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: drug14q1.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: drug14q2.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: drug14q2.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: drug14q3.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: drug14q3.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: drug14q4.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: drug14q4.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: drug15q1.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: drug15q1.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: drug15q2.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: drug15q2.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: drug15q3.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: drug15q3.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: drug15q4.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: drug15q4.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: drug16q1.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: drug16q1.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: drug16q2.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: drug16q2.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: drug16q3.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: drug16q3.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: drug16q4.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: drug16q4.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: drug17q1.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: drug17q1.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: drug17q2.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: drug17q2.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: drug17q3.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: drug17q3.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: drug17q4.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: drug17q4.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: drug18q1.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: drug18q1.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: drug18q2.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: drug18q2.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: drug18q3.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: drug18q3.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: drug18q4.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: drug18q4.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: drug19q1.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: drug19q1.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: drug19q2.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: drug19q2.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: drug19q3.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: drug19q3.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: drug19q4.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: drug19q4.txt (2019) - outside target range\n",
      "  ✅ Including: drug20q1.txt (2020)\n",
      "  ✅ Including: drug20q1.txt (2020)\n",
      "  ✅ Including: drug20q2.txt (2020)\n",
      "  ✅ Including: drug20q2.txt (2020)\n",
      "  ✅ Including: drug20q3.txt (2020)\n",
      "  ✅ Including: drug20q3.txt (2020)\n",
      "  ✅ Including: drug20q4.txt (2020)\n",
      "  ✅ Including: drug20q4.txt (2020)\n",
      "  ✅ Including: drug21q1.txt (2021)\n",
      "  ✅ Including: drug21q1.txt (2021)\n",
      "  ✅ Including: drug21q2.txt (2021)\n",
      "  ✅ Including: drug21q2.txt (2021)\n",
      "  ✅ Including: drug21q3.txt (2021)\n",
      "  ✅ Including: drug21q3.txt (2021)\n",
      "  ✅ Including: drug21q4.txt (2021)\n",
      "  ✅ Including: drug21q4.txt (2021)\n",
      "  ✅ Including: drug22q1.txt (2022)\n",
      "  ✅ Including: drug22q1.txt (2022)\n",
      "  ✅ Including: drug22q2.txt (2022)\n",
      "  ✅ Including: drug22q2.txt (2022)\n",
      "  ✅ Including: drug22q3.txt (2022)\n",
      "  ✅ Including: drug22q3.txt (2022)\n",
      "  ✅ Including: drug22q4.txt (2022)\n",
      "  ✅ Including: drug22q4.txt (2022)\n",
      "  ✅ Including: drug23q1.txt (2023)\n",
      "  ✅ Including: drug23q1.txt (2023)\n",
      "  ✅ Including: drug23q2.txt (2023)\n",
      "  ✅ Including: drug23q2.txt (2023)\n",
      "  ✅ Including: drug23q3.txt (2023)\n",
      "  ✅ Including: drug23q3.txt (2023)\n",
      "  ✅ Including: drug23q4.txt (2023)\n",
      "  ✅ Including: drug23q4.txt (2023)\n",
      "  ✅ Including: drug24q1.txt (2024)\n",
      "  ✅ Including: drug24q1.txt (2024)\n",
      "  ✅ Including: drug24q2.txt (2024)\n",
      "  ✅ Including: drug24q2.txt (2024)\n",
      "  ✅ Including: drug24q3.txt (2024)\n",
      "  ✅ Including: drug24q3.txt (2024)\n",
      "  ✅ Including: drug24q4.txt (2024)\n",
      "  ✅ Including: drug24q4.txt (2024)\n",
      "\n",
      "📊 Filtered 96 files → 40 files (2020-2024)\n",
      "Found 0 total files for pattern DRUG*.txt\n",
      "\n",
      "📊 Filtered 0 files → 0 files (2020-2024)\n",
      "📊 Found 40 files to process for [2020, 2021, 2022, 2023, 2024]\n",
      "\n",
      "🔄 Processing 40 files in chunks of 10\n",
      "📁 Output: faers_drugs_combined.csv\n",
      "\n",
      "📦 Processing chunk 1/4 (10 files)\n",
      "💾 Memory usage: 63.9%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be6271018f24c75aae3e74db0a9ea5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ drug20q1.txt: 1943532 rows, 23 cols (2020)\n",
      "  ✅ drug20q1.txt: 1943532 rows, 23 cols (2020)\n",
      "  ✅ drug20q2.txt: 1825414 rows, 23 cols (2020)\n",
      "  ✅ drug20q2.txt: 1825414 rows, 23 cols (2020)\n",
      "  ✅ drug20q3.txt: 1895153 rows, 23 cols (2020)\n",
      "  ✅ drug20q3.txt: 1895153 rows, 23 cols (2020)\n",
      "  ✅ drug20q4.txt: 1918927 rows, 23 cols (2020)\n",
      "  ✅ drug20q4.txt: 1918927 rows, 23 cols (2020)\n",
      "  ✅ drug21q1.txt: 2208416 rows, 23 cols (2021)\n",
      "  ✅ drug21q1.txt: 2208416 rows, 23 cols (2021)\n",
      "  🔗 Combining 10 dataframes from chunk 1\n",
      "  ✅ Chunk 1: 19582884 rows appended (Total: 19582884)\n",
      "  💾 Memory after chunk 1: 48.0%\n",
      "\n",
      "📦 Processing chunk 2/4 (10 files)\n",
      "💾 Memory usage: 48.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bda71461144b60be217a930ee2c204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ drug21q2.txt: 2291903 rows, 23 cols (2021)\n",
      "  ✅ drug21q2.txt: 2291903 rows, 23 cols (2021)\n",
      "  ✅ drug21q3.txt: 2260570 rows, 23 cols (2021)\n",
      "  ✅ drug21q3.txt: 2260570 rows, 23 cols (2021)\n",
      "  ✅ drug21q4.txt: 1778675 rows, 23 cols (2021)\n",
      "  ✅ drug21q4.txt: 1778675 rows, 23 cols (2021)\n",
      "  ✅ drug22q1.txt: 1994171 rows, 23 cols (2022)\n",
      "  ✅ drug22q1.txt: 1994171 rows, 23 cols (2022)\n",
      "  ✅ drug22q2.txt: 1828103 rows, 23 cols (2022)\n",
      "  ✅ drug22q2.txt: 1828103 rows, 23 cols (2022)\n",
      "  🔗 Combining 10 dataframes from chunk 2\n",
      "  ✅ Chunk 2: 20306844 rows appended (Total: 39889728)\n",
      "  💾 Memory after chunk 2: 44.5%\n",
      "\n",
      "📦 Processing chunk 3/4 (10 files)\n",
      "💾 Memory usage: 44.5%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a999f286149490f9268384d1136acfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ drug22q3.txt: 1835461 rows, 23 cols (2022)\n",
      "  ✅ drug22q3.txt: 1835461 rows, 23 cols (2022)\n",
      "  ✅ drug22q4.txt: 2006967 rows, 23 cols (2022)\n",
      "  ✅ drug22q4.txt: 2006967 rows, 23 cols (2022)\n",
      "  ✅ drug23q1.txt: 1899503 rows, 23 cols (2023)\n",
      "  ✅ drug23q1.txt: 1899503 rows, 23 cols (2023)\n",
      "  ✅ drug23q2.txt: 1885096 rows, 23 cols (2023)\n",
      "  ✅ drug23q2.txt: 1885096 rows, 23 cols (2023)\n",
      "  ✅ drug23q3.txt: 1768391 rows, 23 cols (2023)\n",
      "  ✅ drug23q3.txt: 1768391 rows, 23 cols (2023)\n",
      "  🔗 Combining 10 dataframes from chunk 3\n",
      "  ✅ Chunk 3: 18790836 rows appended (Total: 58680564)\n",
      "  💾 Memory after chunk 3: 45.3%\n",
      "\n",
      "📦 Processing chunk 4/4 (10 files)\n",
      "💾 Memory usage: 45.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7008bdd09b4623a15b20e6332219e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ drug23q4.txt: 1920732 rows, 23 cols (2023)\n",
      "  ✅ drug23q4.txt: 1920732 rows, 23 cols (2023)\n",
      "  ✅ drug24q1.txt: 1909327 rows, 23 cols (2024)\n",
      "  ✅ drug24q1.txt: 1909327 rows, 23 cols (2024)\n",
      "  ✅ drug24q2.txt: 1888937 rows, 23 cols (2024)\n",
      "  ✅ drug24q2.txt: 1888937 rows, 23 cols (2024)\n",
      "  ✅ drug24q3.txt: 1907293 rows, 23 cols (2024)\n",
      "  ✅ drug24q3.txt: 1907293 rows, 23 cols (2024)\n",
      "  ✅ drug24q4.txt: 2030938 rows, 23 cols (2024)\n",
      "  ✅ drug24q4.txt: 2030938 rows, 23 cols (2024)\n",
      "  🔗 Combining 10 dataframes from chunk 4\n",
      "  ✅ Chunk 4: 19314454 rows appended (Total: 77995018)\n",
      "  💾 Memory after chunk 4: 43.7%\n",
      "\n",
      "🎉 Successfully created faers_drugs_combined_2020_2024.csv with 77,995,018 total rows\n",
      "\n",
      "================================================================================\n",
      "🚀 PROCESSING: REAC_DF (2020-2024)\n",
      "================================================================================\n",
      "Found 96 total files for pattern reac*.txt\n",
      "  ⏭️  Skipping: reac13q1.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: reac13q1.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: reac13q2.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: reac13q2.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: reac13q3.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: reac13q3.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: reac13q4.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: reac13q4.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: reac14q1.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: reac14q1.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: reac14q2.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: reac14q2.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: reac14q3.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: reac14q3.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: reac14q4.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: reac14q4.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: reac15q1.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: reac15q1.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: reac15q2.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: reac15q2.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: reac15q3.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: reac15q3.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: reac15q4.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: reac15q4.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: reac16q1.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: reac16q1.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: reac16q2.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: reac16q2.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: reac16q3.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: reac16q3.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: reac16q4.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: reac16q4.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: reac17q1.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: reac17q1.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: reac17q2.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: reac17q2.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: reac17q3.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: reac17q3.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: reac17q4.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: reac17q4.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: reac18q1.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: reac18q1.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: reac18q2.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: reac18q2.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: reac18q3.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: reac18q3.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: reac18q4.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: reac18q4.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: reac19q1.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: reac19q1.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: reac19q2.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: reac19q2.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: reac19q3.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: reac19q3.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: reac19q4.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: reac19q4.txt (2019) - outside target range\n",
      "  ✅ Including: reac20q1.txt (2020)\n",
      "  ✅ Including: reac20q1.txt (2020)\n",
      "  ✅ Including: reac20q2.txt (2020)\n",
      "  ✅ Including: reac20q2.txt (2020)\n",
      "  ✅ Including: reac20q3.txt (2020)\n",
      "  ✅ Including: reac20q3.txt (2020)\n",
      "  ✅ Including: reac20q4.txt (2020)\n",
      "  ✅ Including: reac20q4.txt (2020)\n",
      "  ✅ Including: reac21q1.txt (2021)\n",
      "  ✅ Including: reac21q1.txt (2021)\n",
      "  ✅ Including: reac21q2.txt (2021)\n",
      "  ✅ Including: reac21q2.txt (2021)\n",
      "  ✅ Including: reac21q3.txt (2021)\n",
      "  ✅ Including: reac21q3.txt (2021)\n",
      "  ✅ Including: reac21q4.txt (2021)\n",
      "  ✅ Including: reac21q4.txt (2021)\n",
      "  ✅ Including: reac22q1.txt (2022)\n",
      "  ✅ Including: reac22q1.txt (2022)\n",
      "  ✅ Including: reac22q2.txt (2022)\n",
      "  ✅ Including: reac22q2.txt (2022)\n",
      "  ✅ Including: reac22q3.txt (2022)\n",
      "  ✅ Including: reac22q3.txt (2022)\n",
      "  ✅ Including: reac22q4.txt (2022)\n",
      "  ✅ Including: reac22q4.txt (2022)\n",
      "  ✅ Including: reac23q1.txt (2023)\n",
      "  ✅ Including: reac23q1.txt (2023)\n",
      "  ✅ Including: reac23q2.txt (2023)\n",
      "  ✅ Including: reac23q2.txt (2023)\n",
      "  ✅ Including: reac23q3.txt (2023)\n",
      "  ✅ Including: reac23q3.txt (2023)\n",
      "  ✅ Including: reac23q4.txt (2023)\n",
      "  ✅ Including: reac23q4.txt (2023)\n",
      "  ✅ Including: reac24q1.txt (2024)\n",
      "  ✅ Including: reac24q1.txt (2024)\n",
      "  ✅ Including: reac24q2.txt (2024)\n",
      "  ✅ Including: reac24q2.txt (2024)\n",
      "  ✅ Including: reac24q3.txt (2024)\n",
      "  ✅ Including: reac24q3.txt (2024)\n",
      "  ✅ Including: reac24q4.txt (2024)\n",
      "  ✅ Including: reac24q4.txt (2024)\n",
      "\n",
      "📊 Filtered 96 files → 40 files (2020-2024)\n",
      "Found 0 total files for pattern REAC*.txt\n",
      "\n",
      "📊 Filtered 0 files → 0 files (2020-2024)\n",
      "📊 Found 40 files to process for [2020, 2021, 2022, 2023, 2024]\n",
      "\n",
      "🔄 Processing 40 files in chunks of 10\n",
      "📁 Output: faers_reactions_combined.csv\n",
      "\n",
      "📦 Processing chunk 1/4 (10 files)\n",
      "💾 Memory usage: 43.8%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a1908808cc49349f2206f562a611f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ reac20q1.txt: 1517264 rows, 7 cols (2020)\n",
      "  ✅ reac20q1.txt: 1517264 rows, 7 cols (2020)\n",
      "  ✅ reac20q2.txt: 1437285 rows, 7 cols (2020)\n",
      "  ✅ reac20q2.txt: 1437285 rows, 7 cols (2020)\n",
      "  ✅ reac20q3.txt: 1454044 rows, 7 cols (2020)\n",
      "  ✅ reac20q3.txt: 1454044 rows, 7 cols (2020)\n",
      "  ✅ reac20q4.txt: 1522657 rows, 7 cols (2020)\n",
      "  ✅ reac20q4.txt: 1522657 rows, 7 cols (2020)\n",
      "  ✅ reac21q1.txt: 1505167 rows, 7 cols (2021)\n",
      "  ✅ reac21q1.txt: 1505167 rows, 7 cols (2021)\n",
      "  🔗 Combining 10 dataframes from chunk 1\n",
      "  ✅ Chunk 1: 14872834 rows appended (Total: 14872834)\n",
      "  💾 Memory after chunk 1: 57.8%\n",
      "\n",
      "📦 Processing chunk 2/4 (10 files)\n",
      "💾 Memory usage: 57.8%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7656901897964b828b530a59a3d94532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ reac21q2.txt: 1526544 rows, 7 cols (2021)\n",
      "  ✅ reac21q2.txt: 1526544 rows, 7 cols (2021)\n",
      "  ✅ reac21q3.txt: 1544374 rows, 7 cols (2021)\n",
      "  ✅ reac21q3.txt: 1544374 rows, 7 cols (2021)\n",
      "  ✅ reac21q4.txt: 1355734 rows, 7 cols (2021)\n",
      "  ✅ reac21q4.txt: 1355734 rows, 7 cols (2021)\n",
      "  ✅ reac22q1.txt: 1543059 rows, 7 cols (2022)\n",
      "  ✅ reac22q1.txt: 1543059 rows, 7 cols (2022)\n",
      "  ✅ reac22q2.txt: 1464627 rows, 7 cols (2022)\n",
      "  ✅ reac22q2.txt: 1464627 rows, 7 cols (2022)\n",
      "  🔗 Combining 10 dataframes from chunk 2\n",
      "  ✅ Chunk 2: 14868676 rows appended (Total: 29741510)\n",
      "  💾 Memory after chunk 2: 55.8%\n",
      "\n",
      "📦 Processing chunk 3/4 (10 files)\n",
      "💾 Memory usage: 55.8%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8872766f662e4cdc992b329e40d8918e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ reac22q3.txt: 1449509 rows, 7 cols (2022)\n",
      "  ✅ reac22q3.txt: 1449509 rows, 7 cols (2022)\n",
      "  ✅ reac22q4.txt: 1617584 rows, 7 cols (2022)\n",
      "  ✅ reac22q4.txt: 1617584 rows, 7 cols (2022)\n",
      "  ✅ reac23q1.txt: 1491473 rows, 7 cols (2023)\n",
      "  ✅ reac23q1.txt: 1491473 rows, 7 cols (2023)\n",
      "  ✅ reac23q2.txt: 1478973 rows, 7 cols (2023)\n",
      "  ✅ reac23q2.txt: 1478973 rows, 7 cols (2023)\n",
      "  ✅ reac23q3.txt: 1373338 rows, 7 cols (2023)\n",
      "  ✅ reac23q3.txt: 1373338 rows, 7 cols (2023)\n",
      "  🔗 Combining 10 dataframes from chunk 3\n",
      "  ✅ Chunk 3: 14821754 rows appended (Total: 44563264)\n",
      "  💾 Memory after chunk 3: 58.3%\n",
      "\n",
      "📦 Processing chunk 4/4 (10 files)\n",
      "💾 Memory usage: 58.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28aa93f8b8604458b986baa779f068af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ reac23q4.txt: 1500033 rows, 7 cols (2023)\n",
      "  ✅ reac23q4.txt: 1500033 rows, 7 cols (2023)\n",
      "  ✅ reac24q1.txt: 1445416 rows, 7 cols (2024)\n",
      "  ✅ reac24q1.txt: 1445416 rows, 7 cols (2024)\n",
      "  ✅ reac24q2.txt: 1445044 rows, 7 cols (2024)\n",
      "  ✅ reac24q2.txt: 1445044 rows, 7 cols (2024)\n",
      "  ✅ reac24q3.txt: 1431718 rows, 7 cols (2024)\n",
      "  ✅ reac24q3.txt: 1431718 rows, 7 cols (2024)\n",
      "  ✅ reac24q4.txt: 1472750 rows, 7 cols (2024)\n",
      "  ✅ reac24q4.txt: 1472750 rows, 7 cols (2024)\n",
      "  🔗 Combining 10 dataframes from chunk 4\n",
      "  ✅ Chunk 4: 14589922 rows appended (Total: 59153186)\n",
      "  💾 Memory after chunk 4: 60.5%\n",
      "\n",
      "🎉 Successfully created faers_reactions_combined_2020_2024.csv with 59,153,186 total rows\n",
      "\n",
      "================================================================================\n",
      "🚀 PROCESSING: INDI_DF (2020-2024)\n",
      "================================================================================\n",
      "Found 0 total files for pattern indi*.txt\n",
      "\n",
      "📊 Filtered 0 files → 0 files (2020-2024)\n",
      "Found 96 total files for pattern INDI*.txt\n",
      "  ⏭️  Skipping: INDI13Q1.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: INDI13Q1.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: INDI13Q2.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: INDI13Q2.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: INDI13Q3.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: INDI13Q3.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: INDI13Q4.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: INDI13Q4.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: INDI14Q1.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: INDI14Q1.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: INDI14Q2.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: INDI14Q2.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: INDI14Q3.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: INDI14Q3.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: INDI14Q4.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: INDI14Q4.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: INDI15Q1.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: INDI15Q1.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: INDI15Q2.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: INDI15Q2.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: INDI15Q3.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: INDI15Q3.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: INDI15Q4.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: INDI15Q4.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: INDI16Q1.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: INDI16Q1.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: INDI16Q2.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: INDI16Q2.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: INDI16Q3.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: INDI16Q3.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: INDI16Q4.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: INDI16Q4.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: INDI17Q1.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: INDI17Q1.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: INDI17Q2.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: INDI17Q2.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: INDI17Q3.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: INDI17Q3.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: INDI17Q4.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: INDI17Q4.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: INDI18Q1.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: INDI18Q1.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: INDI18Q2.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: INDI18Q2.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: INDI18Q3.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: INDI18Q3.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: INDI18Q4.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: INDI18Q4.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: INDI19Q1.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: INDI19Q1.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: INDI19Q2.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: INDI19Q2.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: INDI19Q3.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: INDI19Q3.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: INDI19Q4.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: INDI19Q4.txt (2019) - outside target range\n",
      "  ✅ Including: INDI20Q1.txt (2020)\n",
      "  ✅ Including: INDI20Q1.txt (2020)\n",
      "  ✅ Including: INDI20Q2.txt (2020)\n",
      "  ✅ Including: INDI20Q2.txt (2020)\n",
      "  ✅ Including: INDI20Q3.txt (2020)\n",
      "  ✅ Including: INDI20Q3.txt (2020)\n",
      "  ✅ Including: INDI20Q4.txt (2020)\n",
      "  ✅ Including: INDI20Q4.txt (2020)\n",
      "  ✅ Including: INDI21Q1.txt (2021)\n",
      "  ✅ Including: INDI21Q1.txt (2021)\n",
      "  ✅ Including: INDI21Q2.txt (2021)\n",
      "  ✅ Including: INDI21Q2.txt (2021)\n",
      "  ✅ Including: INDI21Q3.txt (2021)\n",
      "  ✅ Including: INDI21Q3.txt (2021)\n",
      "  ✅ Including: INDI21Q4.txt (2021)\n",
      "  ✅ Including: INDI21Q4.txt (2021)\n",
      "  ✅ Including: INDI22Q1.txt (2022)\n",
      "  ✅ Including: INDI22Q1.txt (2022)\n",
      "  ✅ Including: INDI22Q2.txt (2022)\n",
      "  ✅ Including: INDI22Q2.txt (2022)\n",
      "  ✅ Including: INDI22Q3.txt (2022)\n",
      "  ✅ Including: INDI22Q3.txt (2022)\n",
      "  ✅ Including: INDI22Q4.txt (2022)\n",
      "  ✅ Including: INDI22Q4.txt (2022)\n",
      "  ✅ Including: INDI23Q1.txt (2023)\n",
      "  ✅ Including: INDI23Q1.txt (2023)\n",
      "  ✅ Including: INDI23Q2.txt (2023)\n",
      "  ✅ Including: INDI23Q2.txt (2023)\n",
      "  ✅ Including: INDI23Q3.txt (2023)\n",
      "  ✅ Including: INDI23Q3.txt (2023)\n",
      "  ✅ Including: INDI23Q4.txt (2023)\n",
      "  ✅ Including: INDI23Q4.txt (2023)\n",
      "  ✅ Including: INDI24Q1.txt (2024)\n",
      "  ✅ Including: INDI24Q1.txt (2024)\n",
      "  ✅ Including: INDI24Q2.txt (2024)\n",
      "  ✅ Including: INDI24Q2.txt (2024)\n",
      "  ✅ Including: INDI24Q3.txt (2024)\n",
      "  ✅ Including: INDI24Q3.txt (2024)\n",
      "  ✅ Including: INDI24Q4.txt (2024)\n",
      "  ✅ Including: INDI24Q4.txt (2024)\n",
      "\n",
      "📊 Filtered 96 files → 40 files (2020-2024)\n",
      "📊 Found 40 files to process for [2020, 2021, 2022, 2023, 2024]\n",
      "\n",
      "🔄 Processing 40 files in chunks of 10\n",
      "📁 Output: faers_indications_combined.csv\n",
      "\n",
      "📦 Processing chunk 1/4 (10 files)\n",
      "💾 Memory usage: 59.5%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f6b325dc1947c4807d2aba8065415a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ INDI20Q1.txt: 1348658 rows, 7 cols (2020)\n",
      "  ✅ INDI20Q1.txt: 1348658 rows, 7 cols (2020)\n",
      "  ✅ INDI20Q2.txt: 1294010 rows, 7 cols (2020)\n",
      "  ✅ INDI20Q2.txt: 1294010 rows, 7 cols (2020)\n",
      "  ✅ INDI20Q3.txt: 1276348 rows, 7 cols (2020)\n",
      "  ✅ INDI20Q3.txt: 1276348 rows, 7 cols (2020)\n",
      "  ✅ INDI20Q4.txt: 1297446 rows, 7 cols (2020)\n",
      "  ✅ INDI20Q4.txt: 1297446 rows, 7 cols (2020)\n",
      "  ✅ INDI21Q1.txt: 1603039 rows, 7 cols (2021)\n",
      "  ✅ INDI21Q1.txt: 1603039 rows, 7 cols (2021)\n",
      "  🔗 Combining 10 dataframes from chunk 1\n",
      "  ✅ Chunk 1: 13639002 rows appended (Total: 13639002)\n",
      "  💾 Memory after chunk 1: 63.5%\n",
      "\n",
      "📦 Processing chunk 2/4 (10 files)\n",
      "💾 Memory usage: 63.5%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33019e0f2fb440f69b2302ee2cfd5dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ INDI21Q2.txt: 1659378 rows, 7 cols (2021)\n",
      "  ✅ INDI21Q2.txt: 1659378 rows, 7 cols (2021)\n",
      "  ✅ INDI21Q3.txt: 1588679 rows, 7 cols (2021)\n",
      "  ✅ INDI21Q3.txt: 1588679 rows, 7 cols (2021)\n",
      "  ✅ INDI21Q4.txt: 1234766 rows, 7 cols (2021)\n",
      "  ✅ INDI21Q4.txt: 1234766 rows, 7 cols (2021)\n",
      "  ✅ INDI22Q1.txt: 1347146 rows, 7 cols (2022)\n",
      "  ✅ INDI22Q1.txt: 1347146 rows, 7 cols (2022)\n",
      "  ✅ INDI22Q2.txt: 1150299 rows, 7 cols (2022)\n",
      "  ✅ INDI22Q2.txt: 1150299 rows, 7 cols (2022)\n",
      "  🔗 Combining 10 dataframes from chunk 2\n",
      "  ✅ Chunk 2: 13960536 rows appended (Total: 27599538)\n",
      "  💾 Memory after chunk 2: 65.0%\n",
      "\n",
      "📦 Processing chunk 3/4 (10 files)\n",
      "💾 Memory usage: 65.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e209eb735d3465bab4cd002be0c842e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ INDI22Q3.txt: 1159203 rows, 7 cols (2022)\n",
      "  ✅ INDI22Q3.txt: 1159203 rows, 7 cols (2022)\n",
      "  ✅ INDI22Q4.txt: 1321489 rows, 7 cols (2022)\n",
      "  ✅ INDI22Q4.txt: 1321489 rows, 7 cols (2022)\n",
      "  ✅ INDI23Q1.txt: 1176237 rows, 7 cols (2023)\n",
      "  ✅ INDI23Q1.txt: 1176237 rows, 7 cols (2023)\n",
      "  ✅ INDI23Q2.txt: 1165782 rows, 7 cols (2023)\n",
      "  ✅ INDI23Q2.txt: 1165782 rows, 7 cols (2023)\n",
      "  ✅ INDI23Q3.txt: 1063761 rows, 7 cols (2023)\n",
      "  ✅ INDI23Q3.txt: 1063761 rows, 7 cols (2023)\n",
      "  🔗 Combining 10 dataframes from chunk 3\n",
      "  ✅ Chunk 3: 11772944 rows appended (Total: 39372482)\n",
      "  💾 Memory after chunk 3: 62.7%\n",
      "\n",
      "📦 Processing chunk 4/4 (10 files)\n",
      "💾 Memory usage: 62.6%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b04e8b36c804f9c8d02fca1f7808745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ INDI23Q4.txt: 1115961 rows, 7 cols (2023)\n",
      "  ✅ INDI23Q4.txt: 1115961 rows, 7 cols (2023)\n",
      "  ✅ INDI24Q1.txt: 1186115 rows, 7 cols (2024)\n",
      "  ✅ INDI24Q1.txt: 1186115 rows, 7 cols (2024)\n",
      "  ✅ INDI24Q2.txt: 1187626 rows, 7 cols (2024)\n",
      "  ✅ INDI24Q2.txt: 1187626 rows, 7 cols (2024)\n",
      "  ✅ INDI24Q3.txt: 1177133 rows, 7 cols (2024)\n",
      "  ✅ INDI24Q3.txt: 1177133 rows, 7 cols (2024)\n",
      "  ✅ INDI24Q4.txt: 1219759 rows, 7 cols (2024)\n",
      "  ✅ INDI24Q4.txt: 1219759 rows, 7 cols (2024)\n",
      "  🔗 Combining 10 dataframes from chunk 4\n",
      "  ✅ Chunk 4: 11773188 rows appended (Total: 51145670)\n",
      "  💾 Memory after chunk 4: 63.0%\n",
      "\n",
      "🎉 Successfully created faers_indications_combined_2020_2024.csv with 51,145,670 total rows\n",
      "\n",
      "================================================================================\n",
      "🚀 PROCESSING: OUTC_DF (2020-2024)\n",
      "================================================================================\n",
      "Found 0 total files for pattern outc*.txt\n",
      "\n",
      "📊 Filtered 0 files → 0 files (2020-2024)\n",
      "Found 96 total files for pattern OUTC*.txt\n",
      "  ⏭️  Skipping: OUTC13Q1.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: OUTC13Q1.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: OUTC13Q2.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: OUTC13Q2.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: OUTC13Q3.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: OUTC13Q3.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: OUTC13Q4.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: OUTC13Q4.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: OUTC14Q1.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: OUTC14Q1.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: OUTC14Q2.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: OUTC14Q2.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: OUTC14Q3.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: OUTC14Q3.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: OUTC14Q4.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: OUTC14Q4.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: OUTC15Q1.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: OUTC15Q1.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: OUTC15Q2.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: OUTC15Q2.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: OUTC15Q3.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: OUTC15Q3.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: OUTC15Q4.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: OUTC15Q4.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: OUTC16Q1.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: OUTC16Q1.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: OUTC16Q2.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: OUTC16Q2.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: OUTC16Q3.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: OUTC16Q3.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: OUTC16Q4.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: OUTC16Q4.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: OUTC17Q1.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: OUTC17Q1.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: OUTC17Q2.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: OUTC17Q2.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: OUTC17Q3.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: OUTC17Q3.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: OUTC17Q4.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: OUTC17Q4.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: OUTC18Q1.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: OUTC18Q1.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: OUTC18Q2.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: OUTC18Q2.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: OUTC18Q3.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: OUTC18Q3.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: OUTC18Q4.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: OUTC18Q4.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: OUTC19Q1.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: OUTC19Q1.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: OUTC19Q2.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: OUTC19Q2.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: OUTC19Q3.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: OUTC19Q3.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: OUTC19Q4.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: OUTC19Q4.txt (2019) - outside target range\n",
      "  ✅ Including: OUTC20Q1.txt (2020)\n",
      "  ✅ Including: OUTC20Q1.txt (2020)\n",
      "  ✅ Including: OUTC20Q2.txt (2020)\n",
      "  ✅ Including: OUTC20Q2.txt (2020)\n",
      "  ✅ Including: OUTC20Q3.txt (2020)\n",
      "  ✅ Including: OUTC20Q3.txt (2020)\n",
      "  ✅ Including: OUTC20Q4.txt (2020)\n",
      "  ✅ Including: OUTC20Q4.txt (2020)\n",
      "  ✅ Including: OUTC21Q1.txt (2021)\n",
      "  ✅ Including: OUTC21Q1.txt (2021)\n",
      "  ✅ Including: OUTC21Q2.txt (2021)\n",
      "  ✅ Including: OUTC21Q2.txt (2021)\n",
      "  ✅ Including: OUTC21Q3.txt (2021)\n",
      "  ✅ Including: OUTC21Q3.txt (2021)\n",
      "  ✅ Including: OUTC21Q4.txt (2021)\n",
      "  ✅ Including: OUTC21Q4.txt (2021)\n",
      "  ✅ Including: OUTC22Q1.txt (2022)\n",
      "  ✅ Including: OUTC22Q1.txt (2022)\n",
      "  ✅ Including: OUTC22Q2.txt (2022)\n",
      "  ✅ Including: OUTC22Q2.txt (2022)\n",
      "  ✅ Including: OUTC22Q3.txt (2022)\n",
      "  ✅ Including: OUTC22Q3.txt (2022)\n",
      "  ✅ Including: OUTC22Q4.txt (2022)\n",
      "  ✅ Including: OUTC22Q4.txt (2022)\n",
      "  ✅ Including: OUTC23Q1.txt (2023)\n",
      "  ✅ Including: OUTC23Q1.txt (2023)\n",
      "  ✅ Including: OUTC23Q2.txt (2023)\n",
      "  ✅ Including: OUTC23Q2.txt (2023)\n",
      "  ✅ Including: OUTC23Q3.txt (2023)\n",
      "  ✅ Including: OUTC23Q3.txt (2023)\n",
      "  ✅ Including: OUTC23Q4.txt (2023)\n",
      "  ✅ Including: OUTC23Q4.txt (2023)\n",
      "  ✅ Including: OUTC24Q1.txt (2024)\n",
      "  ✅ Including: OUTC24Q1.txt (2024)\n",
      "  ✅ Including: OUTC24Q2.txt (2024)\n",
      "  ✅ Including: OUTC24Q2.txt (2024)\n",
      "  ✅ Including: OUTC24Q3.txt (2024)\n",
      "  ✅ Including: OUTC24Q3.txt (2024)\n",
      "  ✅ Including: OUTC24Q4.txt (2024)\n",
      "  ✅ Including: OUTC24Q4.txt (2024)\n",
      "\n",
      "📊 Filtered 96 files → 40 files (2020-2024)\n",
      "📊 Found 40 files to process for [2020, 2021, 2022, 2023, 2024]\n",
      "\n",
      "🔄 Processing 40 files in chunks of 10\n",
      "📁 Output: faers_outcomes_combined.csv\n",
      "\n",
      "📦 Processing chunk 1/4 (10 files)\n",
      "💾 Memory usage: 61.9%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d69635076c4217975714cc5624ac65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ OUTC20Q1.txt: 335470 rows, 6 cols (2020)\n",
      "  ✅ OUTC20Q1.txt: 335470 rows, 6 cols (2020)\n",
      "  ✅ OUTC20Q2.txt: 307509 rows, 6 cols (2020)\n",
      "  ✅ OUTC20Q2.txt: 307509 rows, 6 cols (2020)\n",
      "  ✅ OUTC20Q3.txt: 358815 rows, 6 cols (2020)\n",
      "  ✅ OUTC20Q3.txt: 358815 rows, 6 cols (2020)\n",
      "  ✅ OUTC20Q4.txt: 365575 rows, 6 cols (2020)\n",
      "  ✅ OUTC20Q4.txt: 365575 rows, 6 cols (2020)\n",
      "  ✅ OUTC21Q1.txt: 371698 rows, 6 cols (2021)\n",
      "  ✅ OUTC21Q1.txt: 371698 rows, 6 cols (2021)\n",
      "  🔗 Combining 10 dataframes from chunk 1\n",
      "  ✅ Chunk 1: 3478134 rows appended (Total: 3478134)\n",
      "  💾 Memory after chunk 1: 64.0%\n",
      "\n",
      "📦 Processing chunk 2/4 (10 files)\n",
      "💾 Memory usage: 64.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135e85e5c1254c05b993b6658a5ac9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ OUTC21Q2.txt: 383928 rows, 6 cols (2021)\n",
      "  ✅ OUTC21Q2.txt: 383928 rows, 6 cols (2021)\n",
      "  ✅ OUTC21Q3.txt: 420729 rows, 6 cols (2021)\n",
      "  ✅ OUTC21Q3.txt: 420729 rows, 6 cols (2021)\n",
      "  ✅ OUTC21Q4.txt: 337168 rows, 6 cols (2021)\n",
      "  ✅ OUTC21Q4.txt: 337168 rows, 6 cols (2021)\n",
      "  ✅ OUTC22Q1.txt: 375497 rows, 6 cols (2022)\n",
      "  ✅ OUTC22Q1.txt: 375497 rows, 6 cols (2022)\n",
      "  ✅ OUTC22Q2.txt: 325309 rows, 6 cols (2022)\n",
      "  ✅ OUTC22Q2.txt: 325309 rows, 6 cols (2022)\n",
      "  🔗 Combining 10 dataframes from chunk 2\n",
      "  ✅ Chunk 2: 3685262 rows appended (Total: 7163396)\n",
      "  💾 Memory after chunk 2: 64.9%\n",
      "\n",
      "📦 Processing chunk 3/4 (10 files)\n",
      "💾 Memory usage: 64.9%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da001b6d3394e658971314b64d6b5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ OUTC22Q3.txt: 345763 rows, 6 cols (2022)\n",
      "  ✅ OUTC22Q3.txt: 345763 rows, 6 cols (2022)\n",
      "  ✅ OUTC22Q4.txt: 334611 rows, 6 cols (2022)\n",
      "  ✅ OUTC22Q4.txt: 334611 rows, 6 cols (2022)\n",
      "  ✅ OUTC23Q1.txt: 309217 rows, 6 cols (2023)\n",
      "  ✅ OUTC23Q1.txt: 309217 rows, 6 cols (2023)\n",
      "  ✅ OUTC23Q2.txt: 303513 rows, 6 cols (2023)\n",
      "  ✅ OUTC23Q2.txt: 303513 rows, 6 cols (2023)\n",
      "  ✅ OUTC23Q3.txt: 307396 rows, 6 cols (2023)\n",
      "  ✅ OUTC23Q3.txt: 307396 rows, 6 cols (2023)\n",
      "  🔗 Combining 10 dataframes from chunk 3\n",
      "  ✅ Chunk 3: 3201000 rows appended (Total: 10364396)\n",
      "  💾 Memory after chunk 3: 65.5%\n",
      "\n",
      "📦 Processing chunk 4/4 (10 files)\n",
      "💾 Memory usage: 65.5%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a8bf03fd844e1d9bc5ef8df459935b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ OUTC23Q4.txt: 327797 rows, 6 cols (2023)\n",
      "  ✅ OUTC23Q4.txt: 327797 rows, 6 cols (2023)\n",
      "  ✅ OUTC24Q1.txt: 295044 rows, 6 cols (2024)\n",
      "  ✅ OUTC24Q1.txt: 295044 rows, 6 cols (2024)\n",
      "  ✅ OUTC24Q2.txt: 291572 rows, 6 cols (2024)\n",
      "  ✅ OUTC24Q2.txt: 291572 rows, 6 cols (2024)\n",
      "  ✅ OUTC24Q3.txt: 288275 rows, 6 cols (2024)\n",
      "  ✅ OUTC24Q3.txt: 288275 rows, 6 cols (2024)\n",
      "  ✅ OUTC24Q4.txt: 308960 rows, 6 cols (2024)\n",
      "  ✅ OUTC24Q4.txt: 308960 rows, 6 cols (2024)\n",
      "  🔗 Combining 10 dataframes from chunk 4\n",
      "  ✅ Chunk 4: 3023296 rows appended (Total: 13387692)\n",
      "  💾 Memory after chunk 4: 65.7%\n",
      "\n",
      "🎉 Successfully created faers_outcomes_combined_2020_2024.csv with 13,387,692 total rows\n",
      "\n",
      "================================================================================\n",
      "🚀 PROCESSING: RPSR_DF (2020-2024)\n",
      "================================================================================\n",
      "Found 0 total files for pattern rpsr*.txt\n",
      "\n",
      "📊 Filtered 0 files → 0 files (2020-2024)\n",
      "Found 96 total files for pattern RPSR*.txt\n",
      "  ⏭️  Skipping: RPSR13Q1.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: RPSR13Q1.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: RPSR13Q2.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: RPSR13Q2.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: RPSR13Q3.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: RPSR13Q3.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: RPSR13Q4.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: RPSR13Q4.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: RPSR14Q1.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: RPSR14Q1.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: RPSR14Q2.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: RPSR14Q2.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: RPSR14Q3.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: RPSR14Q3.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: RPSR14Q4.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: RPSR14Q4.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: RPSR15Q1.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: RPSR15Q1.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: RPSR15Q2.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: RPSR15Q2.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: RPSR15Q3.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: RPSR15Q3.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: RPSR15Q4.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: RPSR15Q4.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: RPSR16Q1.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: RPSR16Q1.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: RPSR16Q2.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: RPSR16Q2.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: RPSR16Q3.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: RPSR16Q3.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: RPSR16Q4.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: RPSR16Q4.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: RPSR17Q1.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: RPSR17Q1.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: RPSR17Q2.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: RPSR17Q2.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: RPSR17Q3.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: RPSR17Q3.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: RPSR17Q4.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: RPSR17Q4.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: RPSR18Q1.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: RPSR18Q1.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: RPSR18Q2.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: RPSR18Q2.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: RPSR18Q3.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: RPSR18Q3.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: RPSR18Q4.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: RPSR18Q4.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: RPSR19Q1.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: RPSR19Q1.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: RPSR19Q2.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: RPSR19Q2.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: RPSR19Q3.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: RPSR19Q3.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: RPSR19Q4.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: RPSR19Q4.txt (2019) - outside target range\n",
      "  ✅ Including: RPSR20Q1.txt (2020)\n",
      "  ✅ Including: RPSR20Q1.txt (2020)\n",
      "  ✅ Including: RPSR20Q2.txt (2020)\n",
      "  ✅ Including: RPSR20Q2.txt (2020)\n",
      "  ✅ Including: RPSR20Q3.txt (2020)\n",
      "  ✅ Including: RPSR20Q3.txt (2020)\n",
      "  ✅ Including: RPSR20Q4.txt (2020)\n",
      "  ✅ Including: RPSR20Q4.txt (2020)\n",
      "  ✅ Including: RPSR21Q1.txt (2021)\n",
      "  ✅ Including: RPSR21Q1.txt (2021)\n",
      "  ✅ Including: RPSR21Q2.txt (2021)\n",
      "  ✅ Including: RPSR21Q2.txt (2021)\n",
      "  ✅ Including: RPSR21Q3.txt (2021)\n",
      "  ✅ Including: RPSR21Q3.txt (2021)\n",
      "  ✅ Including: RPSR21Q4.txt (2021)\n",
      "  ✅ Including: RPSR21Q4.txt (2021)\n",
      "  ✅ Including: RPSR22Q1.txt (2022)\n",
      "  ✅ Including: RPSR22Q1.txt (2022)\n",
      "  ✅ Including: RPSR22Q2.txt (2022)\n",
      "  ✅ Including: RPSR22Q2.txt (2022)\n",
      "  ✅ Including: RPSR22Q3.txt (2022)\n",
      "  ✅ Including: RPSR22Q3.txt (2022)\n",
      "  ✅ Including: RPSR22Q4.txt (2022)\n",
      "  ✅ Including: RPSR22Q4.txt (2022)\n",
      "  ✅ Including: RPSR23Q1.txt (2023)\n",
      "  ✅ Including: RPSR23Q1.txt (2023)\n",
      "  ✅ Including: RPSR23Q2.txt (2023)\n",
      "  ✅ Including: RPSR23Q2.txt (2023)\n",
      "  ✅ Including: RPSR23Q3.txt (2023)\n",
      "  ✅ Including: RPSR23Q3.txt (2023)\n",
      "  ✅ Including: RPSR23Q4.txt (2023)\n",
      "  ✅ Including: RPSR23Q4.txt (2023)\n",
      "  ✅ Including: RPSR24Q1.txt (2024)\n",
      "  ✅ Including: RPSR24Q1.txt (2024)\n",
      "  ✅ Including: RPSR24Q2.txt (2024)\n",
      "  ✅ Including: RPSR24Q2.txt (2024)\n",
      "  ✅ Including: RPSR24Q3.txt (2024)\n",
      "  ✅ Including: RPSR24Q3.txt (2024)\n",
      "  ✅ Including: RPSR24Q4.txt (2024)\n",
      "  ✅ Including: RPSR24Q4.txt (2024)\n",
      "\n",
      "📊 Filtered 96 files → 40 files (2020-2024)\n",
      "📊 Found 40 files to process for [2020, 2021, 2022, 2023, 2024]\n",
      "\n",
      "🔄 Processing 40 files in chunks of 10\n",
      "📁 Output: faers_reports_combined.csv\n",
      "\n",
      "📦 Processing chunk 1/4 (10 files)\n",
      "💾 Memory usage: 65.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22add35e532e43b4a5ba560475babbcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ RPSR20Q1.txt: 15492 rows, 6 cols (2020)\n",
      "  ✅ RPSR20Q1.txt: 15492 rows, 6 cols (2020)\n",
      "  ✅ RPSR20Q2.txt: 13094 rows, 6 cols (2020)\n",
      "  ✅ RPSR20Q2.txt: 13094 rows, 6 cols (2020)\n",
      "  ✅ RPSR20Q3.txt: 17281 rows, 6 cols (2020)\n",
      "  ✅ RPSR20Q3.txt: 17281 rows, 6 cols (2020)\n",
      "  ✅ RPSR20Q4.txt: 14477 rows, 6 cols (2020)\n",
      "  ✅ RPSR20Q4.txt: 14477 rows, 6 cols (2020)\n",
      "  ✅ RPSR21Q1.txt: 14046 rows, 6 cols (2021)\n",
      "  ✅ RPSR21Q1.txt: 14046 rows, 6 cols (2021)\n",
      "  🔗 Combining 10 dataframes from chunk 1\n",
      "  ✅ Chunk 1: 148780 rows appended (Total: 148780)\n",
      "  💾 Memory after chunk 1: 65.7%\n",
      "\n",
      "📦 Processing chunk 2/4 (10 files)\n",
      "💾 Memory usage: 65.7%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8776ca92a1ba471c92bbe588a0fc7415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ RPSR21Q2.txt: 13123 rows, 6 cols (2021)\n",
      "  ✅ RPSR21Q2.txt: 13123 rows, 6 cols (2021)\n",
      "  ✅ RPSR21Q3.txt: 16855 rows, 6 cols (2021)\n",
      "  ✅ RPSR21Q3.txt: 16855 rows, 6 cols (2021)\n",
      "  ✅ RPSR21Q4.txt: 4936 rows, 6 cols (2021)\n",
      "  ✅ RPSR21Q4.txt: 4936 rows, 6 cols (2021)\n",
      "  ✅ RPSR22Q1.txt: 13091 rows, 6 cols (2022)\n",
      "  ✅ RPSR22Q1.txt: 13091 rows, 6 cols (2022)\n",
      "  ✅ RPSR22Q2.txt: 13867 rows, 6 cols (2022)\n",
      "  ✅ RPSR22Q2.txt: 13867 rows, 6 cols (2022)\n",
      "  🔗 Combining 10 dataframes from chunk 2\n",
      "  ✅ Chunk 2: 123744 rows appended (Total: 272524)\n",
      "  💾 Memory after chunk 2: 66.0%\n",
      "\n",
      "📦 Processing chunk 3/4 (10 files)\n",
      "💾 Memory usage: 66.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf5c16640964d6c9b6d6c338479e8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ RPSR22Q3.txt: 14727 rows, 6 cols (2022)\n",
      "  ✅ RPSR22Q3.txt: 14727 rows, 6 cols (2022)\n",
      "  ✅ RPSR22Q4.txt: 14398 rows, 6 cols (2022)\n",
      "  ✅ RPSR22Q4.txt: 14398 rows, 6 cols (2022)\n",
      "  ✅ RPSR23Q1.txt: 13851 rows, 6 cols (2023)\n",
      "  ✅ RPSR23Q1.txt: 13851 rows, 6 cols (2023)\n",
      "  ✅ RPSR23Q2.txt: 13884 rows, 6 cols (2023)\n",
      "  ✅ RPSR23Q2.txt: 13884 rows, 6 cols (2023)\n",
      "  ✅ RPSR23Q3.txt: 11524 rows, 6 cols (2023)\n",
      "  ✅ RPSR23Q3.txt: 11524 rows, 6 cols (2023)\n",
      "  🔗 Combining 10 dataframes from chunk 3\n",
      "  ✅ Chunk 3: 136768 rows appended (Total: 409292)\n",
      "  💾 Memory after chunk 3: 67.2%\n",
      "\n",
      "📦 Processing chunk 4/4 (10 files)\n",
      "💾 Memory usage: 67.2%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3c8d5b4e964a2d8eba70a07f530e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ RPSR23Q4.txt: 13238 rows, 6 cols (2023)\n",
      "  ✅ RPSR23Q4.txt: 13238 rows, 6 cols (2023)\n",
      "  ✅ RPSR24Q1.txt: 12381 rows, 6 cols (2024)\n",
      "  ✅ RPSR24Q1.txt: 12381 rows, 6 cols (2024)\n",
      "  ✅ RPSR24Q2.txt: 11517 rows, 6 cols (2024)\n",
      "  ✅ RPSR24Q2.txt: 11517 rows, 6 cols (2024)\n",
      "  ✅ RPSR24Q3.txt: 10087 rows, 6 cols (2024)\n",
      "  ✅ RPSR24Q3.txt: 10087 rows, 6 cols (2024)\n",
      "  ✅ RPSR24Q4.txt: 11627 rows, 6 cols (2024)\n",
      "  ✅ RPSR24Q4.txt: 11627 rows, 6 cols (2024)\n",
      "  🔗 Combining 10 dataframes from chunk 4\n",
      "  ✅ Chunk 4: 117700 rows appended (Total: 526992)\n",
      "  💾 Memory after chunk 4: 67.1%\n",
      "\n",
      "🎉 Successfully created faers_reports_combined_2020_2024.csv with 526,992 total rows\n",
      "\n",
      "================================================================================\n",
      "🚀 PROCESSING: THER_DF (2020-2024)\n",
      "================================================================================\n",
      "Found 0 total files for pattern ther*.txt\n",
      "\n",
      "📊 Filtered 0 files → 0 files (2020-2024)\n",
      "Found 96 total files for pattern THER*.txt\n",
      "  ⏭️  Skipping: THER13Q1.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: THER13Q1.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: THER13Q2.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: THER13Q2.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: THER13Q3.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: THER13Q3.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: THER13Q4.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: THER13Q4.txt (2013) - outside target range\n",
      "  ⏭️  Skipping: THER14Q1.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: THER14Q1.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: THER14Q2.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: THER14Q2.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: THER14Q3.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: THER14Q3.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: THER14Q4.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: THER14Q4.txt (2014) - outside target range\n",
      "  ⏭️  Skipping: THER15Q1.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: THER15Q1.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: THER15Q2.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: THER15Q2.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: THER15Q3.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: THER15Q3.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: THER15Q4.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: THER15Q4.txt (2015) - outside target range\n",
      "  ⏭️  Skipping: THER16Q1.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: THER16Q1.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: THER16Q2.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: THER16Q2.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: THER16Q3.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: THER16Q3.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: THER16Q4.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: THER16Q4.txt (2016) - outside target range\n",
      "  ⏭️  Skipping: THER17Q1.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: THER17Q1.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: THER17Q2.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: THER17Q2.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: THER17Q3.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: THER17Q3.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: THER17Q4.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: THER17Q4.txt (2017) - outside target range\n",
      "  ⏭️  Skipping: THER18Q1.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: THER18Q1.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: THER18Q2.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: THER18Q2.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: THER18Q3.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: THER18Q3.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: THER18Q4.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: THER18Q4.txt (2018) - outside target range\n",
      "  ⏭️  Skipping: THER19Q1.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: THER19Q1.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: THER19Q2.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: THER19Q2.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: THER19Q3.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: THER19Q3.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: THER19Q4.txt (2019) - outside target range\n",
      "  ⏭️  Skipping: THER19Q4.txt (2019) - outside target range\n",
      "  ✅ Including: THER20Q1.txt (2020)\n",
      "  ✅ Including: THER20Q1.txt (2020)\n",
      "  ✅ Including: THER20Q2.txt (2020)\n",
      "  ✅ Including: THER20Q2.txt (2020)\n",
      "  ✅ Including: THER20Q3.txt (2020)\n",
      "  ✅ Including: THER20Q3.txt (2020)\n",
      "  ✅ Including: THER20Q4.txt (2020)\n",
      "  ✅ Including: THER20Q4.txt (2020)\n",
      "  ✅ Including: THER21Q1.txt (2021)\n",
      "  ✅ Including: THER21Q1.txt (2021)\n",
      "  ✅ Including: THER21Q2.txt (2021)\n",
      "  ✅ Including: THER21Q2.txt (2021)\n",
      "  ✅ Including: THER21Q3.txt (2021)\n",
      "  ✅ Including: THER21Q3.txt (2021)\n",
      "  ✅ Including: THER21Q4.txt (2021)\n",
      "  ✅ Including: THER21Q4.txt (2021)\n",
      "  ✅ Including: THER22Q1.txt (2022)\n",
      "  ✅ Including: THER22Q1.txt (2022)\n",
      "  ✅ Including: THER22Q2.txt (2022)\n",
      "  ✅ Including: THER22Q2.txt (2022)\n",
      "  ✅ Including: THER22Q3.txt (2022)\n",
      "  ✅ Including: THER22Q3.txt (2022)\n",
      "  ✅ Including: THER22Q4.txt (2022)\n",
      "  ✅ Including: THER22Q4.txt (2022)\n",
      "  ✅ Including: THER23Q1.txt (2023)\n",
      "  ✅ Including: THER23Q1.txt (2023)\n",
      "  ✅ Including: THER23Q2.txt (2023)\n",
      "  ✅ Including: THER23Q2.txt (2023)\n",
      "  ✅ Including: THER23Q3.txt (2023)\n",
      "  ✅ Including: THER23Q3.txt (2023)\n",
      "  ✅ Including: THER23Q4.txt (2023)\n",
      "  ✅ Including: THER23Q4.txt (2023)\n",
      "  ✅ Including: THER24Q1.txt (2024)\n",
      "  ✅ Including: THER24Q1.txt (2024)\n",
      "  ✅ Including: THER24Q2.txt (2024)\n",
      "  ✅ Including: THER24Q2.txt (2024)\n",
      "  ✅ Including: THER24Q3.txt (2024)\n",
      "  ✅ Including: THER24Q3.txt (2024)\n",
      "  ✅ Including: THER24Q4.txt (2024)\n",
      "  ✅ Including: THER24Q4.txt (2024)\n",
      "\n",
      "📊 Filtered 96 files → 40 files (2020-2024)\n",
      "📊 Found 40 files to process for [2020, 2021, 2022, 2023, 2024]\n",
      "\n",
      "🔄 Processing 40 files in chunks of 10\n",
      "📁 Output: faers_therapy_combined.csv\n",
      "\n",
      "📦 Processing chunk 1/4 (10 files)\n",
      "💾 Memory usage: 67.2%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c428e2bdbd8d4784a638166d6dcfcfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ THER20Q1.txt: 728199 rows, 10 cols (2020)\n",
      "  ✅ THER20Q1.txt: 728199 rows, 10 cols (2020)\n",
      "  ✅ THER20Q2.txt: 636959 rows, 10 cols (2020)\n",
      "  ✅ THER20Q2.txt: 636959 rows, 10 cols (2020)\n",
      "  ✅ THER20Q3.txt: 646462 rows, 10 cols (2020)\n",
      "  ✅ THER20Q3.txt: 646462 rows, 10 cols (2020)\n",
      "  ✅ THER20Q4.txt: 681411 rows, 10 cols (2020)\n",
      "  ✅ THER20Q4.txt: 681411 rows, 10 cols (2020)\n",
      "  ✅ THER21Q1.txt: 786472 rows, 10 cols (2021)\n",
      "  ✅ THER21Q1.txt: 786472 rows, 10 cols (2021)\n",
      "  🔗 Combining 10 dataframes from chunk 1\n",
      "  ✅ Chunk 1: 6959006 rows appended (Total: 6959006)\n",
      "  💾 Memory after chunk 1: 55.4%\n",
      "\n",
      "📦 Processing chunk 2/4 (10 files)\n",
      "💾 Memory usage: 55.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3944ac30acca4e2b85f455840ae4269b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ THER21Q2.txt: 789968 rows, 10 cols (2021)\n",
      "  ✅ THER21Q2.txt: 789968 rows, 10 cols (2021)\n",
      "  ✅ THER21Q3.txt: 848661 rows, 10 cols (2021)\n",
      "  ✅ THER21Q3.txt: 848661 rows, 10 cols (2021)\n",
      "  ✅ THER21Q4.txt: 671045 rows, 10 cols (2021)\n",
      "  ✅ THER21Q4.txt: 671045 rows, 10 cols (2021)\n",
      "  ✅ THER22Q1.txt: 748899 rows, 10 cols (2022)\n",
      "  ✅ THER22Q1.txt: 748899 rows, 10 cols (2022)\n",
      "  ✅ THER22Q2.txt: 690828 rows, 10 cols (2022)\n",
      "  ✅ THER22Q2.txt: 690828 rows, 10 cols (2022)\n",
      "  🔗 Combining 10 dataframes from chunk 2\n",
      "  ✅ Chunk 2: 7498802 rows appended (Total: 14457808)\n",
      "  💾 Memory after chunk 2: 58.7%\n",
      "\n",
      "📦 Processing chunk 3/4 (10 files)\n",
      "💾 Memory usage: 58.7%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9849a9bedb7245f393ac7d645600f1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ THER22Q3.txt: 717902 rows, 10 cols (2022)\n",
      "  ✅ THER22Q3.txt: 717902 rows, 10 cols (2022)\n",
      "  ✅ THER22Q4.txt: 726767 rows, 10 cols (2022)\n",
      "  ✅ THER22Q4.txt: 726767 rows, 10 cols (2022)\n",
      "  ✅ THER23Q1.txt: 683408 rows, 10 cols (2023)\n",
      "  ✅ THER23Q1.txt: 683408 rows, 10 cols (2023)\n",
      "  ✅ THER23Q2.txt: 678128 rows, 10 cols (2023)\n",
      "  ✅ THER23Q2.txt: 678128 rows, 10 cols (2023)\n",
      "  ✅ THER23Q3.txt: 593027 rows, 10 cols (2023)\n",
      "  ✅ THER23Q3.txt: 593027 rows, 10 cols (2023)\n",
      "  🔗 Combining 10 dataframes from chunk 3\n",
      "  ✅ Chunk 3: 6798464 rows appended (Total: 21256272)\n",
      "  💾 Memory after chunk 3: 66.9%\n",
      "\n",
      "📦 Processing chunk 4/4 (10 files)\n",
      "💾 Memory usage: 66.9%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ceb1b6c62e40aca5b5f76eff26117d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ THER23Q4.txt: 633087 rows, 10 cols (2023)\n",
      "  ✅ THER23Q4.txt: 633087 rows, 10 cols (2023)\n",
      "  ✅ THER24Q1.txt: 594449 rows, 10 cols (2024)\n",
      "  ✅ THER24Q1.txt: 594449 rows, 10 cols (2024)\n",
      "  ✅ THER24Q2.txt: 539334 rows, 10 cols (2024)\n",
      "  ✅ THER24Q2.txt: 539334 rows, 10 cols (2024)\n",
      "  ✅ THER24Q3.txt: 532854 rows, 10 cols (2024)\n",
      "  ✅ THER24Q3.txt: 532854 rows, 10 cols (2024)\n",
      "  ✅ THER24Q4.txt: 561889 rows, 10 cols (2024)\n",
      "  ✅ THER24Q4.txt: 561889 rows, 10 cols (2024)\n",
      "  🔗 Combining 10 dataframes from chunk 4\n",
      "  ✅ Chunk 4: 5723226 rows appended (Total: 26979498)\n",
      "  💾 Memory after chunk 4: 58.8%\n",
      "\n",
      "🎉 Successfully created faers_therapy_combined_2020_2024.csv with 26,979,498 total rows\n",
      "\n",
      "🎉 ALL PROCESSING COMPLETE FOR [2020, 2021, 2022, 2023, 2024]!\n",
      "\n",
      "📊 FINAL SUMMARY:\n",
      "================================================================================\n",
      "✅ demo_df: faers_demographics_combined_2020_2024.csv (2752.1 MB)\n",
      "✅ drug_df: faers_drugs_combined_2020_2024.csv (8985.0 MB)\n",
      "✅ reac_df: faers_reactions_combined_2020_2024.csv (3301.0 MB)\n",
      "✅ indi_df: faers_indications_combined_2020_2024.csv (3314.1 MB)\n",
      "✅ outc_df: faers_outcomes_combined_2020_2024.csv (549.3 MB)\n",
      "✅ rpsr_df: faers_reports_combined_2020_2024.csv (21.8 MB)\n",
      "✅ ther_df: faers_therapy_combined_2020_2024.csv (1469.1 MB)\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# MEMORY-EFFICIENT FAERS DATA PROCESSOR (2020-2024 ONLY)\n",
    "# ===================================================================\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import psutil\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "base_directory = \"/Users/deliciamagdaline/Desktop/Project/faers_menstrual_rag_project/data/raw\"\n",
    "CHUNK_SIZE = 10  # Process files in chunks of 10\n",
    "MEMORY_THRESHOLD = 85  # Stop if memory usage exceeds 85%\n",
    "TARGET_YEARS = [2020, 2021, 2022, 2023, 2024]  # 🆕 ONLY THESE YEARS\n",
    "\n",
    "def check_memory():\n",
    "    \"\"\"Check current memory usage\"\"\"\n",
    "    memory_percent = psutil.virtual_memory().percent\n",
    "    print(f\"💾 Memory usage: {memory_percent:.1f}%\")\n",
    "    return memory_percent\n",
    "\n",
    "def filter_files_by_year(file_list, target_years=TARGET_YEARS):\n",
    "    \"\"\"Filter files to only include target years\"\"\"\n",
    "    filtered_files = []\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        dir_name = os.path.basename(os.path.dirname(os.path.dirname(file_path)))\n",
    "        match = re.match(r'(\\d{4})(q\\d)', dir_name)\n",
    "        \n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            if year in target_years:\n",
    "                filtered_files.append(file_path)\n",
    "                print(f\"  ✅ Including: {os.path.basename(file_path)} ({year})\")\n",
    "            else:\n",
    "                print(f\"  ⏭️  Skipping: {os.path.basename(file_path)} ({year}) - outside target range\")\n",
    "        else:\n",
    "            print(f\"  ❓ Could not extract year from: {file_path}\")\n",
    "    \n",
    "    print(f\"\\n📊 Filtered {len(file_list)} files → {len(filtered_files)} files ({min(target_years)}-{max(target_years)})\")\n",
    "    return filtered_files\n",
    "\n",
    "def find_files(pattern):\n",
    "    \"\"\"Find files with case-insensitive search and year filtering\"\"\"\n",
    "    search_patterns = [\n",
    "        f\"{base_directory}/*/ascii/{pattern}\",\n",
    "        f\"{base_directory}/*/ASCII/{pattern}\",\n",
    "    ]\n",
    "    \n",
    "    all_files = []\n",
    "    for search_path in search_patterns:\n",
    "        files = glob.glob(search_path)\n",
    "        all_files.extend(files)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_files = sorted(list(set(all_files)))\n",
    "    print(f\"Found {len(unique_files)} total files for pattern {pattern}\")\n",
    "    \n",
    "    # 🆕 FILTER BY YEAR BEFORE PROCESSING\n",
    "    filtered_files = filter_files_by_year(unique_files, TARGET_YEARS)\n",
    "    \n",
    "    return filtered_files\n",
    "\n",
    "def process_single_file(file_path):\n",
    "    \"\"\"Process a single file with memory management\"\"\"\n",
    "    try:\n",
    "        # Extract metadata\n",
    "        dir_name = os.path.basename(os.path.dirname(os.path.dirname(file_path)))\n",
    "        match = re.match(r'(\\d{4})(q\\d)', dir_name)\n",
    "        year, quarter = (match.groups() if match else (None, None))\n",
    "        \n",
    "        # 🆕 DOUBLE-CHECK YEAR (safety net)\n",
    "        if year and int(year) not in TARGET_YEARS:\n",
    "            print(f\"  ⏭️  Skipping {os.path.basename(file_path)} - year {year} not in target range\")\n",
    "            return None\n",
    "        \n",
    "        # Try multiple delimiters and encodings\n",
    "        for delimiter in ['$', '\\t', '|']:\n",
    "            for encoding in ['ISO-8859-1', 'utf-8', 'cp1252']:\n",
    "                try:\n",
    "                    df = pd.read_csv(\n",
    "                        file_path, \n",
    "                        delimiter=delimiter, \n",
    "                        encoding=encoding,\n",
    "                        on_bad_lines=\"skip\", \n",
    "                        low_memory=False,\n",
    "                        dtype=str  # Read everything as string to save memory\n",
    "                    )\n",
    "                    \n",
    "                    # Check if we got reasonable data\n",
    "                    if len(df) > 0 and len(df.columns) > 1:\n",
    "                        df['year'] = year\n",
    "                        df['quarter'] = quarter\n",
    "                        df['source_file'] = os.path.basename(file_path)\n",
    "                        \n",
    "                        print(f\"  ✅ {os.path.basename(file_path)}: {len(df)} rows, {len(df.columns)} cols ({year})\")\n",
    "                        return df\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        print(f\"  ❌ Failed to read: {os.path.basename(file_path)}\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def combine_files_chunked(file_list, output_filename, chunk_size=CHUNK_SIZE):\n",
    "    \"\"\"Combine files in chunks to avoid memory crashes\"\"\"\n",
    "    if not file_list:\n",
    "        print(f\"ERROR: No files found for {output_filename}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n🔄 Processing {len(file_list)} files in chunks of {chunk_size}\")\n",
    "    print(f\"📁 Output: {output_filename}\")\n",
    "    \n",
    "    # 🆕 ADD YEAR RANGE TO FILENAME\n",
    "    year_range = f\"_{min(TARGET_YEARS)}_{max(TARGET_YEARS)}\"\n",
    "    output_filename = output_filename.replace('.csv', f'{year_range}.csv')\n",
    "    \n",
    "    # Remove existing output file if it exists\n",
    "    if os.path.exists(output_filename):\n",
    "        os.remove(output_filename)\n",
    "        print(f\"🗑️  Removed existing {output_filename}\")\n",
    "    \n",
    "    total_rows = 0\n",
    "    header_written = False\n",
    "    \n",
    "    # Process files in chunks\n",
    "    for i in range(0, len(file_list), chunk_size):\n",
    "        chunk_files = file_list[i:i+chunk_size]\n",
    "        chunk_num = (i // chunk_size) + 1\n",
    "        total_chunks = (len(file_list) + chunk_size - 1) // chunk_size\n",
    "        \n",
    "        print(f\"\\n📦 Processing chunk {chunk_num}/{total_chunks} ({len(chunk_files)} files)\")\n",
    "        \n",
    "        # Check memory before processing chunk\n",
    "        memory_percent = check_memory()\n",
    "        if memory_percent > MEMORY_THRESHOLD:\n",
    "            print(f\"⚠️  Memory usage too high ({memory_percent:.1f}%). Running garbage collection...\")\n",
    "            gc.collect()\n",
    "            time.sleep(2)\n",
    "        \n",
    "        chunk_dfs = []\n",
    "        \n",
    "        # Process each file in the chunk\n",
    "        for file_path in tqdm(chunk_files, desc=f\"Chunk {chunk_num}\"):\n",
    "            df = process_single_file(file_path)\n",
    "            if df is not None:\n",
    "                chunk_dfs.append(df)\n",
    "        \n",
    "        # Combine chunk dataframes\n",
    "        if chunk_dfs:\n",
    "            print(f\"  🔗 Combining {len(chunk_dfs)} dataframes from chunk {chunk_num}\")\n",
    "            chunk_combined = pd.concat(chunk_dfs, ignore_index=True, sort=False)\n",
    "            \n",
    "            # Append to output file\n",
    "            mode = 'w' if not header_written else 'a'\n",
    "            header = not header_written\n",
    "            \n",
    "            chunk_combined.to_csv(output_filename, mode=mode, header=header, index=False)\n",
    "            \n",
    "            chunk_rows = len(chunk_combined)\n",
    "            total_rows += chunk_rows\n",
    "            header_written = True\n",
    "            \n",
    "            print(f\"  ✅ Chunk {chunk_num}: {chunk_rows} rows appended (Total: {total_rows})\")\n",
    "            \n",
    "            # Clear memory\n",
    "            del chunk_combined\n",
    "            del chunk_dfs\n",
    "            gc.collect()\n",
    "        \n",
    "        print(f\"  💾 Memory after chunk {chunk_num}: {psutil.virtual_memory().percent:.1f}%\")\n",
    "    \n",
    "    if total_rows > 0:\n",
    "        print(f\"\\n🎉 Successfully created {output_filename} with {total_rows:,} total rows\")\n",
    "        return pd.read_csv(output_filename, nrows=0)  # Return just headers for validation\n",
    "    else:\n",
    "        print(f\"\\n❌ No data processed for {output_filename}\")\n",
    "        return None\n",
    "\n",
    "def process_data_type(data_type, patterns, resume_from=None):\n",
    "    \"\"\"Process a single data type with resume capability\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🚀 PROCESSING: {data_type.upper()} ({min(TARGET_YEARS)}-{max(TARGET_YEARS)})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 🆕 UPDATE OUTPUT FILENAMES WITH YEAR RANGE\n",
    "    year_suffix = f\"_{min(TARGET_YEARS)}_{max(TARGET_YEARS)}\"\n",
    "    output_files = {\n",
    "        'demo_df': f'faers_demographics_combined{year_suffix}.csv',\n",
    "        'drug_df': f'faers_drugs_combined{year_suffix}.csv',\n",
    "        'reac_df': f'faers_reactions_combined{year_suffix}.csv',\n",
    "        'indi_df': f'faers_indications_combined{year_suffix}.csv',\n",
    "        'outc_df': f'faers_outcomes_combined{year_suffix}.csv',\n",
    "        'rpsr_df': f'faers_reports_combined{year_suffix}.csv',\n",
    "        'ther_df': f'faers_therapy_combined{year_suffix}.csv'\n",
    "    }\n",
    "    \n",
    "    output_filename = output_files.get(data_type, f'faers_{data_type}_combined{year_suffix}.csv')\n",
    "    \n",
    "    if os.path.exists(output_filename) and resume_from != data_type:\n",
    "        print(f\"✅ {output_filename} already exists. Skipping...\")\n",
    "        try:\n",
    "            df = pd.read_csv(output_filename, nrows=100)  # Just check first 100 rows\n",
    "            print(f\"   File appears valid with {len(df.columns)} columns\")\n",
    "            return df\n",
    "        except:\n",
    "            print(f\"   File appears corrupted. Will reprocess...\")\n",
    "    \n",
    "    # Find files (already filtered by year in find_files function)\n",
    "    all_files = []\n",
    "    for pattern in patterns:\n",
    "        files = find_files(pattern)\n",
    "        all_files.extend(files)\n",
    "    \n",
    "    unique_files = sorted(list(set(all_files)))\n",
    "    \n",
    "    if not unique_files:\n",
    "        print(f\"❌ No files found for {data_type} in years {TARGET_YEARS}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"📊 Found {len(unique_files)} files to process for {TARGET_YEARS}\")\n",
    "    \n",
    "    # Process files\n",
    "    return combine_files_chunked(unique_files, output_filename.replace(year_suffix, ''))\n",
    "\n",
    "# --- MAIN EXECUTION WITH YEAR FILTERING ---\n",
    "def main(resume_from=None, target_years=None):\n",
    "    \"\"\"\n",
    "    Main function with resume capability and year filtering\n",
    "    resume_from: Skip to this data type (e.g., 'drug_df' to resume from drugs)\n",
    "    target_years: List of years to process (default: 2020-2024)\n",
    "    \"\"\"\n",
    "    global TARGET_YEARS\n",
    "    if target_years:\n",
    "        TARGET_YEARS = target_years\n",
    "    \n",
    "    print(\"🚀 MEMORY-EFFICIENT FAERS PROCESSOR\")\n",
    "    print(f\"📅 TARGET YEARS: {TARGET_YEARS}\")\n",
    "    print(\"=\"*80)\n",
    "    check_memory()\n",
    "    \n",
    "    file_types = {\n",
    "        'demo_df': ['demo*.txt', 'DEMO*.txt'],\n",
    "        'drug_df': ['drug*.txt', 'DRUG*.txt'],\n",
    "        'reac_df': ['reac*.txt', 'REAC*.txt'],\n",
    "        'indi_df': ['indi*.txt', 'INDI*.txt'],\n",
    "        'outc_df': ['outc*.txt', 'OUTC*.txt'],\n",
    "        'rpsr_df': ['rpsr*.txt', 'RPSR*.txt'],\n",
    "        'ther_df': ['ther*.txt', 'THER*.txt']\n",
    "    }\n",
    "    \n",
    "    data_frames = {}\n",
    "    processing_order = list(file_types.keys())\n",
    "    \n",
    "    # Find start point if resuming\n",
    "    start_idx = 0\n",
    "    if resume_from and resume_from in processing_order:\n",
    "        start_idx = processing_order.index(resume_from)\n",
    "        print(f\"🔄 RESUMING from {resume_from}\")\n",
    "    \n",
    "    # Process each data type\n",
    "    for data_type in processing_order[start_idx:]:\n",
    "        patterns = file_types[data_type]\n",
    "        \n",
    "        try:\n",
    "            result = process_data_type(data_type, patterns, resume_from)\n",
    "            if result is not None:\n",
    "                data_frames[data_type] = result\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n⚠️  Processing interrupted at {data_type}\")\n",
    "            print(f\"To resume, run: main(resume_from='{data_type}')\")\n",
    "            return data_frames\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error processing {data_type}: {e}\")\n",
    "            print(f\"To resume, run: main(resume_from='{data_type}')\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n🎉 ALL PROCESSING COMPLETE FOR {TARGET_YEARS}!\")\n",
    "    return data_frames\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    # 🆕 EXAMPLES OF HOW TO RUN:\n",
    "    \n",
    "    # Default: 2020-2024 only\n",
    "    data_frames = main()\n",
    "    \n",
    "    # Custom year range:\n",
    "    # data_frames = main(target_years=[2022, 2023, 2024])\n",
    "    \n",
    "    # Resume from specific point:\n",
    "    # data_frames = main(resume_from='drug_df')\n",
    "    \n",
    "    # Both custom years and resume:\n",
    "    # data_frames = main(resume_from='drug_df', target_years=[2020, 2021])\n",
    "    \n",
    "    print(\"\\n📊 FINAL SUMMARY:\")\n",
    "    print(\"=\"*80)\n",
    "    for name, df in data_frames.items():\n",
    "        if df is not None:\n",
    "            year_suffix = f\"_{min(TARGET_YEARS)}_{max(TARGET_YEARS)}\"\n",
    "            output_files = {\n",
    "                'demo_df': f'faers_demographics_combined{year_suffix}.csv',\n",
    "                'drug_df': f'faers_drugs_combined{year_suffix}.csv',\n",
    "                'reac_df': f'faers_reactions_combined{year_suffix}.csv',\n",
    "                'indi_df': f'faers_indications_combined{year_suffix}.csv',\n",
    "                'outc_df': f'faers_outcomes_combined{year_suffix}.csv',\n",
    "                'rpsr_df': f'faers_reports_combined{year_suffix}.csv',\n",
    "                'ther_df': f'faers_therapy_combined{year_suffix}.csv'\n",
    "            }\n",
    "            filename = output_files[name]\n",
    "            if os.path.exists(filename):\n",
    "                size_mb = os.path.getsize(filename) / 1024 / 1024\n",
    "                print(f\"✅ {name}: {filename} ({size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print(f\"❌ {name}: Not processed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
